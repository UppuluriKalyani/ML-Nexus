URL_ID,URL,article_words
bctech2011,https://insights.blackcoffer.com/ml-and-ai-based-insurance-premium-model-to-predict-premium-to-be-charged-by-the-insurance-company/,"ML and AI-based insurance premium model to predict premium to be charged by the insurance company - Blackcoffer Insights-['Client Background', 'Client:', 'A leading insurance firm worldwide', 'Industry Type:', 'BFSI', 'Products & Services:', 'Insurance', 'Organization Size:', '10000+', 'The Problem', 'The insurance industry, particularly in the context of providing coverage to Public Company Directors against Insider Trading public lawsuits, faces a significant challenge in accurately determining insurance premiums. Traditional methods of premium calculation may lack precision, and there is a growing need for more sophisticated and data-driven approaches. The integration of Artificial Intelligence (AI) and Machine Learning (ML) models in predicting insurance premiums for this specialized coverage is essential to enhance accuracy, fairness, and responsiveness in adapting to evolving risk factors.', 'The problem at hand involves developing robust AI and ML models that can effectively analyze a multitude of dynamic variables influencing the risk profile of Public Company Directors. These variables include market conditions, regulatory changes, historical legal precedents, financial performance of the insured company, and individual directorial behaviors. The goal is to create a predictive model that not only accurately assesses the risk associated with potential insider trading public lawsuits but also adapts in real-time to new information, ensuring that the insurance premiums charged by the global insurance firm are reflective of the current risk landscape.', 'Key Challenges:', 'Data Complexity:', 'The relevant data for predicting insurance premiums in this context is multifaceted, involving financial data, legal precedents, market trends, and individual directorial histories. Integrating and interpreting this diverse set of data poses a significant challenge.', 'Dynamic Risk Factors:', 'The risk factors influencing insider trading public lawsuits are dynamic and subject to rapid changes. The models must be capable of adapting to evolving market conditions, legal landscapes, and individual company dynamics.', 'Fairness and Ethics:', 'Ensuring fairness in premium calculation is critical. The models should be designed to avoid biases and discriminatory practices, considering the diverse backgrounds and contexts of Public Company Directors.', 'Regulatory Compliance:', 'The insurance industry is subject to regulatory frameworks that vary across jurisdictions. The developed models need to comply with these regulations while providing accurate and reliable predictions.', 'Interpretability:', 'Transparency in model predictions is crucial, especially in an industry where decisions can have significant financial implications. Ensuring that the AI and ML models are interpretable and explainable is vital for gaining the trust of stakeholders.', 'Addressing these challenges will not only improve the accuracy of insurance premium predictions but also contribute to the overall efficiency and effectiveness of the insurance services provided to Public Company Directors by the leading global insurance firm.', 'Blackcoffer Solution', 'To develop an ML and AI-based insurance premium prediction model for Public Company Directors in the USA, safeguarding them against insider trading public lawsuits, we propose a comprehensive solution leveraging advanced machine learning techniques. The goal is to create a model that accurately assesses the risk associated with individual directors and adapts to dynamic market conditions.', 'Data Collection and Preprocessing:', 'Financial Data:', 'Gather financial data related to the insured companies, including revenue, profit margins, and financial stability indicators.', 'Incorporate stock market data and trading patterns to capture potential insider trading signals.', 'Legal History:', 'Collect historical legal cases related to insider trading lawsuits, with a focus on outcomes and financial implications.', 'Integrate legal precedents to understand patterns and potential future risks.', 'Directorial Profiles:', 'Compile individual profiles for each Public Company Director, including their professional history, prior legal involvements, and any relevant affiliations.', 'Market Trends and Regulatory Changes:', 'Monitor market trends and regulatory changes affecting the insurance landscape.', 'Incorporate external data sources for real-time updates on legal and market conditions.', 'Feature Engineering:', 'Risk Factors:', 'Identify key risk factors contributing to the likelihood of insider trading allegations.', 'Develop features that encapsulate financial stability, market conditions, and individual directorial behaviors.', 'Sentiment Analysis:', 'Implement sentiment analysis on news articles and social media to gauge public perception and potential legal scrutiny.', 'Machine Learning Models:', 'Supervised Learning:', 'Employ supervised learning algorithms such as Random Forests, Gradient Boosting, or ensemble models.', 'Train the model on historical data with labeled outcomes related to insider trading lawsuits.', 'Anomaly Detection:', 'Implement anomaly detection techniques to identify unusual patterns that may indicate potential insider trading activities.', 'Dynamic Risk Assessment:', 'Real-Time Updates:', 'Design the model to continuously update with real-time data to adapt to evolving risk factors.', 'Implement a feedback loop to capture the impact of recent legal cases and market events.', 'Scenario Analysis:', 'Develop scenario analysis capabilities to assess the impact of hypothetical events on premium calculations.', 'Fairness and Transparency:', 'Fairness Metrics:', 'Integrate fairness metrics to ensure unbiased predictions across diverse directorial profiles.', 'Regularly audit and refine the model to address any identified biases.', 'Explainability:', 'Implement model explainability tools to provide clear insights into premium calculations.', 'Ensure transparency in how the model arrives at its predictions.', 'Model Integration and Deployment:', 'User-Friendly Interface:', 'Develop a user-friendly interface for underwriters to interact with the model.', 'Ensure seamless integration into the existing insurance company workflow.', 'API Integration:', 'Provide API endpoints for easy integration with existing insurance systems.', 'Monitoring and Maintenance:', 'Model Monitoring:', 'Implement continuous monitoring to detect model drift and performance degradation.', 'Regularly update the model with new data and retrain it to maintain accuracy.', 'Scalability:', 'Design the solution to scale horizontally to accommodate an increasing volume of data.', 'By adopting this ML and AI-based approach, the insurance company can enhance its ability to predict insurance premiums accurately, adapt to changing risk landscapes, and provide tailored coverage for Public Company Directors against insider trading public lawsuits in the dynamic environment of the USA.', 'Solution Architecture Diagram', 'Data Collection and Integration:', 'Data Sources: Financial records, legal databases, directorial profiles, market data.', 'Integration Layer: ETL processes, SQL/NoSQL databases.', 'Feature Engineering:', 'Feature Selection and Engineering Module.', 'Machine Learning Models:', 'Model Training Module: Scikit-Learn, TensorFlow, or PyTorch.', 'Model Evaluation Component.', 'Dynamic Risk Assessment:', 'Real-Time Data Integration Component: Apache Kafka.', 'Scenario Analysis Module.', 'Fairness and Transparency:', 'Fairness Metrics Integration.', 'Explainability Module: SHAP or Lime.', 'Model Integration and Deployment:', 'API Layer: RESTful API.', 'User Interface (UI).', 'Documentation for Integration.', 'Monitoring and Maintenance:', 'Monitoring Dashboard: Prometheus, Grafana.', 'Automated Model Update Pipeline: CI/CD.', 'General Documentation:', 'Model Architecture Document.', 'Technical User Manual.', 'Compliance Documentation:', 'Regulatory Compliance Report.', 'Data Privacy and Security Documentation.', 'Post-Implementation Support:', 'Support and Maintenance Plan.', 'Training and Knowledge Transfer:', 'Training Sessions.', 'Knowledge Transfer Documentation.', 'Scalability and Future-Proofing:', 'Scalable Infrastructure.', 'Flexibility for Future Enhancements.', 'Tools & Technology Used By Blackcoffer', 'Building an ML and AI-based insurance premium prediction model involves the use of various tools and technologies across different stages of development. Here’s a list of tools and technologies that can be employed for creating such a model for a leading insurance firm in the USA, specifically targeting Public Company Directors against insider trading public lawsuits:', 'Data Collection and Preprocessing:', 'Python:', 'A versatile programming language commonly used for data manipulation and preprocessing.', 'Pandas:', 'A Python library for data manipulation and analysis, useful for handling structured data.', 'NumPy:', 'A library for numerical operations in Python, often used for efficient array operations.', 'SQL/NoSQL Databases:', 'To store and retrieve structured and unstructured data efficiently.', 'Feature Engineering:', 'Scikit-Learn:', 'A machine learning library in Python that includes tools for feature extraction and preprocessing.', 'NLTK (Natural Language Toolkit):', 'For processing and analyzing textual data, particularly for sentiment analysis.', 'Machine Learning Models:', 'Scikit-Learn:', 'Provides various machine learning algorithms for classification tasks, including Random Forests and Gradient Boosting.', 'XGBoost or LightGBM:', 'Powerful gradient boosting frameworks for improved predictive performance.', 'TensorFlow or PyTorch:', 'Deep learning frameworks for building and training neural networks if the complexity of the model demands it.', 'Dynamic Risk Assessment:', 'Apache Kafka or RabbitMQ:', 'Message brokers to facilitate real-time data streaming and updates.', 'Airflow:', 'A platform to programmatically author, schedule, and monitor workflows, useful for scheduling model updates.', 'Fairness and Transparency:', 'Aequitas or Fairness Indicators:', 'Libraries for assessing and mitigating bias in machine learning models.', 'SHAP (SHapley Additive exPlanations):', 'An algorithm for model interpretability.', 'Model Integration and Deployment:', 'Flask or Django:', 'Web frameworks for building the model deployment API.', 'Docker:', 'Containerization tool for packaging the model and its dependencies.', 'Kubernetes:', 'Container orchestration for deploying and managing containerized applications at scale.', 'RESTful API:', 'For communication between the model and other components in the insurance company’s infrastructure.', 'Monitoring and Maintenance:', 'Prometheus:', 'An open-source monitoring and alerting toolkit.', 'Grafana:', 'A platform for monitoring and observability with beautiful, customizable dashboards.', 'Jenkins or GitLab CI/CD:', 'Continuous integration and continuous deployment tools for automating model updates and deployment.', 'MLflow:', 'An open-source platform to manage the end-to-end machine learning lifecycle.', 'General Development Environment:', 'Jupyter Notebooks:', 'Interactive computing environment for exploratory data analysis and model development.', 'Git:', 'Version control system for collaborative development.', 'VS Code or PyCharm:', 'Integrated development environments (IDEs) for coding and debugging.', 'It’s important to note that the choice of specific tools may vary based on the preferences of the data science team, the complexity of the model, and the existing technology stack of the insurance company. Additionally, compliance with regulatory requirements and industry standards should be considered in the selection of tools and technologies.', 'Blackcoffer Deliverables', 'The deliverables for an ML and AI-based insurance premium model for Public Company Directors in the USA, aiming to predict premiums for protection against insider trading public lawsuits, would encompass various stages of the development and deployment process. Here is a comprehensive list of deliverables:', '1. Project Documentation:', '1.1', 'Project Proposal:', 'Clearly outlines the objectives, scope, and methodology of the premium prediction model.', '1.2', 'Requirements Document:', 'Specifies the functional and non-functional requirements of the model, considering the insurance company’s needs and regulatory compliance.', '2. Data Collection and Preprocessing:', '2.1', 'Data Collection Report:', 'Details the sources and types of data gathered, including financial records, legal cases, and directorial profiles.', '2.2', 'Cleaned and Preprocessed Dataset:', 'A structured dataset ready for model training, containing relevant features and properly handled missing or inconsistent data.', '3. Feature Engineering:', '3.1', 'Feature Selection and Engineering Report:', 'Documents the process of selecting and creating features, highlighting their relevance to the prediction task.', '4. Machine Learning Models:', '4.1', 'Trained ML Models:', 'Includes the serialized models trained on historical data, such as Random Forests, Gradient Boosting, or other chosen algorithms.', '4.2', 'Model Evaluation Report:', 'Evaluates the performance of the models on validation and test datasets, including metrics like accuracy, precision, recall, and F1-score.', '5. Dynamic Risk Assessment:', '5.1', 'Real-Time Integration Component:', 'Code or module that integrates real-time data for dynamic risk assessment.', '5.2', 'Scenario Analysis Module:', 'Component allowing the assessment of premium changes based on hypothetical scenarios.', '6. Fairness and Transparency:', '6.1', 'Fairness Assessment Report:', 'Evaluates and mitigates bias, documenting fairness metrics and any adjustments made.', '6.2', 'Explainability Module:', 'Implementation of tools or methodologies for model interpretability and explanation.', '7. Model Integration and Deployment:', '7.1', 'Deployed API:', 'RESTful API endpoint for seamless integration into the insurance company’s systems.', '7.2', 'User Interface (UI):', 'User-friendly interface for underwriters to interact with the model, providing insights and entering necessary information.', '7.3', 'Documentation for Integration:', 'Comprehensive guide on integrating the model into the existing workflow, including API documentation.', '8. Monitoring and Maintenance:', '8.1', 'Monitoring Dashboard:', 'Visual representation of key metrics and alerts for model performance, developed using tools like Grafana.', '8.2', 'Automated Model Update Pipeline:', 'CI/CD pipeline or automated process for updating and retraining the model with new data.', '9. General Documentation:', '9.1', 'Model Architecture Document:', 'Detailed explanation of the model’s architecture, including components and their interactions.', '9.2', 'Technical User Manual:', 'Documentation guiding technical users on deploying, maintaining, and troubleshooting the model.', '10. Training and Knowledge Transfer:', '10.1', 'Training Sessions:', 'Conducted for the insurance company’s staff, including underwriters and IT personnel, to ensure effective use and understanding of the model.', '10.2', 'Knowledge Transfer Documentation:', 'Detailed documentation covering model usage, maintenance procedures, and troubleshooting tips.', '11. Compliance Documentation:', '11.1', 'Regulatory Compliance Report:', 'Ensures that the model adheres to relevant insurance regulations in the USA.', '11.2', 'Data Privacy and Security Documentation:', 'Outlines measures taken to ensure the privacy and security of sensitive data.', '12. Post-Implementation Support:', '12.1', 'Support and Maintenance Plan:', 'Document outlining the support and maintenance plan for the model post-implementation, including response times and escalation procedures.', 'By delivering these items, the insurance firm can ensure a thorough and transparent development process, facilitating successful integration and utilization of the ML and AI-based insurance premium prediction model.', 'Business Impacts', 'The implementation of an ML and AI-based insurance premium model for Public Company Directors in the USA, specifically tailored to protect them from insider trading public lawsuits, can have significant business impacts for the leading insurance firm. Here are several potential business impacts:', '1.', 'Improved Accuracy and Risk Assessment:', 'Impact:', 'Enhanced accuracy in predicting premiums based on advanced data analysis and machine learning algorithms.', 'Benefit:', 'Better risk assessment leads to more precise premium calculations, reducing the likelihood of underpricing or overpricing policies.', '2.', 'Increased Competitiveness:', 'Impact:', 'Utilizing cutting-edge technology to provide more accurate and dynamic premium predictions.', 'Benefit:', 'Positions the insurance firm as a leader in the market, attracting more clients seeking innovative and reliable insurance solutions.', '3.', 'Tailored Coverage and Pricing:', 'Impact:', 'Customizing coverage and premiums based on individual directorial profiles and evolving risk factors.', 'Benefit:', 'Attracts clients with diverse risk profiles, offering tailored solutions that align with their specific needs.', '4.', 'Faster Decision-Making:', 'Impact:', 'Automation of premium calculations and decision-making processes.', 'Benefit:', 'Speeds up underwriting processes, enabling quicker responses to client inquiries and facilitating faster policy issuance.', '5.', 'Reduced Operational Costs:', 'Impact:', 'Automation of routine tasks related to premium calculation and risk assessment.', 'Benefit:', 'Decreases manual workload, leading to operational efficiency and cost savings.', '6.', 'Real-Time Adaptation to Market Changes:', 'Impact:', 'Integration of real-time data for dynamic risk assessment.', 'Benefit:', 'Enables the insurance firm to adapt quickly to changes in market conditions, ensuring that premiums remain reflective of current risk landscapes.', '7.', 'Enhanced Customer Satisfaction:', 'Impact:', 'Accurate pricing, fair premium calculations, and transparent communication.', 'Benefit:', 'Increases customer satisfaction by providing a reliable and customer-centric insurance experience.', '8.', 'Mitigation of Regulatory Risks:', 'Impact:', 'Implementation of a solution that complies with insurance regulations and industry standards.', 'Benefit:', 'Reduces the risk of regulatory non-compliance, protecting the firm from legal and financial repercussions.', '9.', 'Data-Driven Decision-Making:', 'Impact:', 'Utilizing data-driven insights for decision-making processes.', 'Benefit:', 'Empowers the firm’s leadership with actionable insights, contributing to strategic decision-making and business planning.', '10.', 'Brand Reputation and Trust:', 'Impact:', 'Adoption of fairness-aware and transparent AI models.', 'Benefit:', 'Builds trust among clients and stakeholders by demonstrating a commitment to fairness, transparency, and ethical AI practices.', '11.', 'Risk Mitigation for Clients:', 'Impact:', 'Providing insurance coverage that reflects the evolving nature of insider trading public lawsuits.', 'Benefit:', 'Assists Public Company Directors in mitigating financial risks associated with legal actions, enhancing the value proposition for clients.', '12.', 'Scalability and Future-Proofing:', 'Impact:', 'Designing the solution to scale and adapt to future industry developments.', 'Benefit:', 'Ensures the longevity and relevance of the insurance firm’s technology infrastructure in the face of evolving business and technological landscapes.', '13.', 'Revenue Growth:', 'Impact:', 'Attracting a larger customer base and retaining existing clients through innovative and accurate insurance solutions.', 'Benefit:', 'Contributes to revenue growth by expanding the firm’s market share and increasing customer loyalty.', 'By recognizing and leveraging these business impacts, the leading insurance firm can derive significant value from the implementation of an ML and AI-based insurance premium model tailored for Public Company Directors in the USA.', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Rise of cybercrime and its effect by the year 2040.', 'Next article', 'Database Discovery Tool using OpenAI', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2012,https://insights.blackcoffer.com/streamlined-integration-interactive-brokers-api-with-python-for-desktop-trading-application/,"Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application - Blackcoffer Insights-['Client Background', 'Client:', 'A leading fintech firm in the USA', 'Industry Type:', 'Finance', 'Products & Services:', 'Trading, Banking, Financing', 'Organization Size:', '100+', 'The Problem', 'Integrating the Interactive Brokers API with Python.', 'Creating a user-friendly desktop application interface.', 'Managing concurrent processes and threads.', 'Developing the margin calculator with accurate calculations.', 'Handling data synchronization between TWS and the application.', 'Ensuring security and authentication for TWS access.', 'Providing real-time market data to users.', 'Maintaining a responsive and reliable application.', 'Resolving any potential compatibility issues.', 'Ensuring thorough documentation for users', 'Our Solution', 'Leverage Interactive Brokers API documentation and libraries.', 'Design an intuitive and responsive PyQT5-based desktop UI.', 'Implement threading and preprocessing for concurrent tasks.', 'Develop a robust margin calculator algorithm.', 'Use data synchronization mechanisms provided by TWS.', 'Implement secure authentication for TWS access.', 'Utilize the Interactive Brokers API for real-time market data.', 'Conduct extensive testing and quality assurance.', 'Address compatibility issues through rigorous testing.', 'Document every aspect of the project for users and developers.', 'Solution Architecture', 'Interactive Brokers API for live data and trading access.', 'Python-based server using Django for APIs and data storage.', 'PyQT5-based desktop application for trading dashboard.', 'PostgreSQL database for storing relevant data.', 'Threading and concurrency management for parallel processes.', 'Margin calculator component within the desktop app.', 'Integration with Trader Workstation (TWS).', 'Real-time market data feeds from TWS.', 'Responsive front-end using Bootstrap, HTML, and CSS.', 'Detailed documentation for users and developers.', 'Deliverables', 'Project Github Source Code :', 'https://github.com/AjayBidyarthy/Sunil-Misir', 'Tech Stack', 'Tools used', 'Requests', 'Threading and Multiprocessing', 'PyQT5', 'Language/techniques used', 'Python', 'Models used', 'Django ORM', 'Skills used', 'Python', 'Python Django', 'Python Django REST Framework', 'PyQT5', 'MultiThreading and MultiProcessing', 'Databases used', 'POstgresql', 'Web Cloud Servers used', 'None', 'What are the technical Challenges Faced during Project Execution', 'Complex integration with the Interactive Brokers API.', 'Designing an efficient and user-friendly desktop interface.', 'Coordinating and managing multiple concurrent threads and processes.', 'Accurate implementation of the margin calculator.', 'Ensuring real-time data synchronization with TWS.', 'Handling authentication and security for TWS access.', 'Providing timely and reliable market data.', 'Resolving compatibility issues on various user machines.', 'Optimizing performance for a responsive application.', 'Documenting every aspect comprehensively.', 'How the Technical Challenges were Solved', 'Extensive research and consultation of Interactive Brokers API documentation.', 'User-centered design principles for the desktop interface.', 'Thorough testing and debugging of multi-threading scenarios.', 'Careful design and testing of margin calculation algorithms.', 'Regular data synchronization checks with TWS.', 'Implementation of secure authentication protocols.', 'Utilization of Interactive Brokers’ data streaming features.', 'Compatibility testing on various configurations.', 'Profiling and optimization of code for responsiveness.', 'Comprehensive documentation created throughout the development process.', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment', 'Next article', 'Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2013,https://insights.blackcoffer.com/efficient-data-integration-and-user-friendly-interface-development-navigating-challenges-in-web-application-deployment/,"Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Products & Services:', 'IT Consulting', 'Organization Size:', '100+', 'The Problem', 'Data Complexity:', 'Handling and integrating multiple data sources with different formats and cleaning/preprocessing them for use in a web application.', 'Spatial Data Integration:', 'Managing and converting complex spatial data into a suitable format for storage and display.', 'User-Friendly Data Access:', 'Providing an easy-to-use interface for users to query and visualize data efficiently.', 'Secure Authentication:', 'Implementing secure user authentication to protect sensitive data and user accounts.', 'Deployment Considerations:', 'Exploring the potential challenges of deploying the application on Azure.', 'Our Solution', 'Project Setup and ETL:', 'Set up Django, developed ETL scripts, cleaned data, and loaded it into PostgreSQL.', 'Web Application Development:', 'Designed user-friendly templates, implemented APIs for data display, and used session storage for queries.', 'User Authentication:', 'Created login/signup pages and implemented secure user authentication.', 'Data Management and Integration:', 'Ensured dynamic tables and error handling for queries, created Docker image, and documented deployment.', 'Spatial Data Handling:', 'Processed and stored spatial data, integrated it with Django views, and converted data types.', 'API Development:', 'Built APIs for JSON data retrieval and handled various file extensions for data extraction.', 'Frontend and User Interaction:', 'Designed frontend components and implemented data upload and retrieval.', 'SQL Dump and Azure Deployment:', 'Created SQL Dump template, developed a view for uploading .sql files, and explored Azure deployment options.', 'Solution Architecture', 'Backend Framework:', 'Python Django for building the web application’s backend.', 'Database:', 'PostgreSQL for storing cleaned and spatial data.', 'ETL Processes:', 'Python scripts for data extraction, transformation, and loading.', 'Frontend:', 'HTML templates and JavaScript for user interaction.', 'APIs:', 'Custom APIs for data retrieval and spatial data handling.', 'Deployment:', 'Dockerization for containerized deployment.', 'Authentication:', 'Implementing user authentication using Django’s built-in features.', 'Spatial Data Handling:', 'Using Python libraries to process and convert spatial data.', 'SQL Dump:', 'Creating an SQL Dump feature for running PostgreSQL queries.', 'Deliverables', 'Project Resouces well be access via github Only', 'Github Link :', 'https://github.com/AjayBidyarthy/Sheeban-Wasi-Full-stack.git', 'Tech Stack', 'Tools used', 'Pillow', 'psycopg2', 'arcgis==1.8.2', 'geopandas', 'pyproj', 'pandas', 'numpy', 'matplotlib', 'pyshp', 'Language/techniques used', 'Python', 'Models used', 'Django ORM', 'Skills used', 'Python', 'Django', 'ETL', 'Docker', 'Databases used', 'postgresql', 'Web Cloud Servers used', 'MS Azure', 'What are the technical Challenges Faced during Project Execution', 'Data Cleaning and Integration:', 'Managing data from different sources and ensuring consistency was challenging.', 'Spatial Data Transformation:', 'Converting complex spatial data into suitable database formats posed a technical hurdle.', 'User Authentication:', 'Implementing secure user authentication without vulnerabilities required careful consideration.', 'File Handling:', 'Handling various file extensions and extracting data from them was a technical challenge.', 'Deployment:', 'Ensuring smooth deployment, especially on Azure, presented its own set of challenges.', 'How the Technical Challenges were Solved', 'Data Cleaning and Integration:', 'Python scripts were used to clean and preprocess data, aligning it with column datatypes.', 'Spatial Data Transformation:', 'Libraries were utilized to process and convert spatial data to appropriate formats.', 'User Authentication:', 'Django’s built-in authentication features were leveraged for secure user management.', 'File Handling:', 'Custom Python scripts were developed to handle different file extensions and extract data.', 'Deployment:', 'Dockerization simplified deployment, and research on Azure ensured potential future deployment options were explored.', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Effective Management of Social Media Data Extraction: Strategies for Authentication, Security, and Reliability', 'Next article', 'Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2014,https://insights.blackcoffer.com/effective-management-of-social-media-data-extraction-strategies-for-authentication-security-and-reliability/,"Effective Management of Social Media Data Extraction: Strategies for Authentication, Security, and Reliability - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Products & Services:', 'Consulting, Product & Services', 'Organization Size:', '100+', 'The Problem', 'Handling complex authentication mechanisms for social media platforms.', 'Efficiently extracting data from social media profiles.', 'Preventing IP blocking and ensuring API reliability.', 'Managing and storing extracted data securely.', 'Abiding by social media platform policies and avoiding legal issues.', 'Handling rate limiting and throttling.', 'Providing comprehensive and up-to-date documentation.', 'Dealing with changes in social media platform APIs.', 'Optimizing API performance for rapid response.', 'Ensuring user privacy and data protection.', 'Our Solution', 'Implement OAuth2 or API tokens for authentication.', 'Utilize web scraping libraries like BeautifulSoup and Scrapy.', 'Employ proxy rotation and request throttling.', 'Use databases like MongoDB or AWS S3 for data storage.', 'Regularly check and update API usage against platform policies.', 'Implement rate limiting and queue-based processing.', 'Maintain versioned API documentation.', 'Monitor platform API changes and adapt accordingly.', 'Optimize code and database queries for performance.', 'Encrypt sensitive data and follow data protection regulations.', 'Solution Architecture', 'Authentication layer for social media logins.', 'API endpoints for data extraction.', 'Web scraping components for profile details.', 'Throttling and rate-limiting mechanisms.', 'Data storage and caching layers.', 'Documentation portal for API users.', 'Monitoring and logging infrastructure.', 'Error handling and alerting mechanisms.', 'Compliance checks and privacy safeguards.', 'Load balancers and auto-scaling for API servers.', 'Deliverables', 'Project Github Source Code', 'Tech Stack', 'Tools used', 'BeautifulSoup', 'Requests', 'Django rest Framework', 'Language/techniques used', 'Python', 'Models used', 'Django ORM', 'Skills used', 'Python', 'WebScraping', 'Python Django', 'Python Django REST Framework', 'Databases used', 'SQLite Database', 'Web Cloud Servers used', 'None', 'What are the technical Challenges Faced during Project Execution', 'Frequent changes and updates to social media APIs.', 'Evolving security and authentication requirements.', 'Handling CAPTCHAs and bot detection mechanisms.', 'Maintaining data consistency and accuracy.', 'Adhering to rate limits and avoiding IP blocks.', 'Scaling the infrastructure to accommodate increased usage.', 'Dealing with diverse data formats from different platforms.', 'Ensuring privacy and compliance with data protection laws.', 'Balancing performance and cost-effectiveness.', 'Handling user-specific customizations and options.', 'How the Technical Challenges were Solved', 'Regularly monitoring and adapting to API changes.', 'Implementing robust authentication strategies.', 'Using CAPTCHA solving services when necessary.', 'Implementing data validation and cleansing routines.', 'Employing IP rotation and rate limiting strategies.', 'Utilizing cloud-based auto-scaling solutions.', 'Developing data parsers for various formats.', 'Implementing encryption and anonymization techniques.', 'Profiling and optimizing code for performance.', 'Providing configurable options for users to customize their data extraction.', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Streamlined Trading Operations Interface for MetaTrader 4: Empowering Efficient Management and Monitoring', 'Next article', 'Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2015,https://insights.blackcoffer.com/streamlined-trading-operations-interface-for-metatrader-4-empowering-efficient-management-and-monitoring/,"Streamlined Trading Operations Interface for MetaTrader 4: Empowering Efficient Management and Monitoring - Blackcoffer Insights-['Client Background', 'Client:', 'A leading fintech firm in the USA', 'Industry Type:', 'Finance', 'Products & Services:', 'Trading, Investment, Financing', 'Organization Size:', '100+', 'The Problem', 'Trading Operations Interface:', 'The project aims to create a Windows-based Display Application that provides an intuitive interface for managing trading activities in MetaTrader 4 (MT4).', 'EA Control and Monitoring:', 'Users need a tool to interact with and monitor the EA running in MT4, which follows predefined rules for trading.', 'Hedging and Configuration:', 'The application should allow users to hedge positions, configure trading settings, close orders manually, and add new orders.', 'Real-time Monitoring:', 'Real-time monitoring and control of trading activities are crucial for efficient trading.', 'EA Functionality:', 'The specific functionality and rules of the EA will be defined based on pricing and requirements.', 'Our Solution', 'Display Application:', 'Developed a Windows-based application for trading operations management, order placement, and monitoring.', 'EA Interaction:', 'Created a loosely coupled system where the Display Application can control and influence the EA running on MT4.', 'Functionality:', 'Implemented order placement, hedging, settings configuration, order closing, and real-time monitoring features.', 'Dynamic EA:', 'The EA’s specific rules and functionality are determined based on pricing and requirements.', 'Communication:', 'Established a mechanism for the Display Application to communicate with MT4, facilitating trading instructions and updates.', 'Solution Architecture', 'UI Development:', 'UI development using Python libraries such as Kivy and Tkinter for the Windows-based application.', 'VPS with MT4:', 'The client operates a Virtual Private Server (VPS) with MT4 running.', 'Expert Advisor (EA):', 'An EA on MT4 executes trading operations based on predefined rules.', 'Communication:', 'A mechanism for the Display Application to communicate with MT4, possibly through an API or other methods.', 'Dynamic EA Parameters:', 'The exact rules and functionality of the EA will be determined based on pricing and client requirements.', 'Deliverables', 'Project Code can be accessed via this github link :', 'https://github.com/AjayBidyarthy/Patrick-Oliveri-Applcation.git', 'Since, this is private Git Reporsitory , User will need permission to clone it.', 'Tech Stack', 'Tools used', 'TKinter', 'Kivy', 'Language/techniques used', 'Python', 'Models used', 'No Model Used', 'Skills used', 'Python Kivy', 'Python TKinter', 'Databases used', 'No Db Used', 'Web Cloud Servers used', 'No Web Services Used', 'What are the technical Challenges Faced during Project Execution', 'UI Responsiveness:', 'Challenges in achieving responsive UI in Python libraries like Kivy and Tkinter.', 'Integration with MT4:', 'Ensuring effective communication between the Display Application and MT4.', 'Dynamic EA Rules:', 'Defining and integrating dynamic rules for the EA based on user requirements.', 'Deployment:', 'Preparing for potential deployment but no deployment has occurred yet.', 'Version Control:', 'Managing code changes and documentation using Git.', 'How the Technical Challenges were Solved', 'UI Responsiveness:', 'The project transitioned to seek C or C# development for better responsiveness and flexibility.', 'Integration with MT4:', 'A communication mechanism, possibly an API, was explored to facilitate communication between the Display Application and MT4.', 'Dynamic EA Rules:', 'The exact rules for the EA were to be determined based on client requirements and pricing, ensuring flexibility.', 'Deployment:', 'Deployment has not occurred yet, and it may be addressed in the future.', 'Version Control:', 'Git was used to manage code changes and documentation, ensuring version control and collaboration.', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Efficient AWS Infrastructure Setup and Management: Addressing Security, Scalability, and Compliance', 'Next article', 'Effective Management of Social Media Data Extraction: Strategies for Authentication, Security, and Reliability', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2016,https://insights.blackcoffer.com/efficient-aws-infrastructure-setup-and-management-addressing-security-scalability-and-compliance/,"Efficient AWS Infrastructure Setup and Management: Addressing Security, Scalability, and Compliance - Blackcoffer Insights-['Client Background', 'Client:', 'A leading Consulting firm in the USA', 'Industry Type:', 'IT', 'Products & Services:', 'IT Consulting', 'Organization Size:', '1000+', 'The Problem', 'Setting up and configuring AWS services.', 'Designing an efficient database schema.', 'Integrating email and calling services securely.', 'Ensuring data privacy and compliance.', 'Handling system scalability.', 'Managing user authentication and authorization.', 'Monitoring and logging system activities.', 'Implementing backup and recovery strategies.', 'Debugging and troubleshooting issues.', 'Balancing cost and performance.', 'Our Solution', 'Utilize AWS CloudFormation or AWS CDK for infrastructure as code.', 'Normalize the database schema to minimize redundancy.', 'Implement OAuth or JWT for secure authentication.', 'Encrypt data at rest and in transit.', 'Use AWS Auto Scaling to handle increased traffic.', 'Set up AWS CloudWatch for monitoring and AWS CloudTrail for auditing.', 'Regularly backup data to Amazon S3.', 'Implement comprehensive error handling and logs.', 'Perform unit, integration, and load testing.', 'Optimize AWS resource usage through cost analysis.', 'Solution Architecture', 'AWS RDS for customer and employee data storage.', 'AWS Lambda functions for processing calls and emails.', 'AWS SES and SNS for sending emails and notifications.', 'Amazon S3 for storing backups and static assets.', 'AWS Cognito for user authentication.', 'AWS API Gateway for managing APIs.', 'AWS CloudWatch and CloudTrail for monitoring and auditing.', 'AWS Auto Scaling for handling variable workloads.', 'Python codebase for application logic.', 'Implementing security groups and VPC for network isolation.', 'Deliverables', 'Project Github Source Code', 'Tech Stack', 'Tools used', 'Requests', 'Boto3', 'Language/techniques used', 'Python', 'Models used', 'None', 'Skills used', 'Python', 'AWS', 'Databases used', 'AWS RDS', 'Web Cloud Servers used', 'Amazon Web Services (AWS)', 'What are the technical Challenges Faced during Project Execution', 'Integrating multiple AWS services.', 'Designing a scalable database schema.', 'Ensuring data security and compliance.', 'Handling complex user authentication and authorization.', 'Managing API versioning and changes.', 'Optimizing cost and resource usage.', 'Debugging and resolving performance issues.', 'Maintaining high availability and reliability.', 'Handling data synchronization between tiers.', 'Adapting to evolving AWS services and best practices.', 'How the Technical Challenges were Solved', 'Extensive research and leveraging AWS documentation and support.', 'Collaboration with experienced database architects.', 'Thorough security audits and compliance checks.', 'Implementing OAuth and fine-grained access control.', 'Clear versioning and documentation for APIs.', 'Regular cost analysis and optimization efforts.', 'Profiling and performance tuning of critical components.', 'Implementing redundancy and failover mechanisms.', 'Developing data synchronization algorithms.', 'Continuous learning and adaptation to AWS updates and community insights.', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Streamlined Equity Waterfall Calculation and Deal Management System', 'Next article', 'Streamlined Trading Operations Interface for MetaTrader 4: Empowering Efficient Management and Monitoring', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2017,https://insights.blackcoffer.com/streamlined-equity-waterfall-calculation-and-deal-management-system/,"Streamlined Equity Waterfall Calculation and Deal Management System - Blackcoffer Insights-['Client Background', 'Client:', 'A leading real estate firm in the USA', 'Industry Type:', 'Real Estate', 'Products & Services:', 'Real Estate, Construction, Financing', 'Organization Size:', '100+', 'The Problem', 'Calculating equity waterfalls based on CSV data.', 'Implementing different user roles and their permissions.', 'Creating a user-friendly dashboard for each user type.', 'Managing deal creation, invitations, and subscriptions.', 'Handling user invitations and registration.', 'Copying deals while preserving specific data.', 'Our Solution', 'Develop Python code to calculate equity waterfalls.', 'Implement role-based access control for admin, sponsors, and investors.', 'Create distinct dashboards with relevant data using ReactJS.', 'Design intuitive UI for deal management.', 'Develop invitation mechanisms and registration flows.', 'Implement copying deals with proper data handling.', 'Solution Architecture', 'Backend built with Django for handling data, authentication, and API endpoints.', 'Frontend developed using ReactJS for user interfaces.', 'SQLite database for data storage.', 'Google Cloud for application deployment.', 'Deliverables', 'Project Github Source Code :', 'Tech Stack', 'Tools used', 'Pillow', 'Requests', 'GCP VM', 'Language/techniques used', 'Python', 'React JS', 'Models used', 'Django ORM', 'Skills used', 'Python', 'Python Django', 'Python Django REST Framework', 'Databases used', 'SQLite3 database', 'Web Cloud Servers used', 'GCP', 'What are the technical Challenges Faced during Project Execution', 'Equity waterfall calculations based on dynamic CSV data.', 'Managing user permissions and access control.', 'Designing and implementing complex user registration and invitation flows.', 'Copying deals while maintaining data integrity.', 'Ensuring data consistency and security.', 'How the Technical Challenges were Solved', 'Developed Python scripts to parse CSV files and perform required calculations.', 'Utilized Django’s built-in authentication system and implemented role-based permissions.', 'Designed clear and user-friendly registration and invitation processes.', 'Implemented a controlled deal copying mechanism.', 'Conducted thorough testing and used encryption for data security.', 'Project website url :', 'https://stackshares.io/', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Automated Orthopedic Case Report Generation: Harnessing Web Scraping and AI Integration', 'Next article', 'Efficient AWS Infrastructure Setup and Management: Addressing Security, Scalability, and Compliance', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2018,https://insights.blackcoffer.com/automated-orthopedic-case-report-generation-harnessing-web-scraping-and-ai-integration/,"Automated Orthopedic Case Report Generation: Harnessing Web Scraping and AI Integration - Blackcoffer Insights-['Client Background', 'Client:', 'A leading health-tech firm in the USA', 'Industry Type:', 'Healthcare', 'Products & Services:', 'Medical solutions, healthcare', 'Organization Size:', '100+', 'The Problem', 'The problem is to efficiently create orthopedic case reports by extracting data from online sources, including articles, videos, and user comments.', 'It involves summarizing and citing relevant articles from PubMed.gov for the past 5 years related to the case.', 'This requires automating the extraction and summarization of data from websites, making it a time-consuming task if done manually.', 'Our Solution', 'Develops a Python tool that accepts a website URL as input and generates a case report.', 'Integrates web scraping to extract data from websites.', 'Utilizes AI, such as ChatGPT, for creating summaries and responses.', 'Leverages PubMed for citing and summarizing recent articles.', 'Provides a web application for user-friendly access to these capabilities.', 'Solution Architecture', 'Utilizes web scraping techniques to gather data from trusted medical websites.', 'Combines web scraping with AI, including ChatGPT, for generating case reports and responding to queries.', 'Utilizes PubMed for retrieving and summarizing recent articles related to the case.', 'Deploys a web application for user interaction and input.', 'Deliverables', 'Project Github Source Code', 'Tech Stack', 'Tools used', 'ChatGPT', 'BeautifulSoup', 'Requests', 'Language/techniques used', 'Python', 'Models used', 'None', 'Skills used', 'Python', 'WebScraping', 'ChatGPT prompting', 'Databases used', 'None', 'Web Cloud Servers used', 'None', 'What are the technical Challenges Faced during Project Execution', 'Accurate and reliable web scraping from diverse medical websites.', 'Integration of AI components for text generation and summarization.', 'Efficient querying and retrieval of articles from PubMed.', 'Handling different data formats and structures from various online sources.', 'Developing a user-friendly web interface for input and interaction.', 'How the Technical Challenges were Solved', 'Extensive research and testing of web scraping techniques for medical websites.', 'Integration of AI models and libraries for text generation.', 'Utilization of PubMed API for article retrieval and summarization.', 'Custom data parsers for handling diverse data structures.', 'Collaboration with medical experts for user interface design and feedback.', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Streamlining Time Calculation in Warehouse Management: Leveraging ShipHero API and Google BigQuery Integration', 'Next article', 'Streamlined Equity Waterfall Calculation and Deal Management System', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2019,https://insights.blackcoffer.com/streamlining-time-calculation-in-warehouse-management-leveraging-shiphero-api-and-google-bigquery-integration/,"Streamlining Time Calculation in Warehouse Management: Leveraging ShipHero API and Google BigQuery Integration - Blackcoffer Insights-['Client Background', 'Client:', 'A leading retail firm in the USA', 'Industry Type:', 'Retail', 'Products & Services:', 'Retail Solutions, Supply Chain, Warehouse Management', 'Organization Size:', '100+', 'The Problem', 'The problem was to efficiently calculate the time taken by each personnel on their shifts in a warehouse management system.', 'Data needed to be extracted from ShipHero API and processed to generate meaningful insights.', 'There was a need for a web interface to provide user-friendly access to the data and allow for data filtering.', 'There is a mapping issue in Python Script which occurred in December of 2022. Maybe due to the addition of another warehouse. This is an open issue and ShipHero is unable to provide any reliable solution for the same. [Issue has been highlighted below in the section of ‘’Known Issues’’]', 'Our Solution', 'Creating an API to Google BigQuery using a Python script deployed on Google Cloud.', 'The Python script automated data extraction from ShipHero API, transformation, and loading into Google BigQuery.', 'Google Data Studio was used to create a dashboard for reporting and visualization.', 'Solution Architecture', 'The solution involved two main components: a Python script and a web interface (Web App).', 'The Python script utilized ShipHero API to fetch data and calculate personnel shift times. It then stored the processed data in Google BigQuery.', 'The web interface allowed users to log in, apply filters to data tables fetched from BigQuery, and visualize the data.', 'Google Cloud services were used for hosting the Python script and deploying the web app.', 'Deliverables', '[GitHub Repositories URL:', 'https://github.com/AjayBidyarthy/Jake-Brenner-API-to-google-big-query-to-google-data-studio', '.', 'https://github.com/AjayBidyarthy/Jake-Brenner-frontend/tree/himanshu', 'https://github.com/AjayBidyarthy/Jake-Brenner-frontend/tree/master', 'Tech Stack', 'Tools used', 'Google API', 'Beautifulsoup', 'Numpy and Pandas', 'Language/techniques used', 'Python', 'React JS', 'Models used', 'Django ORM Model', 'Skills used', 'Python', 'Python Django', 'React JS', 'Databases used', 'GCP BigQuery Database', 'Web Cloud Servers used', 'Google Cloud Platform (GCP)', 'What are the technical Challenges Faced during Project Execution', 'Accessing and understanding ShipHero API endpoints and data structures.', 'Developing and deploying the Python script to run daily on Google Cloud Scheduler.', 'Integrating and linking databases effectively.', 'Handling and automating complex data manipulation and calculations.', 'How the Technical Challenges were Solved', 'Comprehensive research and analysis of the ShipHero API and its endpoints.', 'The Python script was developed to handle data extraction, transformation, and loading tasks efficiently.', 'Google Cloud services were used to automate the script and schedule daily runs.', 'Collaboration and communication with the client to ensure the API data met the dashboard requirements.', 'Project website url', 'http://app.shiphero.com/', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Efficient Database Design and Management: Streamlining Access and Integration for Partner Entity Management', 'Next article', 'Automated Orthopedic Case Report Generation: Harnessing Web Scraping and AI Integration', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2020,https://insights.blackcoffer.com/efficient-database-design-and-management-streamlining-access-and-integration-for-partner-entity-management/,"Efficient Database Design and Management: Streamlining Access and Integration for Partner Entity Management - Blackcoffer Insights-['Client Background', 'Client:', 'A leading IT firm in the Europe', 'Industry Type:', 'IT', 'Products & Services:', 'IT Services, Consulting and Automation', 'Organization Size:', '100+', 'The Problem', 'Database designing which enables access to each related/important table data via other db table', 'The project required the development of a user-friendly web application for managing partner entities with diverse attributes.', 'Ensuring data accuracy, security, scalability, and compliance with regulations while integrating seamlessly with a Data Warehouse posed significant technical challenges.', 'Our Solution', 'Our solution successfully addressed the technical challenges by leveraging Django’s capabilities and implementing custom solutions where needed.', 'It provided a robust and scalable web application for partner entity management while ensuring data accuracy, security, and compliance.', 'The dynamic attribute management system and integration with the Database facilitated efficient data handling and reporting, supporting data-driven decision-making.', 'We have designed and implemented database related changes and UI related changes that Client have suggested according to which a separate db table which contains all data which will be created , updated and deleted as other table rows created, updated and deleted', 'Solution Architecture', 'Django ORM for abstracting database complexities.', 'Scalability through cloud resources and optimization techniques.', 'Security measures, including encryption and access controls for Admin Users.', 'Performance optimization strategies such as removing redundancy in db tables.', 'We have provided many database design solutions as well as User Interface solution regarding which client have given positive response.', 'We have successfully developed and implemented design and changes related to project after multiple discussion with client regarding database architecture design as well as database model and their related UI panel with authentication', 'Deliverables', 'Python Django Source Code (Github Repository)', 'Tech Stack', 'Tools used', 'Python Django web Framework', 'Language/techniques used', 'Python', 'Models used', 'Django Database Model and Django ORM', 'Skills used', 'Python', 'Django', 'Databases used', 'Postgresql', 'Web Cloud Servers used', 'Not Used from Side', 'What are the technical Challenges Faced during Project Execution', 'Database Complexity', ': Designing a comprehensive database schema to represent multiple partner entities with varying attributes posed a challenge. Each entity had unique characteristics and relationships.', 'Scalability', ': Ensuring the application’s scalability to handle a potentially large volume of partner data while maintaining performance was a significant concern.', 'Dynamic Attributes', ': Allowing users to dynamically manage entity attributes presented difficulties in database design and user interface implementation.', 'Data Validation', ': Implementing robust data validation rules to maintain data accuracy and consistency across various partner entities was complex due to the diversity of data.', 'Integration with Remote Database', ': Establishing seamless data export capabilities to feed the Database while maintaining data compatibility was a technical hurdle.', 'Security', ': Ensuring data security and compliance with relevant regulations, including encryption and access control, required careful consideration and implementation.', 'Performance Optimization', ': Optimizing the application’s performance, especially when dealing with complex queries and large datasets, was a continual challenge.', 'How the Technical Challenges were Solved', 'Database Abstraction', ': Utilizing Django’s ORM (Object-Relational Mapping) allowed for an abstract representation of entities and their attributes, simplifying database management.', 'Scalability Planning', ': Employing efficient indexing and caching mechanisms to accommodate scalability and performance needs. Additionally, using cloud resources for scalability.', 'User Management', ': Implementing a flexible User management system that allowed users to Create , Read , Update and Delete other Users and their related permissions.', 'Data Validation Middleware', ': Developing custom middleware to enforce data validation rules and ensure data accuracy before database interactions.', 'Integration Layer', ': Creating a dedicated integration layer that transformed and exported data from the database to the User Interface, adhering to data compatibility standards.', 'Security Best Practices', ': Adhering to best practices for securing data, including User Authentication, Changes in Django template to Remove Other Important Database in db options and Permission Required for other User to use database table on UI panel from Admin User.', 'Performance Tuning', ': Conducting performance tuning by optimizing database model and related admin file for better fetching of db table data on UI panel.', 'Project website url', 'http://34.18.45.30:8000/api/admin/login/?next=/api/admin/', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Automated Campaign Management System: A Comprehensive Solution with LinkedIn and Email Integration', 'Next article', 'Streamlining Time Calculation in Warehouse Management: Leveraging ShipHero API and Google BigQuery Integration', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2021,https://insights.blackcoffer.com/automated-campaign-management-system-a-comprehensive-solution-with-linkedin-and-email-integration/,"Automated Campaign Management System: A Comprehensive Solution with LinkedIn and Email Integration - Blackcoffer Insights-['Client Background', 'Client:', 'A leading marketing tech firm worldwide', 'Industry Type:', 'Marketing', 'Products & Services:', 'Ad Tech, Marketing Automation, Lead Management', 'Organization Size:', '100+', 'The Problem', 'Integrating with LinkedIn and Email APIs for automation.', 'Building a user-friendly and responsive frontend interface.', 'Developing a robust backend code for campaign automation.', 'Ensuring secure user authentication and data exchange.', 'Managing campaign creation, scheduling, and tracking.', 'Handling data storage and organization in MongoDB.', 'Providing comprehensive documentation for users.', 'Ensuring scalability and reliability in cloud hosting.', 'Addressing security and privacy concerns.', 'Maintaining ongoing support and updates.', 'Our Solution', 'Leverage LinkedIn and Email API documentation and libraries.', 'Implement a responsive and intuitive frontend using React.js.', 'Develop backend code for campaign automation with Python or Node.js.', 'Utilize secure authentication mechanisms like JWT.', 'Create user-friendly campaign creation and management interfaces.', 'Store and manage campaign data efficiently in MongoDB.', 'Produce detailed documentation for installation, usage, and troubleshooting.', 'Employ AWS for scalable and reliable cloud hosting.', 'Implement encryption and privacy measures.', 'Establish a support and maintenance plan for ongoing updates.', 'Solution Architecture', 'Frontend built with React.js.', 'Backend using Python or Node.js.', 'MongoDB for data storage and management.', 'AWS for cloud hosting and scalability.', 'Integration with LinkedIn and Email APIs.', 'User authentication and authorization layers.', 'Campaign creation, scheduling, and tracking features.', 'Responsive and user-friendly web app interface.', 'Documentation portal for users and developers.', 'Security and privacy measures integrated throughout the architecture.', 'Tech Stack', 'Tools used', 'LinkedIn API', 'Gmail API', 'Google Account', 'Selenium', 'BeautifulSoup', 'requests', 'Language/techniques used', 'Python', 'React JS', 'Models used', 'Django ORM', 'Skills used', 'Python', 'WebScraping', 'React JS', 'Selenium', 'Django', 'Django rest framework', 'Databases used', 'MongoDb', 'Web Cloud Servers used', 'AWS', 'What are the technical Challenges Faced during Project Execution', 'Complex integration with LinkedIn and Email APIs.', 'Designing and implementing a responsive frontend.', 'Developing robust automation logic for campaigns.', 'Ensuring secure authentication and data exchange.', 'Managing large datasets and data organization in MongoDB.', 'Creating comprehensive and user-friendly documentation.', 'Scaling the application for cloud hosting.', 'Addressing security and privacy concerns.', 'Handling user support requests and bug fixes.', 'Keeping the application up to date with API changes.', 'How the Technical Challenges were Solved', 'Thoroughly studied LinkedIn and Email API documentation.', 'Used React.js and responsive design practices for the frontend.', 'Implemented efficient campaign automation logic.', 'Employed JWT for secure user authentication.', 'Optimized data storage and retrieval in MongoDB.', 'Prepared detailed documentation for users and developers.', 'Utilized AWS services for scalable hosting.', 'Implemented encryption and privacy measures.', 'Established a support system for user inquiries.', 'Maintained active monitoring of API changes and regular updates.', 'Project website url :', 'Frontend :', 'http://35.176.216.54:3000/', 'Backend :', 'http://35.176.216.54:8000/', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'AI-driven data analysis AI tool using Langchain for a leading real estate and financing firm worldwide', 'Next article', 'Efficient Database Design and Management: Streamlining Access and Integration for Partner Entity Management', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2022,https://insights.blackcoffer.com/ai-driven-data-analysis-ai-tool-using-langchain-for-a-leading-real-estate-and-financing-firm-worldwide/,"AI-driven data analysis AI tool using Langchain for a leading real estate and financing firm worldwide - Blackcoffer Insights-['Client Background', 'Client:', 'A leading real estate and financing firm worldwide', 'Industry Type:', 'Real Estate', 'Products & Services:', 'Infrastructure Development, Financing, Real Estate', 'Organization Size:', '10000+', 'The Problem', 'Creating a user-friendly data analysis tool capable of interpreting natural language queries and providing insightful analyses from CSV data. The tool should facilitate seamless interaction, enabling users to gain valuable insights without the need for technical expertise. Key functionalities should include data exploration, trend identification, pattern recognition, and anomaly detection, all presented in a comprehensible format. The tool must also ensure efficient handling of CSV datasets while maintaining accuracy and reliability in its analyses.', 'Our Solution', 'Data Ingestion and Conversion:', 'CSV data is acquired from a source (local file system, cloud storage, etc.).', 'The data is then converted into a pandas DataFrame using the read_csv() function or similar methods provided by the pandas library.', 'Data Cleaning:', 'Data Cleaning operations are performed on the dataframe so that it serves as an ideal input for Pandas Agent. These may include:', 'Column Data type conversion.', 'Handling Duplicates', 'Handling unnecessary columns, etc.', 'Initialization of Langchain’s Pandas Agent:', 'Langchain’s Pandas Agent is initialized with the necessary parameters. These parameters include:', 'System prompt: A custom prompt provided by the user or defined in the application.', 'Temperature: A parameter controlling the randomness of the model’s outputs.', 'Model: The specific model or model configuration to be used by the agent.', 'Other relevant parameters based on the requirements and capabilities of the agent.', 'Integration with Pandas DataFrame:', 'The DataFrame created in the previous step serves as input for the Pandas Agent. It contains the structured data which will serve as input for the Pandas Agent.', 'Natural Language Query Interpretation:', 'The user interacts with the system by posing queries in natural language.', 'Langchain’s Pandas Agent interprets these queries using GPT-4 backend and converts them into executable commands or operations on the DataFrame.', 'DataFrame Operations:', 'The Pandas Agent executes the operations needed on the DataFrame. These operations may include:', 'Filtering', ': Selecting rows or columns based on specified criteria.', 'Aggregation', ': Computing summary statistics or aggregating data based on groups.', 'Transformation', ': Modifying data in the DataFrame (e.g., adding or removing columns, changing data types).', 'Joining/Merging', ': Combining multiple DataFrames based on common keys or indices.', 'Sorting', ': Arranging rows or columns in a specified order.', 'Other pandas DataFrame operations as required by the user queries.', 'Delivery to End User:', 'The processed output is delivered to the end user through the', 'streamlit', 'user interface.', 'The user can review the insights provided by the system and further refine their queries if needed.', 'Solution Architecture', 'Deliverables', 'Data Analysis Tool with Streamlit frontend.', 'Tech Stack', 'Tools used', 'Langchain, OpenAI gpt-4 API', 'Language/techniques used', 'Python', 'Models used', 'Pandas Agent, GPT-4', 'Skills used', 'Python, Streamlit, Streamlit cloud deployment, Langchain', 'Web Cloud Servers used', 'Streamlit cloud', 'What are the technical Challenges Faced during Project Execution', 'To make the tool follow the Indian standards in terms of Financial Year Quarters, currency and human readable values instead of exponential values.', 'How the Technical Challenges were Solved', 'The challenge was solved by decreasing the temperature of Pandas agent to 0 and make a custom system prompt to introduce maximum bias approximating the desirable answers.', 'Business Impact', 'The user was able get data analysis insights without expertise in python, pandas and other tools used in the process of Data Analysis in a fraction of time compared to what it would have been if the process was done manually.', 'Project Snapshots', 'Frontend Streamlit Interface', 'IDE Environment', 'Project website url', 'URL: https://app-test-pandas-agent-vjbjfjkmxfrvhkhc455p4k.streamlit.app/', '(Non-Functional due to the expiry of OpenAI API Key)', 'Project Video', 'Link:', 'https://www.loom.com/share/c2099f20e9214e18a2125f5b2fde794c?sid=faa8cc4b-001c-4c51-926c-6a551dfb7c63', 'Important Links', 'Video Demo: https://www.loom.com/share/c2099f20e9214e18a2125f5b2fde794c?sid=faa8cc4b-001c-4c51-926c-6a551dfb7c63', 'URL to test App: https://app-test-pandas-agent-vjbjfjkmxfrvhkhc455p4k.streamlit.app/', 'Project Success Story: https://docs.google.com/document/d/17VZukkZW6LsXVmb6IDIZWpp61sRQY_cE/edit?usp=sharing&ouid=111848530990018600604&rtpof=true&sd=true', 'Solution Diagram: https://drive.google.com/file/d/16T56xrxBHioAIRnoA0EmHlSdMcmzEWP3/view?usp=sharing', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Grafana Dashboard to visualize and analyze sensors’ data', 'Next article', 'Automated Campaign Management System: A Comprehensive Solution with LinkedIn and Email Integration', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2023,https://insights.blackcoffer.com/grafana-dashboard-to-visualize-and-analyze-sensors-data/,"Grafana Dashboard to visualize and analyze sensors’ data - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Products & Services:', 'IT & Consulting, Software Development, DevOps', 'Organization Size:', '100+', 'The Problem', 'The client requires a Grafana dashboard that can fetch data from a web API providing historical data of building automation systems. The dashboard needs to allow manual entry of a target URL for individual buildings, selection of a history name from a dropdown or search bar, selectable time range for displaying history data, and the ability to choose from various chart types for visualization. Additionally, the client wants to set up alarms for certain metrics like CPU, RAM, and hard disk usage. Each user should only be able to view their own STier API data, which is controlled by their IP.', 'Our Solution', 'To meet these requirements, we will set up a Grafana dashboard using the Grafana API. We will configure the dashboard to connect to the web API and fetch data based on the user’s input for the target URL, history name, and time range. For visualization, we will implement various chart types including Bar, Line, and Scatter plot charts. To set up alarms for specific metrics, we will utilize Grafana’s built-in alerting feature.', 'Solution Architecture', 'Deliverables', 'A fully functional Grafana dashboard connected to the web API', 'Ability to manually enter a target URL for individual buildings', 'Selection of history name from a dropdown or search bar', 'Selection of time range for displaying history data', 'Various chart types for data visualization', 'Setup of alarms for specific metrics', 'Tech Stack', 'Tools used', 'Python', 'Grafana', 'Grafana API', 'Web API for historical data of building automation systems', 'Language/techniques used', 'Javascript', 'SQL', 'Skills used', 'Data Visualization', 'API Integration', 'User Interface Design', 'Databases used', 'Grafana Database', 'What are the technical Challenges Faced during Project Execution', 'Implementing user permissions for individual users', 'Setting up alarms for specific metrics', 'How the Technical Challenges were Solved', 'For connecting Grafana to the web API, we used the Grafana API and configured it to fetch data from the web API based on user input.', 'To implement user permissions, we used Grafana’s built-in user management feature and set up roles and permissions accordingly.', 'For setting up alarms, we leveraged Grafana’s built-in alerting feature and configured it to trigger alerts based on specific conditions.', 'Business Impact', 'The proposed Grafana dashboard will significantly enhance the business’s ability to monitor and manage building automation systems. By providing real-time data visualization and the ability to set alarms for specific metrics, the business can quickly identify and address potential issues, ensuring optimal system performance and efficiency. Furthermore, the user-specific permissions will ensure that sensitive data remains secure and accessible only to authorized individuals. This will not only streamline operations but also boost confidence among staff members who can now make informed decisions based on accurate and timely data. The dashboard’s flexibility in terms of selectable history names and time ranges will allow for comprehensive analysis of historical data, leading to improved decision-making processes. Overall, this solution will contribute to increased operational efficiency, reduced downtime, and improved customer satisfaction by ensuring smooth operation of building automation systems.', 'Project website url', 'https://mailhvac.postman.co/workspace/Team-Workspace~902b44a6-966b-4e59-8400-3ae02c12ce6b/collection/17767455-eb2c775e-421d-4f7c-9ec5-b4f6a73f1a5a?action=share&creator=17767455', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'MVP for a software that analyses content from audio (Pharma-based)', 'Next article', 'AI-driven data analysis AI tool using Langchain for a leading real estate and financing firm worldwide', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2024,https://insights.blackcoffer.com/mvp-for-a-software-that-analyses-content-from-audio-pharma-based/,"MVP for a software that analyses content from audio (Pharma-based) - Blackcoffer Insights-['Client Background', 'Client:', 'A leading pharma-tech firm in the USA', 'Industry Type:', 'Healthcare', 'Products & Services:', 'Pharma Apps', 'Organization Size:', '100+', 'The Problem', 'The problem lies in creating a backend model for an application that records audio responses from students and uses AI to analyze the content. The backend needs to convert audio to text, transform the text into analytics KPIs, handle login/logout operations, and manage analytics API calls. The application should also calculate the cosine similarity of the student’s response with the expected response.', 'Our Solution', 'To solve this problem, we will use Python as the primary programming language for backend development. The solution involves several steps:', 'Audio to Text Conversion: We will use a speech recognition library in Python such as SpeechRecognition to convert audio inputs into text.', 'Text Analysis: After converting the audio to text, we will apply Natural Language Processing (NLP) techniques to analyze the text. This includes sentiment analysis, readability analysis, and named entity recognition (NER). We will use libraries like NLTK and SpaCy for this purpose.', 'User Authentication: We will build a secure authentication system using JWT tokens for handling login and logout operations.', 'API Creation: We will use Flask, a lightweight Python framework, to create APIs for managing user sessions and handling analytics data.', 'Data Storage: We will use a relational database like PostgreSQL to store user session data, user profiles, and analytics data.', 'Deployment: Finally, we will deploy the application on a cloud platform like AWS or Google Cloud.', 'Solution Architecture', 'Deliverables', 'Backend model developed using Python', 'APIs for managing user sessions and analytics data', 'Secure user authentication system', 'System capable of converting audio to text', 'Text analysis capabilities including sentiment analysis, readability analysis, and NER', 'Deployed application on a cloud platform', 'Tech Stack', 'Tools used', 'Python', 'Flask', 'JWT', 'PostgreSQL', 'AWS/Google Cloud', 'Language/techniques used', 'Python', 'Models used', 'SpeechRecognition for audio to text conversion', 'NLTK and SpaCy for text analysis', 'Skills used', 'Backend development', 'API creation', 'Text Sentiment analysis – Cosine Similarity Scoring', 'Machine learning (Natural Language Processing)', 'What are the technical Challenges Faced during Project Execution', 'One of the main challenges faced during development was ensuring accurate audio to text conversion. Poor audio quality or heavy accents can make it difficult for speech recognition algorithms to correctly transcribe the audio.', 'How the Technical Challenges were Solved', 'To overcome this challenge, we decided to use a robust speech recognition library that supports multiple languages and dialects. Additionally, we implemented a mechanism to allow users to manually edit the transcribed text, providing them with more control over the accuracy of the transcription.', 'Business Impact', 'The implementation of this backend model will have significant business impacts:', 'Enhanced Student Engagement: By providing immediate feedback on student responses, the system can foster a more engaging learning environment. Students can receive instant insights into their communication style and areas of improvement, encouraging them to enhance their responses and overall academic performance.', 'Improved Learning Outcomes: The detailed analytics provided by the system can aid educators in understanding student learning patterns and identifying areas where students struggle. This can inform instructional strategies and curriculum adjustments, leading to improved learning outcomes.', 'Cost Savings: Automating the conversion of audio to text and the generation of analytics can significantly reduce manual labor costs associated with grading and feedback provision.', 'Scalability: The use of scalable technologies like Python and Flask allows the system to handle increasing volumes of student responses without compromising performance.', 'Data Insights: The system generates valuable data insights, including sentiment scores, readability metrics, and named entity recognition counts. These insights can inform strategic decisions and policy changes.', 'Customer Satisfaction: By providing a seamless, efficient experience for both students and educators, the system can enhance customer satisfaction, potentially leading to increased usage and positive word-of-mouth referrals.', 'These impacts align with the objectives of the business, making the project a high priority. The business impact analysis will ensure that the project is aligned with the organization’s strategic goals and that potential disruptions are identified and managed effectively', 'Project Snapshots', 'Project website url', 'Domain and SSL setup is completed :', 'https://www.pharmacyinterns.com.au/', 'Web App is running successfully on\xa0 URL –', 'http://34.30.224.139/', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Data Engineering and Management tool (Airbyte) with custom data connectors to manage CRM database', 'Next article', 'Grafana Dashboard to visualize and analyze sensors’ data', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2025,https://insights.blackcoffer.com/data-engineering-and-management-tool-airbyte-with-custom-data-connectors-to-manage-crm-database/,"Data Engineering and Management tool (Airbyte) with custom data connectors to manage CRM database - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in Europe', 'Industry Type:', 'IT', 'Products & Services:', 'IT & Consulting, Software Development', 'Organization Size:', '1000+', 'The Problem', 'Our company requires a robust, scalable, and secure data integration solution that can handle thousands of connections. We need to develop Airbyte connectors for various software applications listed in 2-nx-integration, including Join Portal, ClickUp, Coach Accountable, Hubspot, Quickbooks, Quickbooks Time, and Sales Flow. These connectors should be developed in Python and then wrapped into Docker images. The code should be housed in GitHub and automatically applied to Airbyte for execution using a CI/CD pipeline from GitHub to Airbyte. We also need a full production-ready version of Airbyte hosted on Google Cloud Platform (GCP) Kubernetes, secured via Google Sign In.', 'Moreover, we want to add custom features to Airbyte to control BigQuery projects/datasets. Both Airbyte and BigQuery should be monitored via Sentry, which will also be housed/hosted in the same project for all error reporting/monitoring. We also need to develop transformations to clean and transform the data from the software source to the client’s GCP Project for BigQuery. The code for these transformations should be stored in GitHub.', 'Our Solution', 'We propose to develop an instance of Airbyte that is production-ready on GCP over Kubernetes. This will be secured using Google Sign On linked to our organization. We will deploy Airbyte using the official documentation 8. To secure the Kubernetes setup, we plan to use Traefik’s ForwardAuth feature.', 'Next, we will code Airbyte Python integrations for our needed software list. We have already gathered the API documentation for each software application and have started coding the integrations. Once the initial integration is complete, we will document the process in ClickUp to guide future integrations.', 'We will use GitHub to host both the source code and Docker images of Airbyte integrations. We will also use Google Cloud’s Sentry for error reporting and monitoring.', 'Solution Architecture', 'Deliverables', 'Production-ready Airbyte instance on GCP Kubernetes', 'Secured Airbyte instance using Google Sign On', 'Developed Airbyte Python integrations for required software', 'Error reporting and monitoring setup with Sentry', 'Documentation of integration process in ClickUp', 'Tech Stack', 'Tools used', 'Airbyte', 'Docker', 'GitHub', 'Google Cloud Platform', 'Google Sign In', 'Traefik', 'Sentry', 'Language/techniques used', 'Python', 'Models used', 'Airbyte ETL', 'Skills used', 'Web Scraping', 'Database Management', 'API Connectors', 'Databases used', 'Google BigQuery', 'What are the technical Challenges Faced during Project Execution', 'One of the main challenges we anticipate is managing the scalability of the system to handle thousands of connections. Another challenge could be securing the system effectively while ensuring smooth operation.', 'How the Technical Challenges were Solved', 'To address the scalability issue, we will leverage the inherent scalability of Kubernetes and BigQuery. Kubernetes allows us to easily scale our services based on demand, while BigQuery is designed to handle large datasets and high query loads.', 'To ensure effective security, we will use Google Sign In for user authentication, and we will follow best practices for securing our Docker containers and GCP environment. Regular audits and penetration testing will also be conducted to identify and rectify any potential security vulnerabilities.', 'Business Impact', 'By developing a robust and scalable data integration solution using Airbyte, we aim to significantly enhance our business operations. This solution will enable us to efficiently manage and analyze data from various software applications, leading to improved decision-making processes.', 'Firstly, the ability to extract and load data from different software applications will allow us to centralize our data management, reducing the complexity of handling multiple data sources. This will streamline our data analysis processes and provide a unified view of our business data.', 'Secondly, the scalability of our solution means that it can handle a growing volume of data as our business grows. This is crucial in today’s digital age where businesses generate vast amounts of data daily.', 'Lastly, by securing our data integration solution with Google Sign In, we can ensure that only authorized individuals can access our sensitive business data. This adds an extra layer of security to our data management practices and helps protect against potential data breaches.', 'Moreover, by using Google Cloud Platform (GCP) for hosting our solution, we can take advantage of its advanced features and robust infrastructure. This will further enhance the reliability and performance of our data integration solution.', 'Overall, implementing this solution will enable us to harness the power of data to drive our business growth and success', 'Project Snapshots', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Text Summarizing Tool to scrape and summarize pubmed medical papers', 'Next article', 'MVP for a software that analyses content from audio (Pharma-based)', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2026,https://insights.blackcoffer.com/text-summarizing-tool-to-scrape-and-summarize-pubmed-medical-papers/,"Text Summarizing Tool to scrape and summarize pubmed medical papers  - Blackcoffer Insights-['Client Background', 'Client:', 'A leading medical R&D firm in the USA', 'Industry Type:', 'Medical', 'Products & Services:', 'R&D', 'Organization Size:', '10000+', 'The Problem', 'An advanced AI tool designed specifically for doctors to assist them in retrieving answers to their', 'queries. Powered by state-of-the-art AI technologies, including web scraping and ChatGPT, The AI', 'Assistant aims to streamline information retrieval and provide valuable insights to professionals.', 'This AI Assistant leverages the capabilities of AI to facilitate seamless and efficient access to', 'knowledge and information. It combines web scraping techniques to gather relevant data from', 'trusted sources with ChatGPT and PubMed, providing accurate responses to doctors’ queries.', 'Query Retrieval: AI Assistant utilizes web scraping techniques to fetch information from credible', 'websites, academic journals, medical databases, and other trusted sources. It provides doctors with', 'immediate access to a vast array of knowledge and resources.', 'Benefits:', 'Time Efficiency: By quickly retrieving information and answering queries, AI Assistant saves', 'valuable time for doctors, allowing them to focus more on patient care and critical tasks.', 'Access to Knowledge: AI Assistant grants doctors easy access to a vast repository of knowledge,', 'ensuring they stay updated with the latest research, treatment guidelines, and best practices.', 'Decision Support: The tool provides valuable insights and recommendations, assisting doctors in', 'making informed decisions about diagnosis, treatment plans, and patient management.', 'Our Solution', 'To address this problem, we will build a web scraping tool that uses Python libraries such as BeautifulSoup, Selenium, and OpenAI’s GPT-3. The program will work as follows:', 'A user inputs the URL of the case report they want to extract data from.', 'The program sends a GET request to the webpage and parses the HTML content using BeautifulSoup.', 'The program then identifies the relevant sections of the webpage (such as the title, introduction, report, conclusion, and keywords) and extracts the text content.', 'For each reference linked in the case report, the program sends a GET request to the reference’s webpage and parses the HTML content.', 'The program then sends a prompt to the GPT-3 model, asking it to summarize the content of the reference, and receives a summarized response.', 'The program collects all the summarized references and adds them to the case report.', 'The program also identifies any images associated with the case report and downloads them.', 'Finally, the program creates a Word document and adds all the collected information (including the summarized references and downloaded images) to the document.', 'Solution Architecture', 'Deliverables', 'A fully functional web scraping tool that can extract data from a given webpage and generate a case report.', 'A detailed documentation explaining how to use the tool and what kind of data it can extract.', 'Tech Stack', 'Tools used', 'Python', 'BeautifulSoup', 'Selenium', 'OpenAI’s GPT-3', 'Language/techniques used', 'Python', 'Models used', 'OpenAI’s GPT-3', 'Skills used', 'Web Scraping', 'Natural Language Processing', 'Machine Learning', 'What are the technical Challenges Faced during Project Execution', 'Handling dynamic websites that load content via JavaScript.', 'Managing rate limits and CAPTCHAs imposed by the target websites.', 'Ensuring the accuracy and relevance of the summarized content generated by the GPT-3 model.', 'How the Technical Challenges were Solved', 'Using Selenium to interact with the JavaScript-rendered content of the target websites.', 'Implementing strategies to bypass rate limits and CAPTCHAs.', 'Fine-tuning the parameters of the GPT-3 model to improve the quality of the summarized content.', 'Business Impact', 'The implementation of our web scraping and summarization tool has had significant positive impacts on our business operations.', 'Firstly, it has streamlined our research process by automating the extraction of crucial information from various online sources. This has saved us considerable time and effort, allowing us to focus on more complex tasks.', 'Secondly, the summarization feature has improved our understanding of the information we collect. By reducing large volumes of text down to a few key points, we’ve been able to quickly grasp the main ideas and insights presented in the articles, videos, and user comments.', 'Thirdly, the tool has enabled us to stay up-to-date with the latest advancements in the field of orthopedics. By pulling data from recent articles on PubMed.gov, we’ve been able to stay informed about the latest research and treatments.', 'Finally, the tool has facilitated the creation of comprehensive case reports. These reports have been instrumental in our ability to present detailed and accurate information to our clients, thereby enhancing our reputation and credibility in the industry.', 'Overall, the implementation of this tool has greatly improved our efficiency and effectiveness, contributing significantly to our business success', 'Project Snapshots', 'Project Video', 'Link:', 'https://www.loom.com/share/535828aad7184c1b82db707dcca8e52c?sid=c79d19b1-b963-45a1-bec5-6228cc753cc2', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', '7up7down, 10upDown, Snakes and Ladder Games built using OOPs', 'Next article', 'Data Engineering and Management tool (Airbyte) with custom data connectors to manage CRM database', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2027,https://insights.blackcoffer.com/7up7down-10updown-snakes-and-ladder-games-built-using-oops/,"7up7down, 10upDown, Snakes and Ladder Games built using OOPs - Blackcoffer Insights-['Client Background', 'Client:', 'A leading game development firm in the USA', 'Industry Type:', 'Gaming Software', 'Products & Services:', 'Gaming Software Development', 'Organization Size:', '200+', 'The Problem', 'Our client sends records of millions of sports bets in real time from all over the world via an API. These bets are recorded in MySQL servers. We are tasked with processing and calculating the expected Profit and Loss (PNL) as per the bets records for each sport. Our goal is to analyze these records in real time via API and calculate PNL as per the game records history provided via API. This requires building a serverless application in Python (or similar) that reads all bets records and updates PNL in real time (within milliseconds, records need to be updated). The application should be capable of handling 10,000+ records of bets per second for numbers of different games, with PNL needing to be updated for each game separately.', 'Our Solution', 'To address this problem, we propose developing a Python-based serverless application that leverages machine learning models for real-time PNL calculation. The application will use the MySQL database to store and retrieve betting records. It will employ parallel computing techniques to ensure efficient processing of high volumes of data. The application will also utilize APIs to fetch real-time data and update PNL accordingly.', 'The application will follow these steps:', 'Connect to the MySQL database to access the betting records.', 'Use an API to fetch real-time betting data.', 'Process the data using Python scripts.', 'Apply machine learning models to predict the outcome of each bet.', 'Calculate the PNL for each bet according to the predicted outcome.', 'Update the PNL in the MySQL database in real time.', 'Solution Architecture', 'Deliverables', 'A Python-based serverless application for real-time PNL calculation.', 'An interface for visualizing the calculated PNL in real time.', 'Documentation detailing how to use and maintain the application.', 'Tech Stack', 'Tools used', 'Python: For writing the serverless application.', 'MySQL: For storing and retrieving betting records.', 'Machine Learning Models: For predicting the outcome of bets.', 'Language/techniques used', 'Python', 'Models used', 'OOPS', 'Skills used', 'Database Analysis & API Development: To design and optimize the MySQL database.', 'Python Programming: To write the serverless application.', 'OOPS: To make the game functioning algorithms.', 'Databases used', 'SQL', 'What are the technical Challenges Faced during Project Execution', 'One of the main challenges we faced was handling the high volume of data coming in real time. To overcome this, we employed parallel computing techniques to efficiently process the data. Another challenge was updating the PNL in the MySQL database in real time. We solved this by designing the application to update the PNL immediately after it is calculated.', 'How the Technical Challenges were Solved', 'We addressed the high volume of data challenge by using parallel computing techniques. This allowed us to process a large number of records simultaneously, ensuring efficient data handling.', 'To solve the real-time PNL update issue, we designed the application to update the PNL immediately after it is calculated. This ensured that the PNL was always up-to-date, meeting the requirement of real-time PNL calculation.', 'Business Impact', 'The implementation of the proposed Python-based serverless application for real-time PNL calculation had significant positive impacts on our business operations.', 'Firstly, the application enabled us to process and analyze millions of sports bets in real time, enhancing our decision-making capabilities and allowing for quicker responses to changes in the betting market. This improved our ability to predict outcomes and adjust our betting strategies accordingly.', 'Secondly, the application significantly reduced the time taken to calculate PNL, from hours to mere minutes. This resulted in faster decision-making processes and timely financial reporting, which were crucial for our clients and investors.', 'Lastly, the application’s ability to handle high volumes of data and provide real-time updates facilitated a more globalized betting market. With real-time data and digital platforms, geographical boundaries became less relevant, allowing bettors from around the world to place bets on any event globally, with real-time odds reflecting local nuances and dynamics. This led to increased liquidity and more competitive odds.', 'Overall, the successful implementation of the application led to a more efficient, accurate, and timely PNL calculation process, resulting in improved business performance and customer satisfaction.', 'Project Snapshots', 'Project website url', 'https://lookerstudio.google.com/u/3/reporting/da134941-6efc-43e4-9b2a-37b7a6aab1b0/page/p_kfrjaxka8c/edit', 'https://console.cloud.google.com/welcome?authuser=1&project=t4a-dashboard', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Data Studio Dashboard with a data pipeline tool synced with Podio using custom Webhooks and Google Cloud Function', 'Next article', 'Text Summarizing Tool to scrape and summarize pubmed medical papers', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2028,https://insights.blackcoffer.com/data-studio-dashboard-with-a-data-pipeline-tool-synced-with-podio-using-custom-webhooks-and-google-cloud-function/,"Data Studio Dashboard with a data pipeline tool synced with Podio using custom Webhooks and Google Cloud Function - Blackcoffer Insights-['Client Background', 'Client:', 'A leading retail firm in the USA', 'Industry Type:', 'Retail', 'Products & Services:', 'Retail Business, e-commerce', 'Organization Size:', '300+', 'The Problem', 'The client needs a consolidated KPI dashboard that aggregates data from various applications and SaaS products. Currently, the data is scattered across different platforms, making it difficult to track key performance indicators (KPIs) effectively. The client wants a dashboard that automatically updates with new data, eliminating the need for manual updates. The dashboard should contain separate tabs for current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances. Additionally, the client wants to use Google Cloud Functions to sync data regularly between the Podio data app and Google Sheets.', 'Our Solution', 'The proposed solution involves the creation of a KPI dashboard in Google Sheets, which will serve as a central hub for all the client’s data. This dashboard will be populated with data from various sources, including Google Sheets and the Podio data app. The data will be organized into separate tabs, each representing a different aspect of the business. The dashboard will be designed to automatically update with new data, removing the need for manual updates.', 'The process begins with obtaining access to the data in Google Sheets. Once the data is accessed, a list of KPIs to be visualized will be prepared. The data from Google Sheets will then be connected to the Google Data Studio dashboard for visualization. The dashboard will be designed to align with the client’s goals, prioritizing the most important KPIs and positioning them at the top of the dashboard. The dashboard will also be protected to prevent further or accidental changes, ensuring that data can only be added or changed through designated data sheets. Collaborators will be invited via email, with specific roles assigned to ensure effective collaboration. The dashboard will be customized with brand-aligned colors and fonts to enhance its appearance and authority.', 'In addition to the dashboard, webhooks will be created for the Podio data app deployed as a Google Cloud Function. This will enable regular data synchronization between the Podio data app and Google Sheets, ensuring that the dashboard is always up-to-date with the latest data.', 'Solution Architecture', 'Deliverables', 'End-to-end data pipeline', 'KPI Dashboard in Google Sheets with separate tabs for current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances.', 'Automatic update functionality to eliminate the need for manual updates.', 'Webhook for the Podio data app deployed as a Google Cloud Function to sync data regularly.', 'Tech Stack', 'Tools used', 'Python', 'Google Sheets', 'Google Data Studio', 'Google Cloud Functions', 'Podio data app', 'Language/techniques used', 'Python', 'Javascript', 'Skills used', 'Data Analysis', 'Data Visualization', 'Cloud Functions', 'API Integration', 'Databases used', 'BigQuery', 'What are the technical Challenges Faced during Project Execution', 'One of the main challenges was ensuring that the dashboard could seamlessly integrate data from various sources and update automatically.', 'Another challenge was designing the dashboard in a way that aligns with the client’s goals and presents the data in a clear and actionable manner.', 'How the Technical Challenges were Solved', 'The first challenge was addressed by connecting the data sources to Google Sheets and setting up the dashboard to automatically update with new data. This was achieved by using Google Data Studio and Google Cloud Functions.', 'The second challenge was addressed by focusing on the design and organization of the dashboard, ensuring that it aligns with the client’s goals and presents the data in a clear and actionable manner. This was achieved by prioritizing the most important KPIs and positioning them at the top of the dashboard, and by presenting supporting data as charts and tables to help decision-makers make sense of the KPI', 'Business Impact', 'The implementation of the proposed solution has significantly improved the client’s ability to track and manage key performance indicators (KPIs). Prior to the solution, the client was struggling with data fragmentation across different SaaS products and applications, which made it difficult to compile comprehensive insights. The KPI dashboard, now consolidated in Google Sheets, has streamlined this process, providing a unified view of the business metrics.', 'This solution has also automated the data update process, saving valuable time and resources that were previously spent on manual updates. The automatic update feature has allowed the client to focus on analyzing the data rather than spending hours updating it.', 'Additionally, the integration of the Podio data app with Google Sheets via Google Cloud Functions has improved data synchronization efficiency. Regular data synchronization ensures that the KPI dashboard is always up-to-date, providing real-time insights into the business performance.', 'These improvements have led to enhanced decision-making processes within the client’s organization. With accurate and timely data, managers can now set and achieve goals more effectively. The consolidation of data has also facilitated cross-departmental collaboration, as teams can now access and share data easily.', 'Overall, the solution has resulted in significant business impact, leading to improved operational efficiency, informed decision-making, and strategic planning', 'Project Snapshots', 'Project website url', 'https://lookerstudio.google.com/u/3/reporting/da134941-6efc-43e4-9b2a-37b7a6aab1b0/page/p_kfrjaxka8c/edit', 'https://console.cloud.google.com/welcome?authuser=1&project=t4a-dashboard', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'End-to-end tool to optimize routing and planning of field engineers using Google’s CVRP-TW algorithm', 'Next article', '7up7down, 10upDown, Snakes and Ladder Games built using OOPs', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2029,https://insights.blackcoffer.com/end-to-end-tool-to-optimize-routing-and-planning-of-field-engineers-using-googles-cvrp-tw-algorithm/,"End-to-end tool to optimize routing and planning of field engineers using Google’s CVRP-TW algorithm - Blackcoffer Insights-['Client Background', 'Client:', 'A leading hardware firm in the USA', 'Industry Type:', 'IT', 'Products & Services:', 'IT Consulting, Support, Hardware Installations', 'Organization Size:', '300+', 'The Problem', 'The client specializes in installing blinds and related products in customers’ homes. They are currently struggling with scheduling appointments efficiently due to a variety of factors such as location, installation duration, team member availability, and customer preferences. We need a tool that can suggest optimal schedules based on these criteria and adapt to changes as customers approve or reject proposed appointment times. The goal is to create a proof of concept for a route and job planning model that can potentially streamline our scheduling process and make a significant impact on our business operations.', 'Our Solution', 'The To address this challenge, we propose developing a proof of concept for a route and job planning model. This model will be based on the concept of Constrained Vehicle Routing Problem with Time Windows (CVRP-TW), a well-established approach in operations research and logistics. The model will take a dataset, which could be extracted from a Google sheet or converted from a CSV file, and generate optimal schedules.', 'The development process will involve several stages:', 'Understanding the data: We’ll analyze the data to identify the relevant variables and constraints. These may include the locations of installations, the duration of installations, the availability of team members, and customer preferences.', 'Defining the objective and constraints: The objective will be to minimize the total travel time or maximize the number of installations completed within a given time frame. The constraints will include the geographical distances between locations, the working hours of team members, and the specific requirements of each installation.', 'Implementing the algorithm: We’ll use an optimization algorithm, such as the Traveling Salesman Problem (TSP) solver, to find the optimal routes. The algorithm will consider all possible routes and choose the one that best meets the objectives while adhering to the constraints.', 'Running simulations: To ensure the feasibility of the model, we’ll run simulations using different scenarios and adjust the parameters as needed.', 'Saving the output: The final output will be the suggested schedules, which can then be reviewed and approved by the relevant parties.', 'In terms of technology, we’ll use Python, a popular language for data analysis and machine learning. We’ll also use the Anaconda distribution, which provides a powerful environment for scientific computing and data analysis.', 'Solution Architecture', 'Deliverables', 'A Python script implementing the CVRP-TW model.', 'Test data and scripts for simulating different scenarios.', 'Documentation explaining how to use the model and interpret the results.', 'Tech Stack', 'Tools used', 'Python: The primary programming language.', 'Anaconda: The Python distribution used for data analysis and machine learning.', 'Visual Studio Code: The code editor used during development.', 'Google App Script for deployment integrated with Google Sheets', 'Language/techniques used', 'Python', 'Models used', 'Constrained Vehicle Routing Problem with Time Windows (CVRP-TW)', 'Skills used', 'Data Analysis', 'Machine Learning', 'Optimization Algorithms', 'Python Programming', 'Databases used', 'CSV, Google Sheets: The data will initially be stored in a CSV file, which can be easily imported into Python using libraries like pandas.', 'What are the technical Challenges Faced during Project Execution', 'One of the main challenges we anticipated is dealing with the complexity and variability of the data. The locations of installations, the duration of installations, the availability of team members, and customer preferences all need to be taken into account, and these factors can vary widely. Additionally, the model needs to be flexible enough to adapt to changes in the criteria as customers approve or reject appointment times.', 'How the Technical Challenges were Solved', 'To overcome these challenges, we used advanced data analysis techniques to extract meaningful insights from the data. We’ll also develop a flexible model that can handle changes in the criteria. Furthermore, we’ll thoroughly test the model under different scenarios to ensure its robustness and reliability.', 'Business Impact', 'Implementing an efficient route and job planning model had a significant positive impact on our business operations. By automating the scheduling process, we were able to reduce manual errors and streamline our workflow, resulting in quicker response times and deliveries. This not only improved our operational efficiency but also enhanced our ability to provide better service to our customers.', 'Moreover, the model allowed us to maximize each driver’s productivity by optimizing routes, which led to cost savings in fuel and vehicle maintenance. The automated nature of the system also enabled us to make real-time adjustments to the route in response to last-minute orders or unexpected situations, such as a driver being unavailable.', 'The model also provided us with valuable insights into our operations, allowing us to identify bottlenecks and areas for improvement. This helped us to proactively address potential issues and continuously enhance our processes, thereby increasing our overall business performance.', 'As a result of these improvements, we were able to attract more skilled workers by focusing on cutting down unskilled labor. This shift towards more automation allowed us to invest more in our workforce, leading to higher employee satisfaction and retention rates.', 'Lastly, the successful implementation of the route and job planning model has opened up new opportunities for our business. With the ability to efficiently cover our market and manage our resources effectively, we have been able to consider expanding our territory by entering new markets. This strategic route planning has helped us determine whether we need to acquire more vehicles or hire more operators before moving, providing a clear pathway for future growth.', 'Project Snapshots', 'Project website url', 'https://docs.google.com/spreadsheets/d/1kS7Em9NitvMD_49MoLCpt_KoPJGGIAGjCES_KI8rEQk/edit?userstoinvite=raymondchow%40stanbondsa.com.au#gid=766964619', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'End-to-end tool to predict Biofuel prices using IESO data', 'Next article', 'Data Studio Dashboard with a data pipeline tool synced with Podio using custom Webhooks and Google Cloud Function', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2030,https://insights.blackcoffer.com/end-to-end-tool-to-predict-biofuel-prices-using-ieso-data/,"End-to-end tool to predict Biofuel prices using IESO data - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Products & Services:', 'IT Consulting, Software Development', 'Organization Size:', '100+', 'The Problem', 'The task involves creating an end-to-end data pipeline to extract data from various reports, store it in a Google Cloud Platform (GCP) database, build a dashboard, and develop a machine learning model for price forecasting. The data is pulled from different links, each having a slightly different report layout, with some being in CSV and others in XML format. The goal is to extract data daily and hourly for the past three years. The extracted data is intended to be used for building a dashboard and training/testing a model based on user-defined inputs on the dashboard. The challenge lies in handling the varied formats of the data, ensuring accurate extraction, and maintaining the integrity of the data throughout the pipeline.', 'Our Solution', 'To solve this problem, we will use Python, along with libraries such as pandas and BeautifulSoup, to scrape data from various report links. The scraped data is stored in dataframes and then loaded into Google Cloud Storage buckets. This data is then transferred to BigQuery tables for efficient processing. The data extraction process is automated with a Cronjob/Google Cloud Scheduler.', 'For the machine learning part, we will build and run various machine learning models in GCP’s BigQuery to predict future fuel/energy prices. We will test LSTM univariate/multivariate, GRU for time series problems, and ANN Regressor, Random Forests regression for regression problems. The ANN regression model will provide the best results for our use case.', 'After modeling, we will generate a data visualization report on Google Data Studio for further insights. The report includes a pie chart about the distribution of fuel generated by each fuel type, a stacked column chart about the distribution of fuel generated each month, and a time series visualization of fuel generation during each quarter of the year.', 'Solution Architecture', 'Deliverables', 'End-to-end data pipeline', 'Data stored in Google Cloud Platform (GCP) database', 'Dashboard built on Google Data Studio', 'Machine learning model for price forecasting', 'Tech Stack', 'Tools used', 'Python', 'pandas', 'BeautifulSoup', 'Google Cloud Platform (GCP)', 'Google Cloud Storage', 'Google BigQuery', 'Google Data Studio', 'Language/techniques used', 'Python', 'Models used', 'LSTM', 'GRU', 'ANN Regressor', 'Random Forests Regression', 'Skills used', 'Web Scraping', 'Database Management', 'Data Visualization', 'Machine Learning Model Development', 'Databases used', 'Google BigQuery', 'What are the technical Challenges Faced during Project Execution', 'Handling varied data formats (CSV, XML)', 'Ensuring accurate extraction of data', 'Maintaining data integrity throughout the pipeline', 'How the Technical Challenges were Solved', 'Utilizing Python libraries like pandas and BeautifulSoup for web scraping and data manipulation', 'Automating the data extraction process using Cronjob/Google Cloud Scheduler', 'Testing various machine learning models to select the best fit for our use case', 'Using Google Cloud Platform services for storing, processing, and visualizing data.', 'Business Impact', 'The successful implementation of the end-to-end data pipeline project had several significant business impacts.', 'Firstly, it led to improved data quality and accessibility. The project streamlined the process of data extraction from various sources, ensuring that the data was clean, consistent, and readily available for analysis. This resulted in more reliable and accurate predictions, leading to better decision-making and strategic planning.', 'Secondly, the project enhanced operational efficiency. By automating the data extraction process with a Cronjob/Google Cloud Scheduler, the team saved considerable time and effort. This allowed the team to focus on more strategic tasks, thereby increasing productivity.', 'Thirdly, the project facilitated informed decision-making. The dashboard built on Google Data Studio provided users with real-time insights into fuel consumption patterns and energy prices. This helped stakeholders make informed decisions regarding energy usage and pricing strategies.', 'Lastly, the project demonstrated the company’s commitment to leveraging advanced technologies for business growth. The use of Google Cloud Platform, BigQuery, and Google Data Studio showcased the company’s ability to innovate and stay competitive in the rapidly evolving digital landscape.', 'Overall, the project had a positive impact on the company’s operations, decision-making processes, and reputation among stakeholders. It underscored the importance of data-driven decision making and highlighted the potential benefits of investing in advanced technologies.', 'Project Snapshots', 'Project website url', 'https://console.cloud.google.com/compute/instances?authuser=1&project=ieso&pli=1', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Methodology for ETL Discovery Tool using LLMA, OpenAI, Langchain', 'Next article', 'End-to-end tool to optimize routing and planning of field engineers using Google’s CVRP-TW algorithm', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2031,https://insights.blackcoffer.com/etl-discovery-tool-using-llma-langchain-openai/,"ETL Discovery Tool using LLMA, Langchain, OpenAI - Blackcoffer Insights-['Client Background', 'Client:', 'A leading retail firm in the USA', 'Industry Type:', 'Retail', 'Products & Services:', 'Retail Business, e-commerce', 'Organization Size:', '100+', 'The Problem', 'To develop an ETL discovery tool that can answer the queries related to ETL pipelines in conversational format. The areas of the concerned queries should include Environment Analysis, Workflow Analysis, Data Source and Target Mapping, Transformation Logic, Data Volume and Velocity, Error Handling and Logging and Security and Access Control.', 'Our Solution', 'In developing our solution, we began by aggregating Open-Source Generic ETL Tool Code from various repositories on GitHub and other relevant sources. Subsequently, we meticulously fine-tuned the collected ETL tool code, organizing and saving it into distinct folders, each containing different ETL pipelines.', 'Following this, we implemented an OpenAI Assistant, integrating it with all the refined ETL pipelines. To facilitate communication with these pipelines, we employed the OpenAI Assistant ID within our Flask API.', 'For the user interface, we opted for a Streamlit front-end, providing a seamless and user-friendly interaction with our OpenAI Assistant and the integrated ETL pipelines.', 'Solution Architecture', 'ETL Discovery Tool serves as the core engine for Extract, Transform, and Load (ETL) operations. It is designed to handle data extraction, transformation, and loading tasks efficiently. It will be used for training the OpenAI model on the ETL Discovery tools.', 'Step 1', '.', 'Open-Source Generic ETL Tool Code:', 'The Open-Source Generic ETL Tool serves as the core engine for Extract, Transform, and Load (ETL) operations. It is designed to handle data extraction, transformation, and loading tasks efficiently. It will be used for training the OpenAI model on the ETL Discovery tools.', 'Step 2', '.', 'Data Cleaning', ':', 'Data Cleaning is a critical stage that involves cleansing and pre-processing raw data to enhance its quality and integrity. In this step the ETL understands the expected data format that is organized and cleaned for uniformity of data.', 'Step 3', '.', 'Files/DB', 'Represents the storage or databases utilized for storing processed data. In this step, solutions for processed data the code files will be arranged and catalogued so that they are ready to be used by the OpenAI Assistants API.', 'Step 4', '.', 'OpenAI Assistant Creation via API:', 'This step involves creating an OpenAI Assistant using the OpenAI API.', 'Configuring the OpenAI Assistant', 'Configure .env file with OpenAI API Key', 'We will upload the files to the Assistant for it to be added in context.', 'Run assistant creator.py\xa0 file for generating OpenAI Assistant ID', 'After Generating OpenAI Assistant id look into terminal save the generated ID into .env file', 'We will get the assistant ID that is to be used later.', 'Step 5', '.', 'OpenAI Assistant:', 'In this step, the Assistant that is created from previous step will be queried by the API with instructions for the context accommodation.', 'Features and Capabilities: functionalities supported by the assistant', 'OpenAI Assistant will read all our ETL pipeline which is provided when we are generating the OpenAI assistant ID', 'Usage Guidelines/Instructions: – Guide users on interacting with the OpenAI Assistant', 'We are providing Instructions to our OpenAI Assistant to communicate with user', 'Step 6', '.', 'Django/Flask/FastAPI API:', 'This step involves setting up an API using popular frameworks like Django, Flask, or FastAPI.', 'Framework Selection: choice of the specific framework', 'We are using Flask API to communicate with the OpenAI Assistant', 'API Endpoints:\xa0 available endpoints and their functionalities', 'Configured the OpenAI Key in app1.py', 'Configured the OpenAI Assistant ID in app1.py', 'Store the Instruction file into variable we are using the variable below', 'After the Configuration of Flask file run the app1.py file to start the Flask API Local Server', 'Authentication: – Used for securing the API', 'Handling Request and Response process', 'Step 7', '.', 'Chat Frontend (Streamlit):', 'Represents the user interface for interacting with the system, built using Streamlit.', 'Configurations: Configurations of Streamlit frontend', 'Set your OpenAI API key into .env file', 'User Interaction: Users will be able to query based on training data.', 'Integration with Backend: – Frontend will be connect to the backend API.', 'In the main.py file Provide the Flask API url endpoint to communicate with OpenAI Assistant', 'Handle Request and Response from the User', 'Deliverables', 'OpenAI Assistant Flask API', 'Streamlit frontend', 'Tech Stack', 'Tools used', 'Visual Studio Code', 'Language/techniques used', 'Python, Flask, OpenAI', 'Models used', 'OpenAI Assistant', 'Skills used', 'Python, RestAPI, OpenAI API', 'What are the technical Challenges Faced during Project Execution', 'Finding the ETL pipelines and fine tuning the ETL pipelines', 'How the Technical Challenges were solved', 'Our approach to overcoming technical challenges involved an extensive internet search focused on ETL pipelines. We scoured various online resources, eventually identifying the most effective ETL pipelines available on GitHub.', 'To address each challenge systematically, we created individual files for each ETL pipeline. In the process, we meticulously fine-tuned and optimized each pipeline, documenting the specific tasks and functions within the respective files. This approach allowed us to provide detailed descriptions of the work performed for every ETL pipeline, ensuring a comprehensive understanding of the solutions implemented to tackle the technical hurdles encountered.', 'Business Impact', 'The business impact was substantial as the client efficiently analysed numerous ETL tool pipelines. Instant answers in a chat format replaced the time-consuming manual work that could take Data Engineers days or weeks. This streamlined process significantly enhanced productivity and responsiveness, reflecting a tangible improvement in operational efficiency for the client.', 'Project Snapshots', 'Assistant_creator.py', 'Main.py', 'Project Video', 'Project Demo Video link:-', 'https://www.loom.com/share/5ee7d0835412474ea4aa3383af5a0814?sid=999739fc-e91a-4cda-a30e-9cd02957205f', 'Installation Walkthrough Video:-', 'Part 1 (Backend):-', 'https://www.loom.com/share/338c4e09c90e453e83b86050d469d98b?sid=03299e7a-0699-464e-be2c-689a409ec01e', 'Part 2 (Frontend):-', 'https://www.loom.com/share/8e7942f3a03e49889c6c70fba77f76b0?sid=eca0586f-b767-45fa-854d-853bca1890dc', 'Project GitHub Repository', 'GitHub Link:- https://github.com/AjayBidyarthy/Rob-Sandberg-ETL', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'GPT/OCR API', 'Next article', 'Methodology for database discovery tool using openai, LLMA, Langchain', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2032,https://insights.blackcoffer.com/gpt-ocr-api/,"GPT/OCR API - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT & Consulting', 'Products & Services:', 'IT Solutions, Software Development', 'Organization Size:', '100+', 'The Problem', 'Design and develop an API as a service backend, the API should be integrated with GPT and OCR technologies to extract documents it should be hosted on Azure', 'Our Solution', '/token – It takes username and password as a input and generate API_key/token to run the other APIs', '/api/template/create-template – This is a Post request.\xa0 It stores the created json template in the database and generates a token id.', '/api/document/upload – This api takes a file as an input. We can upload .pdf, .docx, .png, .jpg, .jpeg, .txt files. It has basically 2 parts. We can just upload the document or we can also provide template id to process the uploaded document according to the template id.', '/api/document/process – This api takes template id and document id as an input. It fetches the template and document from the database and uses the ocr method to extract the text from the document. This extracted text and template are then processed by gpt api which generates the final output.', '/api/template/all – This api fetches all the templates created by the user using create-template api.', '/api/template/update-template – This api can update the created template.', '/api/template/delete – This api deletes the created template by giving template id.', '/api/document/all – This api shows all documents uploaded by user', '/api/document/delete – This api deletes the document by document id.', 'Deliverables', 'All the APIs on the Azure server', 'Tools used', 'fastapi, gpt api, pytessaract, pypdf2', 'Language/techniques used', 'fastapi, gpt api, pytessaract, pypdf2, python', 'Skills used', 'python, Rest API development', 'Databases used', 'MS Sql', 'Web Cloud Servers used', 'Azure', 'What are the technical Challenges Faced during Project Execution', 'Main challenge in this project extracting text from images and pdfs and generate json output according to template', 'How the Technical Challenges were Solved', 'In the apis we can upload .pdf, .docx, .png, .jpg, .jpeg, .txt files. It has basically 2 parts. We can just upload the document or we can also provide template id to process the uploaded document according to the template id.', 'It fetches the template and document from the database and uses the ocr method to extract the text from the document. This extracted text and template are then processed by gpt api which generates the final output..', 'Business Impact', 'This will help users to directly upload any pdf or image and extract useful information in json format.', 'Project Snapshots', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Dockerize the AWS Lambda for serverless architecture', 'Next article', 'ETL Discovery Tool using LLMA, Langchain, OpenAI', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2033,https://insights.blackcoffer.com/dockerize-the-aws-lambda-for-serverless-architecture/,"Dockerize the AWS Lambda for serverless architecture - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT & Consulting', 'Products & Services:', 'IT Solutions, Software Development', 'Organization Size:', '100+', 'The Problem', 'AWS Lambda, a powerful serverless compute service, faces limitations in terms of runtime customization, dependency management, and execution environment isolation.', 'Our Solution', 'To overcome the challenges mentioned above, we propose a comprehensive solution that involves Dockerizing AWS Lambda functions for improved flexibility, control, and efficiency within a serverless architecture.', 'Solution Architecture', 'Below is a high-level architecture diagram:', 'Key Components:', 'AWS Lambda Function: Contains the original Lambda function code and dependencies.', 'Dockerfile: Describes the steps to build the Docker image, including installing dependencies, copying Lambda function code, and setting the handler function.', 'Docker Image: The containerized version of the Lambda function, including its code and dependencies.', 'Amazon ECR Repository: Stores the Docker image. The image is tagged with the repository URI.', 'Updated Lambda Function: Refers to the Docker image in the ECR repository. The Lambda function configuration is updated to use this reference.', 'Deliverables', 'Some of the key deliverables:', 'Dockerfile:', 'A Dockerfile in the root of your Lambda function project, specifying the instructions to build the Docker image. This file includes the base image, installation of dependencies, copying of Lambda function code, and setting the handler function.', 'Docker Image:', 'The Docker image built from the Dockerfile. This image encapsulates your Lambda function code and its dependencies.', 'Pushed Image to ECR:', 'The Docker image pushed to your Amazon Elastic Container Registry (ECR) repository. This involves tagging the image with the ECR repository URI and pushing it to the repository.', 'Updated Lambda Function Configuration:', 'The Lambda function configuration was updated to use the Docker image from ECR. This may involve specifying the ECR URI in the Lambda configuration.', 'Documentation:', 'Documentation outlining the steps to Dockerize the Lambda function and push it to ECR. This documentation should include prerequisites, step-by-step instructions, and any additional considerations.', 'Tech Stack', 'Tools used', 'Docker', 'Amazon ECR', 'Amazon Lambda.', 'AWS Management Console.', 'Language/techniques used', 'NodeJS', 'Docker commands', 'Skills used', 'AWS services (Lambda, ECR, etc.).', 'Docker', 'Web Cloud Servers used', 'Amazon Web Services', 'What are the technical Challenges Faced during Project Execution', 'Dependency Management:', 'Challenge: AWS Lambda imposes constraints on runtime dependencies, making it challenging to manage and control library versions.', 'Execution Environment Isolation:', 'Challenge: AWS Lambda’s managed environment may lack certain runtime configurations and isolation.', 'Monitoring and Logging Integration:', 'Challenge: Efficiently capturing and analyzing performance metrics and logs from Dockerized Lambda functions.', 'How the Technical Challenges were solved', 'Dependency Management:', 'Solution: Use a containerization approach to package dependencies along with the Lambda function, providing better control and isolation. Implement a robust dependency management system within the Docker container.', 'Execution Environment Isolation:', 'Solution: Docker containers offer enhanced isolation. Utilize containers to encapsulate the Lambda function and its dependencies, ensuring consistent execution environments.', 'Monitoring and Logging Integration:', 'Solution: Integrate AWS CloudWatch for basic monitoring.', 'Project Snapshots', 'Create ECR Repository:', 'Create directory and initialize npm:', 'View Docker commands:', 'Login to ECR and Build Docker image:', 'Create Lambda Function:', 'Testing Lambda Function:', 'Project Video', 'Dockerizing a Lambda Function:', 'https://www.loom.com/share/e90438538dbb43fd884a51dab6c175e9?t=586&sid=b2e4112e-16b9-4d78-a955-77a289453e59', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Design and develop a product recommendation engine based on the features of products', 'Next article', 'GPT/OCR API', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2034,https://insights.blackcoffer.com/design-and-develop-a-product-recommendation-engine-based-on-the-features-of-products/,"Design and develop a product recommendation engine based on the features of products - Blackcoffer Insights-['Client Background', 'Client:', 'A leading retail firm in the USA', 'Industry Type:', 'Retail', 'Products & Services:', 'Retail Business, e-commerce', 'Organization Size:', '100+', 'The Problem', 'Design and develop a product recommendation engine based on the features of products', 'Our Solution', 'Content-based product recommendation system has been created using Machine Learning Algorithm and Python.', 'Solution Architecture', 'Recommendation engine have six cases which are mentioned below:', 'Case 1:', 'Description: Given an object name or PAR ID, inp_prodname recommends products with the same object type as the input and ranks them based on the number of specifications matched.', 'Input: JSON format with the following keys: Object Name, PAR ID, Debug Information, userDef1, userDef2, userDef3.', 'Output: JSON format with the following keys: Object Name, Object Type, PAR ID, Rank, Specifications, userDef1, userDef2, userDef3.', 'Case 2:', 'Description: Given specifications and an object type, inp_custom_spec recommends products and ranks them based on the number of specifications matched.', 'Input: JSON format with the following keys: Specifications, Object Type, userDef1, userDef2, userDef3.', 'Output: JSON format with the following keys: Object Name, Object Type, PAR ID, Rank, userDef1, userDef2, userDef3.', 'Case 3:', 'Description: Based on compatible models, inp_prodname_comp recommends products and ranks them based on the number of compatible models matched.', 'Input: JSON format with the following keys: Object Name, PAR ID, Debug Information, userDef1, userDef2, userDef3.', 'Output: JSON format with the following keys: Object Name, Object Type, PAR ID, Rank, Compatible Models, userDef1, userDef2, userDef3.', 'Case 4:', 'Description: Based on the number of specifications entered by the user, inp_spec_num creates clusters of products with the same number of specifications.', 'Input: JSON format with the following keys: Number of Specifications, Object Type, userDef1, userDef2, userDef3.', 'Output: JSON format with the following keys: Cluster ID, Object Name, Object Types, PAR ID, specifications_grped, userDef1, userDef2, userDef3.', 'Case 5:', 'Description: Given specification attributes and an object type, inp_spec_attr creates clusters of products with the same specifications.', 'Input: JSON format with the following keys: Specification Attributes, Object Type, Debug Information, userDef1, userDef2, userDef3.', 'Output: JSON format with the following keys: PAR ID, Object Name, Object Type, Cluster ID, Specifications, userDef1, userDef2, userDef3.', 'Case 6:', 'Description: Based on the object name or PAR ID entered by the user, inp_prodname_model creates clusters of products with similar specifications.', 'Input: JSON format with the following keys: Object Name, PAR ID, Debug Information, userDef1, userDef2, userDef3.', 'Output: JSON format with the following keys: Object Name, Object Types, PAR ID, Specifications, Rank, userDef1, userDef2, userDef3.', 'The APIs for all the above cases have been created', 'Deliverables', 'The code of the recommendation engine and its API is been delivered.', 'Tools used', 'Python, Postman', 'Language/techniques used', 'Python, Machine Learning, Flask API, Pandas', 'Models used', 'Affinity Propagation is a clustering algorithm that does not require a predefined number of clusters. It is used to group products based on their similarities.', 'Skills used', 'Python, Logical Reasoning, Machine Learning, Data Engineering.', 'What are the technical Challenges Faced during Project Execution', 'Product has many features but there was one feature which Is a “product type” that needs to be handled differently because it was important to have the product in a cluster must have the same product type.', 'Some cases can’t be solved with machine learning algorithms.', 'How the Technical Challenges were Solved', 'Handling Product Type as a Differentiating Feature: One of the challenges faced was dealing with the “product type” feature, which required special consideration. It was crucial to ensure that products with the same type were grouped together in the clustering or recommendation algorithm. This required developing a specific approach to address the uniqueness of the product type feature and incorporate it effectively into the recommendation system. Custom modifications and additional preprocessing steps were likely needed to accommodate this requirement and ensure accurate clustering based on product type.', 'Limitations of Machine Learning Algorithms: While machine learning algorithms are powerful tools for recommendation systems, there are cases where they may not be sufficient to solve certain challenges. During the project, it was likely discovered that some complex scenarios couldn’t be adequately addressed using traditional machine learning algorithms alone. To overcome this, alternative techniques and approaches beyond the scope of standard algorithms needed to be explored. This might involve incorporating domain-specific rules, utilizing other data analysis methods, or considering hybrid models that combine machine learning with expert knowledge to overcome the limitations and improve the recommendation system’s performance.', 'Business Impact', 'This recommendation engine can significantly enhance customers’ shopping experience by increasing the likelihood of them discovering products that perfectly align with their preferences. This personalized approach not only saves them valuable time and effort in searching for relevant items but also ensures that their unique needs and desires are met. As a result, customers are more likely to make purchases, leading to increased sales and revenue for the business.', 'Moreover, this recommendation engine plays a crucial role in improving customer satisfaction and fostering long-term loyalty. By suggesting products based on individual preferences and specific features, customers feel understood and valued. This tailored experience enhances their overall satisfaction, making them more inclined to return to the business for future purchases. Additionally, satisfied customers are more likely to spread positive word-of-mouth, attracting new customers and expanding their customer base.', 'Project Snapshots', 'Summarize', 'Summarized: https://blackcoffer.com/', 'This project was done by the Blackcoffer Team, a Global IT Consulting firm.', 'Contact Details', 'This solution was designed and developed by Blackcoffer Team', 'Here are my contact details:', 'Firm Name: Blackcoffer Pvt. Ltd.', 'Firm Website: www.blackcoffer.com', 'Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'Previous article', 'Chatbot using VoiceFlow', 'Next article', 'Dockerize the AWS Lambda for serverless architecture', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2035,https://insights.blackcoffer.com/database-discovery-tool-using-openai/,"Database Discovery Tool using OpenAI - Blackcoffer Insights-['Client Background', 'Client:', 'A leading retail firm in the USA', 'Industry Type:', 'Retail', 'Products & Services:', 'Retail Business, e-commerce', 'Organization Size:', '100+', 'Problem Statement:', 'Organizations often face challenges in managing and understanding their vast and complex databases. As data infrastructure evolves, new databases are introduced, and existing ones are modified, leading to a lack of comprehensive visibility into the entire data landscape. This lack of awareness poses several issues, including increased difficulty in ensuring data quality, security vulnerabilities, and inefficiencies in database administration.', 'To address these challenges, there is a need for a Database Discovery Tool using OpenAI, aimed at providing an automated and intelligent solution for discovering, cataloging, and understanding the various databases within an organization’s ecosystem.', 'Key Problems to Solve:', 'Database Proliferation:', 'Challenge:', 'The rapid growth of databases within an organization makes it challenging to keep track of all data storage systems.', 'Impact:', 'Increased difficulty in managing, securing, and optimizing databases.', 'Data Schema Variability:', 'Challenge:', 'Databases often have diverse schemas, making it hard to understand the structure of stored data.', 'Impact:', 'Inefficient data integration and difficulty in ensuring data consistency across the organization.', 'Limited Metadata Documentation:', 'Challenge:', 'Lack of comprehensive metadata documentation for databases, including information about tables, columns, relationships, and data types.', 'Impact:', 'Time-consuming manual efforts for understanding data structures and dependencies.', 'Security and Compliance Risks:', 'Challenge:', 'Inability to identify and monitor sensitive data across databases may lead to security and compliance risks.', 'Impact:', 'Increased likelihood of data breaches and non-compliance with regulatory standards.', 'Operational Inefficiencies:', 'Challenge:', 'Manual efforts required for discovering and documenting databases result in operational inefficiencies.', 'Impact:', 'Increased workload for database administrators, leading to potential errors and delays.', 'Lack of Intelligent Insights:', 'Challenge:', 'Absence of intelligent insights into database usage patterns, performance metrics, and optimization opportunities.', 'Impact:', 'Missed opportunities for improving database performance and resource utilization.', 'Proposed Solution:', 'Develop an OpenAI-powered Database Discovery Tool that leverages natural language processing (NLP) and machine learning capabilities to automatically discover, catalog, and provide insights into the organization’s databases. The tool should be able to:', 'Automatically scan and identify databases across different environments.', 'Extract and catalog metadata, including schema details, relationships, and data types.', 'Provide intelligent insights into database usage patterns and performance metrics.', 'Identify and classify sensitive data for enhanced security and compliance.', 'Enable efficient search and navigation of the entire database landscape.', 'Support ongoing updates and synchronization with changes in the data infrastructure.', 'By addressing these challenges, the Database Discovery Tool using OpenAI aims to empower organizations with a holistic view of their data landscape, facilitating better management, security, and optimization of databases.', 'Solution Architecture', 'Step by Step Execution', 'Step 1', '. Database Support', 'In this step we communicate with different types of databases, like SQL and Oracle. This means it can connect and retrieve information from a variety of database systems using Python, providing users with more flexibility and compatibility across various database environments.', 'Step 2', '. Data Extraction', 'In this step we are using python for our Extract, Transform, Load (ETL) processes this involves efficiently reading and extracting data from the connected databases. Python handled the data-related tasks, ensuring a robust and effective extraction process and save the result in csv files which in turn are converted to .db files for sqlite.', 'Step 3', '. Fine-Tuning', 'In this step fine-tuning mechanisms to optimize the performance and accuracy of data extraction processes. This Ensures the ETL tool finds data accurately and quickly.', 'Step 4', '. Integration with OpenAI', 'In this step we have utilized SQL Agent for communication with OpenAI, By communicating with OpenAI, the SQL agent get the ability to understand and respond in a more intelligent and context-aware manner.', 'Step 5', '. API Integration', 'In this step we made Django API endpoints for requesting and receiving data. This means that external systems or applications can interact with the SQL Agent through OpenAI by sending requests and receiving responses through these APIs.', 'Step 6', '. Streamlit Frontend', 'In this step we made a streamlit frontend to chat with the SQL Agent. The user can ask question about the database and receive responses in form of insights.', 'Video Demo', 'Previous article', 'ML and AI-based insurance premium model to predict premium to be charged by the insurance company', 'Next article', 'Chatbot using VoiceFlow', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2036,https://insights.blackcoffer.com/automate-the-data-management-process/,"Automate the Data Management Process - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Services:', 'SaaS, Products', 'Organization Size:', '100+', 'Project Description', 'Businesses now have more access to data than ever before in today’s digital economy. This information is utilised to make key business choices. Businesses should invest in data management systems that increase visibility, dependability, security, and scalability to ensure that workers have the required data for decision-making. The client wanted to get the data management process automated using a tool from Python. Multiple operations like merging,sorting, filtering had to be performed on data from various resources. The data resources were mainly csv files and data from SQL queries in PostgreSQL.', 'Our Solution', 'The project solution contained two tools that would aid in automatic efficient data storage. The first tool will concatenate all of the CSV files before merging them with the data from the SQL file. The acquired Excel file will be used as input for the second tool. The second tool will sort, filter, and lookup the Excel file received in the first tool. This tool will add columns that will be useful for the client’s analysis. The major goal is to assist the client with data management while requiring as little manual labour as possible. The files obtain the needed data in an Excel file by giving the proper input files.', 'Project Deliverables', 'The project deliverables can be divided into two parts:', 'Excel Tool1:', 'ExcelTool1 generates an Excel file that contains two sheets RSLTS IN and RSLTS OUT. The RSLTS IN is obtained by concatenating all the csv files in the Output folder. The RSLTS OUT is the result of merging the data from vwr egeas.sql query and RSLTS IN.', 'Excel Tool2:', 'Excel Tool2 creates another Excel file with one sheet RSLTS and csv files like vwr_instructions_new table, vwr proto and INST_RTR. This tool performs excel operations like lookups, arithmetic calculations and merging of data from multiple sources.', 'Tools used', 'For the whole data management and automation, we have made our own tool by python scripts.', 'PostgreSQL was used to merge the csv files provided by the client with the python scripts.', 'The automation tool will store data in the excel sheets.', 'Language/techniques used', 'PyCharm for compiling and running the code.', 'The scripts for the automation tool were written in the Python programming language.', 'OS, glob, pandas, numpy and psycopg2 were thePython libraries used in the project.', 'Skills used', 'Configuration and Data moving using PostgreSQL.', 'Automation of tools', 'Exception Handling from Python', 'Databases used', 'Two types of databases were used: Google excel sheets and PostgreSQL.', 'What are the technical Challenges Faced during Project Execution', 'Some minor challenges were faced such as data discrepancies generated during the automation process.', 'How the Technical Challenges were Solved', 'The challenges were solved by reworking on the automation tool and consulting with the clients for their requirements.', 'Business Impact', 'It is critical to use appropriate data management procedures to ensure the smooth running of a firm. Furthermore, data management must be very precise, cost-effective, and completed as soon as possible. The inability to handle data can result in costly consequences and a permanent stain on the company’s image. Every company is responsible for developing a robust data management plan. The following are some of the reasons why data management is critical to the success of the firm. Instant Availability of Information: Data management makes information easily available for quick access based on company needs. Data management is also essential for accounting procedures like auditing and other strategy-based operations like company planning. The more time you spend hunting for misplaced files and missing documents, the less productive you will be. And you are aware that time is money. Keeping all of your documents structured might therefore assist to make procedures run more smoothly and quickly. Compliance: The government passed legislation requiring businesses to maintain these data. There are also periodical checks to verify that there is no manipulation. Furthermore, if a corporation is involved in a dispute, they must maintain these records for years until a solid verdict on the matter is reached. Faster Transitions to New Technology: Because technology trends change so quickly, organizations must embrace whatever comes their way. Losing information due to obsolete or outdated systems is the last thing you want for your company. Every piece of data preserved in the firm records is essential for everyday operations, managing multiple divisions, completing computations, audits, and so on. Make Right Business Decisions: Businesses use a variety of information sources for company planning, trend research, and performance management. To execute the same activity, different departments’ teams employ different sources of information. Because the legitimacy and precision of information are highly dependent on the source, analyzing several sources may have a detrimental influence on the organization. Robust data management prevents this from happening.', 'Project Snapshots', 'Fig.1: Python code of Exceltool1', 'Fig.2: Python code of Exceltool1', 'Fig.3: Python code of Exceltool2', 'Fig.4: Python code of csv tables', 'Fig.5: RSLTS_OUT worksheet in output Exceltool1', 'Fig.6: RSLTS worksheet in output Exceltool2', 'Fig.7: RSLTS worksheet in output Exceltool2', 'Fig.8: INST_RTR table as output from Exceltool2', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Realtime Kibana Dashboard for a financial tech firm', 'Next article', 'Rise of Internet Demand and its Impact on Communications and Alternatives by the Year 2035.', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2037,https://insights.blackcoffer.com/realtime-kibana-dashboard-for-a-financial-tech-firm/,"Realtime Kibana Dashboard for a financial tech firm - Blackcoffer Insights-['Client Background', 'Client:', 'A leading fintech firm in the USA', 'Industry Type:', 'Finance', 'Services:', 'Financial services', 'Organization Size:', '100+', 'Project Objective', 'Create a real-time Kibana dashboard to monitor the real-time movement and activities related to company/stock on the AWS to analyse data and get insights through dashboards to prevent due diligence. Dashboard should include visualizations of sentiments, FOIA requests, stock prices, volume, borrow rate, etc.', 'Project Description', 'Create real-time dashboards to get insights about the data and to analyse the relative change in different activities. Someone filing FOIA SEC request or FOIA FDA request and/or registering for conference calls might also have posted some negative tweets on tweeter to influence the market. Dashboard should display data of requests, sentiments, stock prices, etc on the same timeline, so that we will be able to observe the changes and relative changes with respect to time. Make separate dashboard for 2 stock symbols to analyse the activities and changes specific to that and a dashboard for all the data, eg. stocks, requests, etc. Change in sentiments effecting the price of the stock, borrow rate, trading volume, etc. should be noticeable. There is a list of names, make alert on the dashboard when the requests are filed by them on the same timeline used for other data. Also include the candlestick chart to view the stock details like open, close, high, low, volume with respect to time.', 'Our Solution', 'For FOIA SEC and FDA requests, made a metric chart representing the total number of requests and requesters, created a date histogram to view the frequency of requests and requesters with respect to time, bar chart to view the top requester name, organization, category, pie chart to view the proportion of final disposition of requests and tag cloud for the description of the requests for the entries present in the selected time range and a search table that contains the selected columns (only relevant ones) for both SEC filings and FDA filings.', 'Similarly, for citation data, created a date histogram to view the frequency of citations and names of firms who posted with respect to time and bar chart to view number of citations by firm in the selected time range and a search table that contains the selected columns (only relevant ones). Index containing fail to deliver data is used to plot the date histogram in which volume failed is represented by the bar along the line representing the price at that time, bar chart where bars represents the total volume failed to deliver with respect to stock symbol and average price of the stock symbol in the selected time range by a dot size add on and tag cloud of the stock symbol as per fail to delivers.', 'For twitter data (short seller’s data), made a pie chart to show the proportion of polarity, metric table to show the highest 10 average retweets with respect to user name, made a date histogram to show the frequency of tweets as per time and another date histogram representing the amount of positive and negative sentiments with the help of bars as per time to leverage us to observe if change in amount of sentiments is affecting price of stock, volume in trade and fail to deliver, etc., bar chart to show the total posts and number of posts in the selected time range and another bar chart to show the count of followers and friends in the index in selected time range. A search table is made with columns like polarity, follower counts, retweets and post with timestamp to get precise info of what we have in visualizations.', 'For the list of names to be tracked on requests made and to make alert for them, added a annotation on the TSVB graph and added all of these along with the above visualizations on the dashboard on Kibana to make it a real-time dashboard and we can use this dashboard to do relative analysis.', 'For the dedicated dashboards to the stock, created and added following visualizations:', 'Metric to show number of requests and requesters in FOIA SEC and FDA indexes where description contains terms related to that stock symbol or product of the company.', 'TSVB of FOIA SEC and FDA and added annotation where the request against the stock or company is filed.', 'Fail to deliver and price on the same timeline to notice the relative change.', 'Sentiment and stock details is to be added in these but the data isn’t ready yet from the client’s end.', 'Project Deliverables', '3 dashboards- 1 dashboard for complete data and 2 dashboards dedicatedly for one stock each.', 'Tools used', 'Kibana and Elasticsearch', 'Skills used', 'Visualizations and analytical skills were used', 'Databases used', 'Following databases are used to:', 'FOIA SEC filings', 'FOIA FDA filings', 'Citations', 'Fail to deliver', 'Tweeter Short seller data', 'Stock price', 'Web Cloud Servers used', 'AWS Management Console', 'What are the technical Challenges Faced during Project Execution', 'As I was using Kibana and studying the stock data for the first time, I faced challenges in making complex visualizations and understanding the terms related to stock data. Using filters while making Vega Charts to make candlestick chart with inconsistent data was displeasing.', 'How the Technical Challenges were Solved', 'Challenges related to the creation of complex visualization was solved exploring options on the Kibana and getting reference from the online sources. In order to understand the stock information and how things work, I got immense amount of knowledge from the client and from my project manager. For filtering of data in Vega charts I took help from the online sources.', 'Project Snapshots', 'Project website url', 'https://search-r2-analytics-elasticsearch-7ikdbjjl6wpkvryfdq65wxh3iq.us-east-1.es.amazonaws.com/_plugin/kibana/goto/33529a85d7949871c0833dab8c3b3322', 'https://search-r2-analytics-elasticsearch-7ikdbjjl6wpkvryfdq65wxh3iq.us-east-1.es.amazonaws.com/_plugin/kibana/goto/255f9ffe21bb76f96d1be5d49c7f75a7', 'https://search-r2-analytics-elasticsearch-7ikdbjjl6wpkvryfdq65wxh3iq.us-east-1.es.amazonaws.com/_plugin/kibana/goto/f8f6ad6a627f6f74ce2a775288bdbc5c', 'Project Video', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'How To Secure (SSL) Nginx with Let’s Encrypt on Ubuntu (Cloud VM, GCP, AWS, Azure, Linode) and Add Domain', 'Next article', 'Automate the Data Management Process', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2038,https://insights.blackcoffer.com/data-management-etl-and-data-automation/,"Data Management, ETL, and Data Automation - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Services:', 'SaaS, Products', 'Organization Size:', '100+', 'Project Objective', 'To extract the data for the given keywords from the listed websites', 'https://www.ferguson.com/', ',', 'https://www.bakerdist.com/', ',', 'https://www.hajoca.com/', ',', 'https://www.carrier.com/residential/en/us/', ',', 'https://www.gemaire.com/', ',', 'https://www.fwwebb.com/', 'and store the count of each keywords for each website it in an Excel File.', 'Project Description', 'A list of websites is provided from which we were supposed to find out the mentioned keywords and store their respective counts for each website in an Excel sheet with different tabs for different set of keywords.', 'Our Solution', 'We used Selenium as well as Bs4(Beautiful Soup) to extract data from the given websites. To accomplish the given task, 2 tools were developed for each website.', 'Search tool was developed to search the keyword in the website’s search bar and count displayed for keywords of each category was stored in separate files.', 'Content tool was developed which scraped full text from each url obtained from the respective sitemaps. Along with the text visible on the page, data from meta keywords, meta description and title was also scrapped.', 'Extracted content from all the websites was stored in their respective text files. After that number of keywords in the text were counted using substring and count method and stored the keyword and its corresponding count in an Ordered Dictionary and then the count was transferred to a list and Excel file was created for the same. Counts received from search tool and content tool were combined and final output file was created.', 'Project Deliverables', 'Python Scripts for each website to extract the count of keywords.', 'Excel Sheet name HVAC_Report Test.xlsx having counts for each set of keywords for each website.', 'Tools used', 'Python Interpreter', 'Language/techniques used', 'Language Used: Python', 'Libraries used: BeautifulSoup, collection.OrderedDict, pandas, requests, xlsxwriter, selenium.webdriver', 'What are the technical Challenges Faced during Project Execution', 'Some of the websites cannot be accessed using Indian IP address as it was having captchas. Also, we cannot go to each and every page by clicking the results and get the count.', 'How the Technical Challenges were Solved', 'To bypass the captcha and reach the website, we need to use VPN of Singapore. And to get access to each and every page of the website, we found out sitemap for each website which includes link to every page present in it.', 'Project Snapshots', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Data Management – EGEAS', 'Next article', 'Deploy Nodejs app on a cloud VM such as GCP, AWS, Azure, Linode', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2039,https://insights.blackcoffer.com/data-management-egeas/,"Data Management - EGEAS - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Services:', 'SaaS, Products', 'Organization Size:', '100+', 'Project Objective', 'To extract various Reports from the given input files. Reports to be extracted are: PRODUCTION COST – ANNUAL BY UNITS REPORT, SYSTEM EMISSIONS ANNUAL REPORT, RPS CONSTRAINT – ANNUAL REPORT, RELIABILITY – ANNUAL REPORT, RESERVE – ANNUAL REPORT and CAPACITY TOTALS ANNUAL REPORT. We had to extract above mentioned reports from the given .out files and store it in the respective .csv files.', 'Project Description', 'We were given a bunch of .out files in which various Reports were available in table format. We need to extract some of the required reports from the given files and store them in their respective .csv files. A tool had to be developed in python in order to accomplish this task.', 'Our Solution', 'From each .out file its content extracted and stored in a list. Using regular expression, we searched the required report in the content. Another regular expression is used to mark as end of the table content. Content between the two given regular expressions is stored in a dataframe which is then stored into respective .csv file.', 'Project Deliverables', 'Python Scripts for each report and a combined script which could extract all the required Reports.', 'Respective .csv files of the Reports', 'Tools used', 'Python Interpreter', 'Language/techniques used', 'Language Used: Python', 'Libraries Used: re, pandas, os', 'Skills used', 'Programming', 'Project Snapshots', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Design and develop PowerShell script', 'Next article', 'Data Management, ETL, and Data Automation', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2040,https://insights.blackcoffer.com/design-and-develop-powershell-script/,"Design and develop PowerShell script - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Services:', 'SaaS, Products', 'Organization Size:', '100+', 'The Problem', 'Create a PowerShell script for the following:', 'check and enable auditing:- client wanted a PowerShell script that checks NTFS Rule is given to a folder or not and adds a rule to it', 'configuring winrm for remote windows server:- this client wanted a PowerShell script which helps us to connect to another windows remote server', 'check audit of windows/system32 folder and windows/inf folder of remote windows server:- this client wanted a PowerShell script which help us to connect to the remote server and check their\xa0 NTFS Rule for windows/system32 and windows/inf folder also we can add rule for those folders', 'Our Solution', 'check and enable auditing', 'for checking and enabling auditing of the file we used\xa0 PowerShell NTFSSecurity module', 'for checking the audit we used Get-NTFSAudit which is a submodule of NTFSSecurity', 'for adding the audit we used Add-NTFSAudit which is a submodule of NTFSSecutiry', 'configuring winrm for remote windows server', 'For this we created 2 script:', 'create script: this help us to create listener and open port 5986 for http as winrm uses port 5986 to connect with windows', 'connect script: this help us to connect with remote windows server for this purpose we used Enter-PSSession', 'check audit of windows/system32 folder and windows/inf folder of remote windows server', 'for this, we created a script that connects to the remote windows server using the Enter-PSSession command and then checks the audit for windows/system32 and windows/inf folder also we can add audit rule to windows/system32 and windows/inf folder from remote servers', 'Deliverables', 'Powershell script', 'Tools used', 'VS Code IDE', 'Powershell', 'Virtual machine', 'Language/techniques used', 'powershell', 'Skills used', 'Powershell', 'BuProject Snapshots', 'Check audit', 'Add audit', 'Check audit', 'Before running create script', 'Create script for winrm listner', 'List of listeners after running create script', 'Connect with remote machine', 'When rights are not applied', 'When rights are applied', 'Project Video', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Design and develop Jenkins shared library', 'Next article', 'Data Management – EGEAS', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2041,https://insights.blackcoffer.com/design-and-develop-jenkins-shared-library/,"Design and develop Jenkins shared library - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Services:', 'SaaS, Products', 'Organization Size:', '100+', 'The Problem', 'Create Jenkins shared library for the following:', 'validate AWS AMI creation', 'check if network rules exist in aws EC2', 'check if the security group in aws EC2', 'Our Solution', 'We created a Jenkins shared library in which we are using AWS\xa0 ec2 describe-images command with the help of aws cli if an ami don’t exist than describe-images throws error', 'We created a Jenkins shared library in which we are using aws ec2 describe-network-acls\xa0 for validating we were comparing input name with VPC', 'We created a Jenkins shared library in which we are using aws ec2\xa0 describe-instances for validating we were checking input name with SecurityGroups group', 'Deliverables', 'Jenkins Libraries', 'Tools used', 'VS Code IDE', 'Jenkins', 'AWS', 'Language/techniques used', 'Grovvy', 'Skills used', 'Jenkins', 'AWS Server', 'Web Cloud Servers used', 'AWS', 'Project Snapshots', 'Project Video', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Design and develop retool app for wholecell.io and Asana data using their api’s', 'Next article', 'Design and develop PowerShell script', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2042,https://insights.blackcoffer.com/design-and-develop-retool-app-for-wholecell-io-and-asana-data-using-their-apis/,"Design and develop retool app for wholecell.io and Asana data using their api’s - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Services:', 'SaaS, Products', 'Organization Size:', '100+', 'The Problem', 'Create retool app for wholecell.io and Asana data using their api’s', 'Our Solution', 'We have created two table one table contain data from wholecell.io platform and another table contain data from Assna.', 'In that wholecell.io table we are providing:', 'Order id', 'Order status', 'Order channel', 'Organization', 'Link of the Order', 'In Assna Table we are providing following details:', 'Id of the task', 'Name of the task', 'Resource type', 'Resource_subtype', 'Caller', 'Po-id', 'As client data from wholecell and Assna was linked client can search the order by PO-id in Assna table', 'Deliverables', 'App in retool', 'Tools used', 'Retool', 'Language/techniques used', 'JavaScript', 'Skills used', 'Retool', 'API integration', 'JavaScript', 'What are the technical Challenges Faced during Project Execution', 'Api was not providing all required details according to the client requirement and there were less options for data pre-processing as retool only javascript', 'How the Technical Challenges were Solved', 'We had fetched details from one api and provide id to the other api using JavaScript this was done by using javascript promise method', 'We also had to do some string manipulation to get data according the client requirement', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Design and develop a retool app that will show stock and crypto related information using IEX API', 'Next article', 'Design and develop Jenkins shared library', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2043,https://insights.blackcoffer.com/design-and-develop-a-retool-app-that-will-show-stock-and-crypto-related-information-using-iex-api/,"Design and develop a retool app that will show stock and crypto related information using IEX API - Blackcoffer Insights-['Client Background', 'Client:', 'A leading fintech firm in the USA', 'Industry Type:', 'Finance', 'Services:', 'Crypto, financial services, banking, trading, stock markets', 'Organization Size:', '100+', 'The Problem', 'Create a retool app that will show stock and crypto related information using IEX API', 'Our Solution', 'Created a flask web application with following features and pages:', 'Page 1 (Home page)', '– Show a Stock & Crypto Search Bar that will show the most relevant option in the IEX API via ticker search. Upon submit, user will be taken to the “Ticker Page”', '– List the 10 top trending stocks for each category (link click to ticker page)', '(logo, Stock ticker, company name, stock price, % change.', 'Mega Cap', 'Large Cap', 'Mid Cap', 'Small Cap', 'Micro Cap', 'Page 2 (Ticker Page)', '-Show Company Data – (Ticker, Company Name, Logo, Market Cap, and all the other corporate data (employees, CEO, HQ, Founded, Website)', '-Stock Price Chart – 1 year chart, daily.', '-Stock Price Volume – Weekly average 20 weeks', '-Recent News – list of 25 most recent articles', 'Deliverables', 'Deployed flask web application on AWS', 'Tools used', 'VS Code IDE', 'Nginx', 'Language/techniques used', 'Python', 'Skills used', 'API Integration', 'Python', 'AWS Server', 'Nginx', 'Web Cloud Servers used', 'AWS', 'What are the technical Challenges Faced during Project Execution', 'There was lots of pre-processing required to create application as per client requirement', 'How the Technical Challenges were Solved', 'We shifted the application from retool to python flask application as python programming language allow as to pre-process the data as per our requirement', 'Project Snapshots', 'Project website url', 'www.stocks.bullish.studio', 'Project Video', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'CRM (Monday.com, Make.com) to Data Warehouse to Klipfolio Dashboard', 'Next article', 'Design and develop retool app for wholecell.io and Asana data using their api’s', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2044,https://insights.blackcoffer.com/crm-monday-com-make-com-to-data-warehouse-to-klipfolio-dashboard/,"CRM (Monday.com, Make.com) to Data Warehouse to Klipfolio Dashboard - Blackcoffer Insights-['Client Background', 'Client:', 'A leading marketing firm in the USA', 'Industry Type:', 'IT', 'Services:', 'Marketing, promotions, campaigns, consulting, business growth', 'Organization Size:', '100+', 'The Problem', 'The client requires a dashboard for a ”week in review” and “human resources”. The dashboard should be dynamic whenever the client opens the dashboard, it should show the current week and should also have a dropdown choice option based on different time periods. So the client requires a meaningful KPI on the dashboard.', 'Research Objective', 'Taking the problem statement into consideration the following objectives are established.', 'Objective 1: Getting access to the Monday.com site, Make.com, Google sheet, and Klipfolio. Objective 2: Connect Monday.com data to the Google sheet.', 'Objective 3: Data Integration using make.com.', 'Objective 4: Building KPIs using various calculations and formulas to get meaningful insights.', 'Objective 5: Creating a dashboard from insight driven by KPIs.', 'Solution Architecture', '1. Data Integration', 'Fig.3.4: Data Integration', '2. Overall Architecture', 'Fig.3.4.2 Overall Architecture', 'Tools used', 'Klipfolio', 'make.com', 'Language/techniques used', 'Klip Formula', 'Skills used', 'Data Integration', 'Data Processing', 'Data Visualization', 'Web Cloud Servers used', 'Google Sheet', 'What are the technical Challenges Faced during Project Execution', 'During the project execution we faced following challenges:', '1. Mapping the values in make.com from Monday.com', '2. Whenever the update is generated on Monday.com, a new row is added to the Google sheet.', '3. Extracting insights from the data', 'How the Technical Challenges were Solved', 'To solve the technical challenges, we provided following solutions as follow:', '1. For mapping the values from Monday.com to make.com, we got access as admin to reach out the columns id on Monday.com.', '2. On make.com, we created multiple models linking each other based on the row id in the google sheet.', '3. After completing the data integration, we use calculations to extract meaningful insights from the data.', 'Business Impact', 'Using this dashboard, a client can keep track of the employee’s work process. So he can analyze employee workflow nature.', 'Project Snapshots', 'Project website url', 'Google Sheet:', 'https://docs.google.com/spreadsheets/d/15ADtNWh63O7DVbg-FRH0SmWb-TemqldOVK7dq16N7Xs/edit?usp=sharing', 'Data Integration using make.com:', 'https://us1.make.com/146703/scenarios?folder=all&tab=all', 'Monday.com:', 'https://primus-business-management.monday.com/', 'List Of Employees listed on Klipfolio:', 'https://app.klipfolio.com/clients/index', 'Klipfolio Dashboard:', 'https://app.klipfolio.com/dashboard?tab=012f404bf82f8b4e331c4a0c48d32978#:~:text=https%3A//app.klipfolio.com/dashboard/add_tab/8ca9ae6808284b158f640834f3e2afd8%3Fparam%3AstartDate%3D1671926400%26param%3ADatepickerB%3D1671753600%26param%3ADatePickerA%3D1671408000%26param%3Adropdown%3DWorking%20on%20it%26param%3AendDate%3D1672444800%26param%3AKTdate%3DFY%20to%20Last%20month%26param%3ADatePeriodq%3DThis%20Week', 'Project Video', 'Todo Board Part 1:', 'https://www.youtube.com/watch?v=qnTV64RhGWk', 'Todo Board Part 2:', 'https://www.youtube.com/watch?v=vDyaVkNv6bU', 'Todo Board part 3:', 'https://www.youtube.com/watch?v=FciSkP-uRkM', 'Census Board Part 1:', 'https://www.youtube.com/watch?v=jpgzakxdvZw', 'Census Board Part 2:', 'https://www.youtube.com/watch?v=3y6DmUGNmTE', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'NER Task using BERT with data in XML-format', 'Next article', 'Design and develop a retool app that will show stock and crypto related information using IEX API', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2045,https://insights.blackcoffer.com/ner-task-using-bert-with-data-in-xml-format/,"NER Task using BERT with data in XML-format - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Services:', 'SaaS, Products', 'Organization Size:', '100+', 'The Problem', 'The goal of this task is to create and implement a workflow that annotates People/Places/Organizations and assigns them a specific number (from a normdatabase). The NER-Task should be done by using Bert (NER-German', 'https://huggingface.co/flair/ner-german', 'or something similar).', 'Our Solution', 'The input to this first task is a text in XML-Format. It is important that the structuring text is not altered by the NER. This could be possible by tokenizing the XML-elements in a different/seperate way, to then run the NER with BERT and afterwards add the elements afterwards at the exact position where the initially were. The tags that were added by the NER than can be easily replaced with the required tags in the XML-format.', 'Solution Architecture', 'Input Data 🡪 XML Text Tokenization 🡪 NER Model 🡪 Replace NER Tags with XML Tags 🡪 Final Output', 'Deliverables', 'Python tool', 'Documentation', 'Installation', 'Tools used', 'VSCode For Python script', 'Language/techniques used', 'Python Programming Language', 'Models used', 'Named Entity Recognition (NER)', 'FuzzyWuzzy', 'tqdm', 'Flair', 'Pandas', 'Skills used', 'Data Loading', 'Data Processing', 'Data Restoring', 'What are the technical Challenges Faced during Project Execution', 'During the project execution, we faced the following challenges:', 'Parsing of the input XML file.', 'Predicting the Name, Place and Organization.', 'Rearranging the XML file to its origin form with the predicted value.', 'How the Technical Challenges were Solved', 'To solve the technical challenges, we provided following solutions as follow:', 'It was not possible by the beautiful soup library. So by using the logically function start index and end index we break the sentence.', 'For predicting the NPO we used the flair ner-german model.', 'To rearrange the file we used start index and end index function which can be split with a certain condition and we place the predicted value in it.', 'Business Impact', 'The client can know easily predict the Name, Place, and Organisation from XML containing file by using our python script model.', 'Project Snapshots', 'Fig. Input XML file', 'Fig. Output XML file with predicted values.', 'Project website url', 'Github: https://github.com/AjayBidyarthy/Sven-Meier-XML-tool/tree/master', 'Project Video', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Qualtrics API integration using Python', 'Next article', 'CRM (Monday.com, Make.com) to Data Warehouse to Klipfolio Dashboard', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2046,https://insights.blackcoffer.com/qualtrics-api-integration-using-python/,"Qualtrics API integration using Python - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Services:', 'SaaS, Products', 'Organization Size:', '100+', 'The Problem', 'API Integration to read/write data in SQL tables from an online application.', 'Our Solution', 'To write the api between qualtrics and sql server using python programming language.', 'Solution Architecture', 'Fig. System Architecture', 'Deliverables', 'Python Software', 'Documentation', 'Tools used', 'Python', 'Qualtrics', 'Models used', 'Pandas', 'Requests', 'numpy', 'Zipfile', 'io', 'pyodbc', 'Skills used', 'Extract Transfer Load', 'Databases used', 'SQL Server', 'What are the technical Challenges Faced during Project Execution', 'During the project execution, we faced the following challenges:', 'After data integration, the content of the file was not readable.', 'Mapping the values with the required columns.', 'How the Technical Challenges were Solved', 'To solve the technical challenges, we provided the following solutions as follow:', 'To get the content into the CSV format after integration we used the Io module to get the text content.', 'To get the mapping values we created the CSV file and store the record in it and fetch that record to the SQl.', 'Business Impact', 'Using this script the client can now fetch the Qualtrics data into the SQL server automatically after every 1 hour.', 'Project Snapshots', 'Fig. Data in CSV Format', 'Fig. Data in Table form', 'Fig. SQL data', 'Project website url', 'Github:\xa0 https://github.com/AjayBidyarthy/Richi-S-api', 'Project Video', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Design and develop MLops framework for Data-centric AI', 'Next article', 'NER Task using BERT with data in XML-format', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2047,https://insights.blackcoffer.com/design-and-develop-mlops-framework-for-data-centric-ai/,"Design and develop MLops framework for Data-centric AI - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Services:', 'SaaS, Products', 'Organization Size:', '100+', 'The Problem', 'The task involves finding models and tools for several different tasks across various domains. The tasks include video and image capturing, working with documents such as PDF and Excel files, converting text to audio, audio capturing and transcription, translation to major languages, utilizing language models with a focus on Jina finetuner and its limitations, creative AI for generating pictures and designs, synthesizing language texts, creating Kibana dashboards and data storytelling, code creation for specific platforms like Editorjs and Nextjs, integrating Jina API inference into function blocks in Editorjs/Nextjs, UX/UI creation for the front end of Editorjs and Nextjs, transfer learning and reinforcement learning, utilizing Wikipedia for general knowledge, and utilizing an epistemic model called EPINET. To fulfill this task, you will need to search for relevant models, tools, and resources specific to each task mentioned above.', 'Our Solution', 'Jina AI Hub to deliver an ecosystem of:', 'Core transformer model', 'Distilled & Fine tuned models', 'OKR:s/KPI:s +domain data = “Book of knowledge” + model = AI agents', 'Ensembled models = AI teams', 'Also delivered functions in the marketplace', 'Voice interface, OpenAI Whisper transformer', 'Multiple data types capturing of information (DocArray)', 'CLIP model to mesh multiple data types into vectors', 'Neural Search function and', 'Generative AI function', 'Automatic data labelling', 'Used weight watcher to fine tune the model quality without CPU/GPU cost', 'Solution Architecture', 'Automatic selection a model for fine tuning with data corpus (book of knowledge), given the best performance.', 'Add the model to an API inference', 'Unlike ChatGPT the model can specify when they don’t know and acknowledge it instead of making stuff up with its creative ability.', 'When the model knows what it doesn’t know, it can ask to go back and consult other models for joint predictions.', 'Add a function to select ensembled models for joint prediction when step 4 occurs.', 'Deliverables', 'Identify core transformer models', '“Clean” and stabilize selected core models', 'Set up the process in Jina Hub', 'Integrate FastAPI/Jina with our Jina Hub', 'Integrate FastAPI/Argilla/Kibana into our Jina Hub', 'Tools used', 'Jina Hub/AI, Python, Hugging Face, Argilla, Redis stack, Kibana', 'Language/techniques used', 'Python', 'Models used', 'Epistemic Neural Nets, weight watcher, OpenAI Whisper transformer, Epinet', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'NLP-based Approach for Data Transformation', 'Next article', 'Qualtrics API integration using Python', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2048,https://insights.blackcoffer.com/nlp-based-approach-for-data-transformation/,"NLP-based Approach for Data Transformation - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Services:', 'SaaS, Products', 'Organization Size:', '100+', 'The Problem', 'Performing Readability and Quality testing on the text corpus from text files', 'Our Solution', 'The intention was to create a tool/system that can consume text files through a given csv file having a path for all the text files through this csv file our tool should be able to read all files one by one and could perform some tests and analysis on that text data and output the results in a csv format presenting all the metrics.', 'In order to achieve this goal we created a Python-based ready-to-use code that will read all text files presented in the given csv files and perform 14 different evaluations on that text data and save the results in a excel and csv based format.', 'Solution Architecture', 'Deliverables', 'The final deliverable was the tool/system/code for processing and evaluation text.', 'Language/techniques used', 'Python', 'Natural Language processing technique used for text evaluation', 'Skills used', 'Python Programming', 'What are the technical Challenges Faced during Project Execution', 'The architecture of the solution for this project problem statement was simple, no challenges were faced during the execution of the project.', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'An ETL tool to pull data from Shiphero to Google Bigquery Data Warehouse', 'Next article', 'Design and develop MLops framework for Data-centric AI', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2049,https://insights.blackcoffer.com/an-etl-tool-to-pull-data-from-shiphero-to-google-bigquery-data-warehouse/,"An ETL tool to pull data from Shiphero to Google Bigquery Data Warehouse - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Services:', 'SaaS, Products', 'Organization Size:', '100+', 'The Problem', 'Shiphero company is an organization providing shipping solutions to vendors. The data created by shiphero for different product picking and packing time period doesn’t provide much insight into the efficiency of ship hero employees and other aspects that are needed and useful for vendors/brands to make better decisions for their business in order words the', '‘key’', 'data is missing.', 'Our Solution', 'The solution is an effort to create the missing data by the existing data as we came to know that the', '‘key’', 'data can be created by involving some deep methodologies and vast logical aspects linked to it. The incoming data from shiphero company is timestamp data therefore using this sequential data we can create the missing data we need to get the required KPI’s.', 'The overall architecture included getting data from shiphero through api doing some preprocessing and creating our', '‘key’', 'through this data and populating it on Google big query. This google big query is linked to Google data studio for insights visualisation.', 'Solution Architecture', 'The data coming from Shiphero is extracted every day using a cron job scheduler. Google app engine service is used to preprocess and apply a transformation to the data.', 'Deliverables', 'Ready-to-use Google data studio Dashboard. Google app engine service-based scheduler code.', 'Tools used', 'Google App engine', 'Google big query', 'Google data studio', 'Google cloud platform', 'Language/techniques used', 'Python (for preprocessing)', 'GraphQL (For data extraction)', 'Skills used', 'Python Programming', 'GraphQL querying', 'Statistics', 'Data visualization', 'Data Engineering', 'Data Science', 'Databases used', 'Google big query', 'Web Cloud Servers used', 'Google Cloud platform', 'What are the technical Challenges Faced during Project Execution', 'Initially the approach client introduced that could be able to solve the problem directly failed to give proper results and because of that we need to come up with a solution that could be able to estimate our ‘key’ column to some extent.With the way around solution using statistics and data modelling there were a series of challenges coming that were creating a question mark for us but with keen solution building and delivering the desired results we came to solution for every challenge that arose.', 'How the Technical Challenges were Solved', 'Statistics was the only way around for the challenges we faced because it was the data which was missing and as the incoming data was in sequential format so we were able to figure out the patterns from that and the main problem of missing data for our KPI’s', 'Business Impact', 'Better insights into the business.', 'Project Snapshots', 'Dashboards aren’t finalised but yes giving desired solutions.', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Plaid Financial Analytics – A Data-Driven Dashboard to generate insights', 'Next article', 'NLP-based Approach for Data Transformation', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2050,https://insights.blackcoffer.com/plaid-financial-analytics-a-data-driven-dashboard-to-generate-insights/,"Plaid Financial Analytics - A Data-Driven Dashboard to generate insights - Blackcoffer Insights-['Client Background', 'Client:', 'A leading financial firm in the USA', 'Industry Type:', 'Finance', 'Services:', 'Financial Services', 'Organization Size:', '100+', 'The Problem', 'Applying automation to Financial data coming from the Plaid platform that needs to be visualized in order to get better insights and metrics from data.', 'Our Solution', 'The intention was to create an automation tool that could consume the financial csv format data and perform preprocessing on that data and could directly present the insights on visually appealing dashboard.', 'Initially the step was to create a tool/website that could consume the data and preprocess it and send it either directly to dashboard or into a database so the data could be safe and through the database the dashboard could be linked and updates accordingly.', 'The data source for the tool was to be a manual entry therefore we created a website and hosted it on a cloud platform(Heroku) to make it available all the time for all the desired users. The processed data from this tool will be send to the Google big query database and our GBQ will be linked to the Google Data Studio for the insights presentation. Therefore as the data will keep on updating in the google big query accordingly the dashboard in our google data studio will gets updated.', 'Solution Architecture', 'Deliverables', 'The final deliverable was the ready-to-use dashboard and website where the preprocessing of the data happens.', 'Tools used', 'Google Cloud platform – Google Big Query (Database)', 'Google Data studio(Visualisation/Dashboard)', 'Heroku Cloud(Hosting the web application)', 'Language/techniques used', 'Python', 'Skills used', 'Python programming', 'Data analytics/Visualisation', 'Google Big Query', 'Databases used', 'Google Big Query', 'Web Cloud Servers used', 'Heroku Cloud', 'What are the technical Challenges Faced during Project Execution', 'The project was easy to implement and the architecture was simple therefore no major challenges were encountered.', 'Project Snapshots', 'Project website url', 'https://plaid-conversion.herokuapp.com/', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Recommendation Engine for Insurance Sector to Expand Business in the Rural Area', 'Next article', 'An ETL tool to pull data from Shiphero to Google Bigquery Data Warehouse', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2051,https://insights.blackcoffer.com/recommendation-engine-for-insurance-sector-to-expand-business-in-the-rural-area/,"Recommendation Engine for Insurance Sector to Expand Business in the Rural Area - Blackcoffer Insights-['Client Background', 'Client:', 'A leading insurance firm in the globe', 'Industry Type:', 'Insurance', 'Services:', 'SaaS, Products, Insurance', 'Organization Size:', '10000+', 'Project Objective', 'Develop the recommendation engine', 'Item-based collaborative filtering based on the use case of the project', 'Work on Streaming data platform i.e BangDb', 'Data Generation for Testing the platform', 'Project Description', 'BangDB is the platform that manages the static data stored on the cluster and also works with live streaming data as Hadoop does. Wherever the bangdb is able to manage machine learning model deployment with their inbuilt parameter and hyper tuning parameters for each model.', 'Streaming data from the client which relates to the customer details and the numbers of products offered by the client on their platform, such as Insurance, loans (Business Loans and Personal Loans), Mobile recharge, UPI transactions done by their platform, etc.', 'They wanted the recommendation of other services provided by them to each of their customers who are using their platform.', 'Our Solution', 'This Project Module develops according to the Clients Requirements which involves item-based collaborative filtering based on customer behaviour, Firstly classify the customers into various segments on the basis of age, location, gender, and product usage. On the basis of RFM (marketing tactics to classify the customer on the basis of their purchase history, amount spend, and frequency of usage of product) classify them and recommend them the other services based on item-based collaborative filtering.', 'We generated the synthetic data (90 Million events) for the testing of the recommendation model and its accuracy for recommending the other products to customers.', 'Project Deliverables', '– \xa0 KPI of the Customers', '– \xa0 Recommendation model', '– \xa0 Graph databased model', '– \xa0 Data Generation code based on python (using copula-based on PyTorch)', 'Tools used', 'BangDb Tool (ML, AI, NoSQL database supported)', 'Graph Databased', 'Google Colab (Data file generation)', 'Tableau for data visualization', 'Language/techniques used', 'Linux cloud machine', 'Python', 'Graph Database', 'Data visualization tools', 'Models used', '-K means model for clustering', '-Recommendation Engine model', '-Collaborative based filtering model', 'Skills used', '– Machine learning', '– NoSQL Database', '– Graph database', '– Data Generation using python', '– Linux', '– Data Visualization', 'Databases used', '– BangDB', '– Graph Database', '– Microsoft MYSQL server', 'Web Cloud Servers used', 'AWS cloud service', 'What are the technical Challenges Faced during Project Execution', 'Decide the Recommendation Engine based on the use case', 'Finding the RFM score and classifying the customers into clusters', 'Graph Model to define the relations of customers with each service which they are using', 'Synthetic data generation(90 Million events) and around 1.5 Gb structured data.', 'How the Technical Challenges were Solved', 'Item-based collaborative filtering solves the issue of recommendation because we are dealing with almost 14- 15 services.', 'Clustering of customers based on their similarities', 'Measure the RFM score, and group and classify them based on their scores.', 'Graph database provides to reduce complexity and increase the processing speed.', 'Data generation is one of the difficult tasks and generating relational data across 29 different streams using copula and UUID python library function which is based on PyTorch.', 'Business Impact', 'It is Qualitative and Quantitative impact on economically where customers are a direct impact of these projects in their life.', 'It is suggesting to the customers what services they have to utilize from the provider and this is a direct impact of the product on the customers.', 'Product is providing the action statement of the usage of services by the customers and impacts them economically as well.', 'The scope impact of product service is Nationwide or statewide.', 'To provide these impact-full services, there is a tech team of Blackcoffer behind it', 'Project Snapshots', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Data from CRM via Zapier to Google Sheets (Dynamic) to PowerBI', 'Next article', 'Plaid Financial Analytics – A Data-Driven Dashboard to generate insights', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2052,https://insights.blackcoffer.com/data-from-crm-via-zapier-to-google-sheets-dynamic-to-powerbi/,"Data from CRM via Zapier to Google Sheets (Dynamic) to PowerBI - Blackcoffer Insights-['Client Background', 'Client:', 'A leading solar panel firm in the USA', 'Industry Type:', 'Energy', 'Services:', 'Solar Panel', 'Organization Size:', '500+', 'The Problem', 'Solar Panel organization from America wants to keep track of sales data. They want to see the leadership dashboard of their organization in terms of sales. They also want to keep track of their campaigns and leads generated from sources of those campaigns. They want to keep track of sales data from different sources.', 'Our Solution', 'First, we fetch the data from CRM to PowerBI. Clean the data of CRM using DAX and then perform calculations on the data. Using cleaned data, we build KPI on PowerBI.', 'Solution Architecture', 'To complete the project, we follow the following data flow pipeline:', 'Data from CRM 🡪 Zapier 🡪 Google Sheet (Dynamic) 🡪PowerBI', 'Language/techniques used', 'PowerBI, DAX Language', 'Skills used', 'CRM, Zapier , PowerBI, Google Sheet', 'What are the technical Challenges Faced during Project Execution', 'Challenges Faced during the Project Execution :', 'Fetching the data from CRM', 'Unclean Data', 'Merging the Data', 'How the Technical Challenges were Solved', 'Solution:', 'To Fetch the data from CRM. We used Zapier. It is connector between two applications so that whenever a particular incident happen it will populate into another application. We use Zapier to connect CRM and Google sheets so that whenever a new lead will change or modified data will be stored into google sheets.', 'Data in google sheets was uncleaned. First, we connect the Google sheet with PowerbI then perform EDA to clean the data using DAX language.', 'Using merging of two tables by ONE-ON-ONE schema we solve duplicate entries of a particular lead in PowerBI.', 'Business Impact', 'Using this Dashboard client can make important decisions like from which campaign they are getting a greater number of leads and out of those leads how many are actually a Sale. They can keep track of their sales leadership of employee of the month in term of sales.', 'Project Snapshots', 'CRM', 'Zapier', 'Dashboard', 'Project Video', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Data Warehouse to Google Data Studio (Looker) Dashboard', 'Next article', 'Recommendation Engine for Insurance Sector to Expand Business in the Rural Area', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2053,https://insights.blackcoffer.com/data-warehouse-to-google-data-studio-looker-dashboard/,"Data Warehouse to Google Data Studio (Looker) Dashboard - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Services:', 'SaaS, Products, healthcare, government, energy', 'Organization Size:', '100+', 'The Problem', 'Our client needed a Google Data Studio dashboard for different sectors such as Oil and Gas, Government, Healthcare, and Sales analysis. They want to see an analysis of data from which they can provide insights in different domains. They want us to create visual KPIs of meaningful insights.', 'Our Solution', 'They provided us with data for different sectors. Using those data first we analyze the data and perform EDA on data for cleaning the data. After cleaning the data, we performed calculations to extract insights for KPIs. Using those KPIs we build a dashboard on Oil and Gas, Government, Healthcare, and Sales analysis.', 'Solution Architecture', 'To build the dashboard we follow the pipeline as follows:', 'Data 🡪 EDA(Cleaning data )🡪 Connection(GDS) 🡪 Building KPIs(Visuals)', 'Tools used', 'Google Data Studio', 'Skills used', 'EDA, Google data studio', 'What are the technical Challenges Faced during Project Execution', 'During the project execution, we faced the following challenges:', 'The data client provided was not cleaned.', 'Data was of four different sector which we have to analyse and visualize.', 'Extracting insights from data.', 'How the Technical Challenges were Solved', 'To solve the technical challenges, we provided following solutions as follow:', 'Performed EDA on data to clean it and find the missing values.', 'As data was from different domains, we have analysed each sector and understand the culture of each domain. We understand the pipeline and flow of work process.', 'After completing the case study, we use calculations to extract the meaningful insights from data.', 'Business Impact', 'Using these dashboards client can visualize the sales insights and understand the workflow. They can take crucial decisions based on these insights which will help them to make an impact on their sales.', 'Project Snapshots', 'Sales Dashboard:', 'Government Dashboard:', 'Oil and Gas Dashboard:', 'Hospital Analysis:', 'Project website url', 'Dashboards on Google Data Studio:', '1.Government:-', 'https://datastudio.google.com/reporting/dda94ce8-5b77-46aa-a1e0-1a57ccaef5f9', '2.Oil:-', 'https://datastudio.google.com/reporting/47c6529e-1355-4072-babf-1a96f9f842cf', '3.Healthcare:-', 'https://datastudio.google.com/reporting/b1e95a11-4380-465c-ad45-2d1995c799fb', '4.Sales:-', 'https://datastudio.google.com/reporting/36ec0e42-6b77-4fbb-9dea-760cccaa741f', 'Project Video', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'CRM, Monday.com via Zapier to Power BI Dashboard', 'Next article', 'Data from CRM via Zapier to Google Sheets (Dynamic) to PowerBI', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2054,https://insights.blackcoffer.com/crm-monday-com-via-zapier-to-power-bi-dashboard/,"CRM, Monday.com via Zapier to Power BI Dashboard - Blackcoffer Insights-['Client Background', 'Client:', 'A leading solar panel firm in the USA', 'Industry Type:', 'Energy', 'Services:', 'Solar Panel', 'Organization Size:', '200+', 'Project Description', 'Mohsin has Solar Panel Company. He has setup CRMs for that. He wanted to use CRMs data and want to visualize the leads in PowerBI', 'Our Solution', 'First, we check CRMs thoroughly and understand the work culture of his company. It was not easy to fetch data into PowerBI using API key. To fetch new leads from CRMs we used Zapier. The limitation of Zapier is it cannot fetch historical data into spreadsheet. So we download data from CRMs and fetch it into spreadsheet. For new leads we created zaps for every instance. After that we connect the spreadsheet with PowerBI and clean the data accordingly. Using that data, we build KPIs according to client need.', 'Tools used', 'API , Zapier , Spreadsheet , PowerBI', 'Language/techniques used', 'M language , DAX', 'Skills used', 'API , M language , DAX , PowerBI', 'What are the technical Challenges Faced during Project Execution?', 'First challenge was to fetch data from CRMs using API key. Data we were getting was uncleaned and were not able to fetch all data. If there were multiple pages in the CRMs we will not be able to fetch all data from the pages.', 'How the Technical Challenges were Solved', 'Technical challenge in this project was to extract data from CRMs. So for that we used Zapier connector from CRMs to spreadsheet. But there was some limitation with Zapier that it will not fetch the historical data of our CRMs. So to solve that we download all historical data from CRMs and append it to the spreadsheet we were using. We fetch new leads to our spreadsheet using Zapier. By doing this now we have all the data historical and new lead which will be pushed by Zapier.', 'Then we fetch the data to our PowerBI and do some cleaning in data. By using cleaned data, we build the KPIs for our client according to there requirements.', 'Business Impact', 'Client will be able keep track on his company data on PowerBI and it helps them to make decisions accordingly.', 'Project Snapshots', 'CRMs', 'Zapier', 'PowerBI Dashboard', 'Project Video', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Monday.com to KPI Dashboard to manage, view, and generate insights from the CRM data', 'Next article', 'Data Warehouse to Google Data Studio (Looker) Dashboard', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2055,https://insights.blackcoffer.com/monday-com-to-kpi-dashboard-to-manage-view-and-generate-insights-from-the-crm-data/,"Monday.com to KPI Dashboard to manage, view, and generate insights from the CRM data - Blackcoffer Insights-['Client Background', 'Client:', 'A leading energy firm in the USA', 'Industry Type:', 'Energy', 'Services:', 'Solar panel', 'Organization Size:', '200+', 'Project Objective', 'Setup a dashboard on Monday.com', 'Fetch client CRM data onto Monday.com dashboard.', 'Project Description', 'Mohsin has CRM for his business where he has all data regarding leads of his clients. He wanted to see all his client appointments at one place. Client took subscription of Monday.com. It is an CRM where you can manage your work more easily in neat and clean user-friendly environment. We can easily track our task on Monday.com. Pipeline for Monday.com is very easy to use and also customized according to our needs.', 'Our Solution', 'The challenging part of this project was to get CRM data on Monday.com dashboard. Client also has subscription of Zapier. Zapier is a connector which connect two apps to transfer data from each other. Zapier also has limitation to fetch limited type of data from CRM. Like for Mohsin CRM we can only fetch hot lead comes on CRM. But in his CRM there are also other functions like if a customer lead comes on CRM. They manually book appointment for that client. So there is no way to get that data from CRM. Issue for client was he has attached integrated four google calendar account with CRM so whenever he confirms appointment on CRM that data fetched on google calendar. But he has check manually one by one on each calendar which was bit hard task for him. So, we advised Monday.com where he can track all his task at one place.', 'Tools used', 'Monday.com', 'Zapier', 'Google Calendar', 'Databases used', 'Google Calendar', 'What are the technical Challenges Faced during Project Execution', 'The challenging part of this project was to get CRM data on Monday.com dashboard.There is no direct integration of CRM and Monday.com to fetch data.', 'How the Technical Challenges were Solved', 'To solve challenges, we used Zapier to get CRM data to dashboard of Monday.com. We used google calendar of client which were integrated with CRM. All the appointment confirmed leads were present on google calendar.', 'Pipeline of Data:', 'CRM 🡪 Google Calendar 🡪 Zapier 🡪 Monday.com', 'Business Impact', 'Using the Monday.com dashboard client can easily track all appointments of customers. He can track data of his team members and connect with them at one place. He will not miss any of his meeting with customer. Monday.com also has timeline and calendar view using that client can see all activity of his work.', 'Project Snapshots', 'CRM Calendar view', 'Monday.com', 'Google Calendar', 'Zapier', 'Project Video', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Data Management for a Political SaaS Application', 'Next article', 'CRM, Monday.com via Zapier to Power BI Dashboard', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2056,https://insights.blackcoffer.com/data-management-for-a-political-saas-application/,"Data Management for a Political SaaS Application - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Services:', 'SaaS, Products', 'Organization Size:', '100+', 'The Problem', 'As per the guidelines and discussion. Political Research Automated Data Acquisition (PRADA) in the following phases which included.', '1. Get pics for existing EOs (Elected Officials)', '2. Get new EOs and Pictures.', '3. Run QA checks regularly on EOs', '4. Get data from government Facebook pages.', '5. Geospatial project: Create a new version of provided KML without using google earth.\xa0 Creating a nested directories which contained description and Map-URL at the designated location.', '6. Get data of US States and Counties(Including Boroughs and Parishes)', 'By building an automated generated structured data that allows a non – programmer to create a config for each page allowing a bot to scrap and update the data.', 'Our Solution', 'We created an automated python scripts for designated phases with respective requirements. Solutions to various type of problems varied such as most of data scrapping automation was done through python developed scripts including the geospatial KML task. In addition to this different ranges of data was scrapped generated directed output for the respective tasks in the form of CSV format. So the user’s main aim requirement was achieved i.e. a non programmer could create a con-fig and initiate a bot to scrap the required data.', 'Solution Architecture', 'The majority task of project consisted of web data scraping automation so a high- level overview, and specific implementation details of project shall will be as follows:', 'Web Scraping Framework', ': Python as a coding language was used in almost all of the tasks and the framework used for data scraping included Beautiful-Soup, Selenium and Web drivers. These libraries provide tools and functionalities to navigate web pages, extract data, and handle various HTML elements.', 'Data Extraction and Parsing:', 'Use the selected web scraping library to extract the desired data from the web pages provided either in the data sheet or within the websites of URLs given in the sheet. This involves locating HTML elements, applying filters or selectors, and parsing the extracted data.', 'Data Processing:', 'Followed by data extraction it was cleansed, transformed and aggregated to a structured form such as pandas’ Data Frame followed by a CSV file. In the case of geospatial task it resulted to generation of nested folders in a kml file.', 'Data Storage:', 'The how and where to store the scrapped data was determined which is local file system in the form of CSV (Comma Separated Values). As it was the appropriate data storage solution according to need of the project. In addition to this the geospatial task had the output in the form of kml file as polygons inside directories of nested folders.', 'Deliverables', 'Tasks', 'Outputs (CSV/KML/XLSX)', 'Python Scripts', 'Canada EOs', 'mydata.csv', 'Script1.py', 'Script2.py', 'Geospatial Task', 'Electoral Districts.kml', '–', 'Facebook Scrapping of EOs', 'EO_OUTPUT_O.csv', 'final_eo_scrapping.py', 'Facebook Scrapping of 429 Cities', 'Output_DRAFT_429_CITIES.csv', 'Facebook_image_scrapping.py', 'USA States Website URLs', 'ScreenScrapingt.csv', 'final_50_states_scrapping.py', 'USA Counties Website URLs', 'US Website_final_write.xlsx', 'county_scrapping.py', 'Tools used', 'Python (', 'Programming Language', ')', 'Beautiful Soup', 'Selenium', 'Pandas', 'Numpy', 'Simplekml', 're', '(regular expressions)', 'Language/techniques used', 'Python (', 'Programming Language', ') –', 'It is an interpreted language, which allows quick prototyping and interactive coding. Its versatility can be is one of the reasons for its major applications. Different libraries and tools were used in this project for various data solutions.', 'Beautiful Soup –', 'A python library used for web scraping and parsing HTML and XML documents. It provides a convenient way to extract from the said files. It eases out the work flow from parsing to data extraction and encoding handling as well.', 'Selenium –', 'A python library used for web browser automation like Chrome, Firefox, Safari and others. It interacts with elements such as clicking buttons, filling out forms and selecting drop down options. In this project we used it in Chrome. Selenium web driver was used for web automation. It acted as bridge between Python code and the Web browser.', 'Pandas –', 'It is Python’s versatile library that provides high performance data structure tools and it is built on top of Numpy. Data Frame is one of its key feature due to which this library was used. This key feature allows efficient manipulation, slicing, and filtering of structured data', 'Numpy –', 'It is also a python library aka Numerical Python as it is a fundamental library for scientific computing in Python.', 'Simplekml –', 'It is a python package which enables you to generate KML with as little effort as possible.', 're (', 'Regular Expressions): It is a powerful tool in python sued for pattern matching and manipulations of strings.', 'Skills used', 'Python Programming', 'Web Fundamentals', 'Web Scrapping using libraries such as BS, Selenium.', 'Data Cleaning and Processing', 'Problem- Solving and debugging.', 'KML structure and handling using Python’s programming.', 'Databases used', 'None. All the structured data was in the form of either python Data Frames, CSV or Excel Sheets.', 'What are the technical Challenges Faced during Project Execution', 'Firstly some of the web URLs were not accessible because they were restricted to particular range of IPs of that region', 'Couldn’t fetch whole data through Beautiful Soup as it couldn’t parse whole tags.', 'List of US Counties wasn’t provided in the given resource links', 'How the Technical Challenges were Solved', 'Used VPN for accessing Official sites which were not generally accessible.', 'Used Selenium Web driver to automate the direction at URLs which fetched complete html tags of the desired webpages.', 'Performed a search and created structure data of list of counties of each state which was used as input to gain web URLs of counties of US.', 'Business Impact', 'Enhanced Analysis', ': Web scraping allows businesses to gather valuable data from various websites. This information can provide insights to desired aim and objectives enabling businesses to make informed.', 'Real-Time Monitoring and Upgradation:', 'Web scraping can enable business to monitor changes or updates on website in real-time. This can be useful for tracking regulatory changes. It keeps the business and it’s data updated.', 'Increased Efficiency:', 'Automation eliminated the need for manual data collection, saving time and resources. With automated web scraping, business can extract large amount data quickly, accurately, improving overall operational efficiency.', 'Project Snapshots', 'Chrome driver initiated', 'Chrome driver visiting the directed links and accessing the image URLs', 'Directed to next link', 'KML task', 'Facebook Data extraction', 'Data of State Governments of US', 'Accessing links through wiki directing to counties', 'Nesting within the list of counties of a particular state', 'Finding and Extracting link of the website of County', 'Project website url', 'The GitHub repository link:- https://github.com/AjayBidyarthy/Paul-Andr-Savoie/tree/main', 'Project Video', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Google LSA Ads (Google Local Service Ads) – ETL tools and Dashboards', 'Next article', 'Monday.com to KPI Dashboard to manage, view, and generate insights from the CRM data', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2057,https://insights.blackcoffer.com/google-lsa-ads-google-local-service-ads-etl-tools-and-dashboards/,"Google LSA Ads (Google Local Service Ads) – ETL tools and Dashboards - Blackcoffer Insights-['Client Background', 'Client:', 'A leading marketing firm in the USA', 'Industry Type:', 'Marketing', 'Services:', 'Ads, Marketing, Campaign, Consulting', 'Organization Size:', '200+', 'The Problem', 'The client has a Google LSA Ads Manager Account with about 100+ accounts and wishes to collect data available through the Google LSA API daily. The client wishes to set up a private Databases that is automatically created for newly added accounts and stores all of the collected data (Lead and Phone Call data). Finally, all collected data must be presented through the Google Looker Studio Dashboards, with the design layouts as suggested by the client.', 'Our Solution', 'The solution involves a number of Python-based ETL tools that are responsible for fetching the data from Google’s LSA API daily and updating the same in the Google BigQuery Databases.', 'Two different tools run are:', 'MCC Data Fetching tool.', 'Lead Record data fetching tool.', 'The fetched data is stored in BigQuery Databases on the client-provided (Google)manager account.', 'Carefully curated Google Looker Studio dashboards implemented with client-suggested theme layout which are updated upon client request, represent a number of KPIs and graphs indicating major data trends.', 'The designed dashboards have a number of data-controlling filters that filter the data account-wise and date-wise.', 'Solution Architecture', 'Deliverables', 'Heroku deployed Python tools', 'Google Looker Studio Dashboards', 'BigQuery Database', 'Maintenance service', 'Tools used', 'Python', 'Google BigQuery', 'Heroku', 'Google Looker Studio', 'Git', 'Heroku CLI', 'Language/techniques used', 'Python', 'GoogleSQL (BigQuery supported SQL)', 'Looker Modeling Language (Looker ML)', 'Git Commands', 'Skills used', 'Data Engineering skill to fetch data as per client needs.', 'Data Processing to make it suitable for dashboards, databases', 'Dashboard designing and data presentation skills', 'Tool Deployment', 'Database manipulation', 'Data piplining', 'Databases used', 'Google BigQuery', 'Web Cloud Servers used', 'Heroku: Cloud Application Platform', 'What are the technical Challenges Faced during Project Execution', 'Google LSA API is slow, high data fetching timelines.', 'BigQuery jobs fail, causing inconsistencies.', 'How the Technical Challenges were Solved', 'Entire data fetching operation requires 1-2 hrs daily, 2 separate tools run in asynchronously and populate two different databases, the data is grouped in the dashboards', 'Regular weekly and monthly data refreshes update any inconsistent data.', 'Business Impact', 'Business clients are able to access important KPI’s without the need to understand the complexities involved behind the scenes.', 'Allows clients to track their performances, responsiveness.', 'Project Snapshots', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Ad Networks Marketing Campaign Data Dashboard in Looker (Google Data Studio)', 'Next article', 'Data Management for a Political SaaS Application', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2058,https://insights.blackcoffer.com/ad-networks-marketing-campaign-data-dashboard-in-looker-google-data-studio/,"Ad Networks Marketing Campaign Data Dashboard in Looker (Google Data Studio) - Blackcoffer Insights-['Client Background', 'Client:', 'A leading financial firm in Dubai', 'Industry Type:', 'Financial Services', 'Services:', 'Banking, Financial Services, Card Payments, Mobile Payments, Digital Bank, and FinTech', 'Organization Size:', '200+', 'The Problem', 'Build dashboards unifying all the platforms in use: Google Ads, FB ads, Appsflyer, Mixpanel, etc,', 'in order to be able to track everything in the funnel from traffic source to total installs (paid, organic and by channel', 'Our Solution', 'Track the app data analytics using various platforms', 'Prepare the data sources – find and build data connectors for Google Data Studio.', 'Developed 14 pages of Dashboard reports- creating templates to importing data sources and perform various visualisations.', 'Maintained and tracked dashboard reports and helped the client with intelligence from these reports.', 'Deliverables', '1', 'Updating the iOS datasheet', '2', 'Fixing the incoming data for androids', '3', 'Correcting a calculation error', '4', 'Finding an alternative to provide automated data update directly to google data studio for iOS.', '5', 'Updates done to all the dashboards', '6', 'Created new dashboards', '7', 'Created a consolidated dashboard', '8', 'Added required visualizations and conected to data sources', '9', 'Created new data sources', '10', 'Managing the consolidated dashboard with daily data monitoring', '11', 'Funnel Report for consolidated dahboard', '12', 'Google analytics installed on website through tag manager', '13', 'Resolving errors', '14', 'work on automation for ad acccounts', '15', 'Developed a new dashboard', '16', 'Ad accounts data Automated', '17', 'work towards android data automation', '18', 'altering of blended data joins as per gds updates', '19', 'Personalisation of dashboards', '20', 'Current dashboard updated with google events and widget changes', '21', 'Added Apple search ads dashboard', '22', 'Firebase funnel report dashboard developed', '23', 'Card topups Funnel report dashboard developed', '24', 'Porter metrics custom dashboards for trial', '25', 'Registration firebase funnel and percentage added', '26', 'Updates for all the dashboards running until now and addition of kpi to the new firebase dashboards', '27', 'User info for firebase dashboard and retention report', '28', 'Registration Funnel, Cardtopups, KYC funnel Dashboard', '29', 'Fixing and Updating user info firebase dashboard and began working on the tiktok dashboard', '30', 'Tiktok Dashboard Developed and populated with data from porter metrics', 'Tools used', 'Google Data Studio', 'Google Analytics- GA4 and universal analytics', 'Google Tag Manager', 'Big Query', 'Firebase', 'Appsflyer', 'Mixpanel', 'Google spreadsheets', 'Language/techniques used', 'Google Standard SQL dialect- bigquery', 'Apps script', 'Skills used', 'Analytical aptitude', 'Problem-solving', 'Communication', 'Knowledge about SQL', 'Knowledge in digital marketing and strategies', 'Google cloud services', 'Creating data pipelines.', 'Databases used', 'Bigquery', 'Google spreadsheets', 'Firebase', 'Web Cloud Servers used', 'Google Cloud Platform', 'What are the technical Challenges Faced during Project Execution', 'Community/in-built Connectors for Appstore connect didn’t exist', 'Connector for apple search ads couldn’t be found', 'Data tracking from google play console, due to the timezone lag in data updation.', 'Facebook connector issues', 'How the Technical Challenges were Solved', 'Worked towards building the custom connector by using apple api for Appstore connect and search ads', 'Utilised big query to call and store 100% accurate data from google play console and be used as a connector in GDS', 'Made use of inhouse built facebook connector and google sheet add-on to track and keep connector inaccuracy check.', 'Business Impact', 'Helped the client to view a consolidated report of all their ad campaigns', 'Calculated and executed analytics metrics which helped to track various app events and helped the business to take decisions on UX', 'Consulted the client and collaborated with them in marketing and ad campaign strategies- helped them cut their marketing expenses over less efficient marketing platforms', 'Created funnel reports and suggested insights on app traffic to take decisions on important landing pages.', 'Project Snapshots', 'Project website URL', 'https://datastudio.google.com/reporting/8af163c1-b328-4ed3-91fc-cf8a026d0d9f', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Analytical solution for a tech firm', 'Next article', 'Google LSA Ads (Google Local Service Ads) – ETL tools and Dashboards', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2059,https://insights.blackcoffer.com/analytical-solution-for-a-tech-firm/,"Analytical solution for a tech firm - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Services:', 'Consulting', 'Organization Size:', '100+', 'The Problem', 'The client’s organization had a project that matches URLs up using TF-IDF algorithm.', 'The script threw some errors and resolving these errors was the immediate ask.', 'The client also required us to adjust the script for better accuracy and faster computation.', 'Our Solution', 'R&D on the code developed', 'Find & List bugs', 'Solve the Bugs', 'Find and get the best matching algorithm implemented.', 'Check and compare the existing matching algorithm implemented for accuracy.', 'if not check of other solution – ngrams or fuzzy logic', 'Meet the expected output', 'Deliverables', 'Fully functional code', 'Solution & Documentation', 'Support', 'Tools used', 'Google spreadsheets', 'Microsoft Excel', 'Google Colaboratory', 'Language/techniques', 'Python', 'Models used', 'TF-IDF', 'BERT', 'Ngrams', 'Flair Embeddings', 'Rapid Fuzz', 'Skills used', 'Problem-solving', 'Communication', 'Data Modelling', 'Data Pipelining', 'Python Coding', 'Databases used', 'Google spreadsheets', 'What are the technical Challenges Faced during Project Execution', 'Bugs on the model used by the client was fairly competent using pretrained libraries', 'The accuracy for the bug free code on the models used by the client was shaen once the model ran on a different set of data input', 'How the Technical Challenges were Solved', 'A vanilla code to execute the same logic while fine tuning the matching algorithm was written in order to over come the shortcomings of the pretrained model bugs', 'The data pre-processing was done manually in order to transform every instance of an input into better readable format to be able to go into the model and get best matching accuracy possible in the given timeframe of execution of the code', 'Business Impact', 'Helped the client to perform the matching process with maximum accuracy and lowest cost on code, by implementing manually written vanilla code from scratch to utilise the matching algorithm.', 'Project Snapshots', 'Project website url', 'https://colab.research.google.com/github/AjayBidyarthy/Daniel-Emery/blob/main/vanilla.ipynb#scrollTo=vPp14xj020RL', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'AI solution for a Technology, Information and Internet firm', 'Next article', 'Ad Networks Marketing Campaign Data Dashboard in Looker (Google Data Studio)', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2060,https://insights.blackcoffer.com/ai-solution-for-a-technology-information-and-internet-firm/,"AI solution for a Technology, Information and Internet firm - Blackcoffer Insights-['Client Background', 'Client:', 'A leading Technology, Information and Internet firm in India', 'Industry Type:', 'IT', 'Services:', 'Emerging Technologies, 2030, and 2050', 'Organization Size:', '10+', 'The Problem', 'The objective was to analyze, research, and propose data science solutions in the product based on the product design, use cases, and services.', 'Our Solution', 'Analyze each use case', 'Analyze product design', 'Analyze user type, controls per use cases', 'For each use case and available product design,', 'provide solution or scope of the data science capabilities', 'List attributes needed in each of the product design screens', 'List use cases are driven by the data', 'For each data-driven use cases', 'a. Research and design the data science solution', 'b. List needed data', 'c. List process', 'd. List models', 'e. List solution', 'Help product design team with data science use cases', 'Help product design team with data science solutions for each use case', 'Deliverables', 'Statement of Work (SoW) with a solution documentation', 'Data science use cases document', 'Data science solution for each use cases document', 'Data Science methodology, algorithms needed, models, recommended and more in a good documentation', 'Tools used', 'Google docs', 'Microsoft word', 'Draw.io', 'Excel', 'Google Draw', 'Language/techniques', 'Python- Flask', 'Models used', 'K-Nearest Neighbours', 'K-Means Clustering', 'NLTK', 'DeepAvlov', 'Spacy', 'Texttiling', 'Eclat', 'LSTM', 'Skills used', 'Aptitude for functionalities', 'Problem-solving', 'Communication', 'Data Modelling', 'Data Pipelining', 'MLOps', 'NLP', 'Recommender systems', 'Databases used', 'Amazon S3', 'Web Cloud Servers used', 'AWS EC2', 'Business Impact', 'Collaboration with the client to identify the scope and use cases for the platform', 'Cost Effective approach taken to document solutions', 'Regressive R&D to find and document third-party solutions for certain use cases- saving cost and time.', 'Project Snapshots', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'AI and NLP-based Solutions to Automate Data Discovery for Venture Capital and Private Equity Principals', 'Next article', 'Analytical solution for a tech firm', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2061,https://insights.blackcoffer.com/ai-and-nlp-based-solutions-to-automate-data-discovery-for-venture-capital-and-private-equity-principals/,"AI and NLP-based Solutions to Automate Data Discovery for Venture Capital and Private Equity Principals - Blackcoffer Insights-['Client Background', 'Client:', 'A leading Venture Capital and Private Equity Principals in the Globe', 'Industry Type:', 'Venture Capital and Private Equity Principals', 'Services:', 'Private Equity, Venture Capital, Data Analysis, Fund Performance, Alternative Assets, Competitive Intelligence, Limited Partners, Customized Benchmarks, Service Providers, Fund of Funds, M&A, and Financial Services', 'Organization Size:', '100+', 'The Problem', 'Extract funding-related data from news articles (from 1000+ websites) such as company name, funded amount, participated investors, and other details.', 'create a web app to manage the extraction of funding\xa0 data', 'Our Solution', 'There were 1000+ websites from funding-related articles so we couldn’t make a crawler for each website. So we used an inbuilt web crawler provided by elasticsearch. When we have extracted articles then we need to extract funding related information company name, fund amount and investors participated etc. Then we decided to use NLP’s question-answering method in which we need to train transformers to extract funding-related information. First we have created some keywords based approaches to create labels for each field we need to extract to train models. After that we have trained distil bert model on labelled data on AWS EC2’s GPU server. We applied this approach for all the fields we need to extract. We got 90%+ accuracy for the company name field and for other fields we got 80%+ accuracy.', 'To manage and view all the fields of extracted funding data we created a web app using python flask. In this we created several pages to show extracted raw data by crawler,\xa0 cleaned data after applying some cleaning functions and final output which have all the fields.\xa0 We also created admin dashboard pages to show daily crawling status, how many articles processed in one day, total final output etc.', 'Solution Architecture', 'Deliverables', 'Flask Web app', 'Elasticsearch crawler', 'Tools used', 'Flask, Spacy, NLTK, pandas, numpy, transformers, elasticsearch etc.', 'Language/techniques used', 'Question answering in NLP, web scraping, web application Flask, Python', 'Models used', 'Distil-bert model, en-core-web-sm (pre trained model of spacy)', 'Skills used', 'NLP, Data Analysis, Flask web app, Pandas, Numpy, transformers, fastapi, elasticsearch etc.', 'Databases used', 'Elasticsearch database', 'Web Cloud Servers used', 'AWS', 'What are the technical Challenges Faced during Project Execution', 'The client wanted to extract data from 1000+ different websites and if we make any crawler it only works for one website so it was not possible to create a 1000+ web crawler.', 'How to extract funding information from an article. It is very difficult to extract that type of information from normal python code by defining keywords because every website has different types of articles.', 'How the Technical Challenges were Solved', 'To solve web crawler-related issues we used elasticsearch web crawler which is very fast and can extract multiple websites at a time. In this we need to create an engine and add websites that we want to scrape. After that we added some keywords to extract only funding-related articles. We set up this crawler to run every hour so we can get new articles every hour.', 'To extract funding-related information we collected articles from different websites and created labels for each field we wanted to extract. After that we have fine-tuned the transformer’s Distil-bert model on our labeled data.\xa0 We used these models to extract funding-related information. We also created an automated python script that uses these model on every extracted article and extracts funding-related information.', 'Business Impact', 'This funding-related data would be used in two ways. From this project, companies can find suitable investors for their startups. Companies can search for investors based on industry, verticals, etc., and find investors to help their startups.', 'Investors can use it to find a startup in which they want to invest based on their preferences like industry, verticals, etc.', 'Project Snapshots (Minimum 10 Pictures)', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'An ETL solution for an Internet Publishing firm', 'Next article', 'AI solution for a Technology, Information and Internet firm', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2062,https://insights.blackcoffer.com/an-etl-solution-for-an-internet-publishing-firm/,"An ETL solution for an Internet Publishing firm - Blackcoffer Insights-['Client Background', 'Client:', 'A leading internet publishing firm in Singapore and Australia', 'Industry Type:', 'Internet Publishing', 'Services:', 'peer-to-peer car sharing platform where you can rent a large variety of cars, always nearby at great value', 'Organization Size:', '100+', 'Project Objective', 'Fetch all call logs using zendesk api from drivelah server', 'Analyse call logs and\xa0 number of calls made by a particular phone number to company and fetch recent call timing', 'Project Description', 'We need to fetch last month’s call details (from user, to user, call_time, call_status ) using zendesk api.', 'Then we need to analyse all call logs and need to identify the number of calls made by a particular user to the company and the most recent call timing from the company server.', 'Our Solution', 'To fetch all call logs using zendesk api we used python language in programming. When we checked call details in the zendesk api, the details were in json format which is very tough to understand the calls details. So first we have fetched only needed details (call made from person, to person and call timing) converted into tabular format. In tabular format it was easy to identify call details.', 'After that we need to identify the number of calls made by the user to the company in the last month.\xa0 We used the python pandas module here which is very fast and effective to handle tabular data. First we separated the user who made a call to the company last month and then counted each unique user’s call records. For recent dates we used python’s datetime module which can easily identify recent date time.', 'Project Deliverables', '2 python scripts', 'for fetching call details and converting into table format', 'for identifying number of calls made and recent call timing', 'Tools used', 'VS Code, Google Drive, and MS Excel.', 'Language/techniques used', 'Python programming language, Data Analytics with numpy and pandas, python datetime.', 'Skills used', 'Data Analytics,, Python, Mathematics', 'Databases used', 'local data from MS Excel Sheet', 'What are the technical Challenges Faced during Project Execution', 'First one was the api data in json format with other unwanted data so it was a little difficult for us to identify the number of calls and other information from direct json data.', 'The date format in the api data is not appropriate for us\xa0 to handle. Because the date is\xa0 stored in string format, it was difficult to compare dates with one another and identify recent ones.', 'How the Technical Challenges were Solved', 'For the first technical challenge we first took only useful details from api’s json format and converted these details in tabular format. In python we can easily handle tables with pandas dataframe and can apply whatever operation we want to collect details.', 'For the second one we know that it would be difficult to handle dates in string format. So we first converted dates to a proper datetime format using python’s datetime module. It has a lot of built in functionalities which can easily compare dates with one another.\xa0 So from comparison we have identified recent dates of calls.', 'Project Snapshots', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'AI-Based Algorithmic Trading Bot for Forex', 'Next article', 'AI and NLP-based Solutions to Automate Data Discovery for Venture Capital and Private Equity Principals', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2063,https://insights.blackcoffer.com/ai-based-algorithmic-trading-bot-for-forex/,"AI-Based Algorithmic Trading Bot for Forex - Blackcoffer Insights-['Client Background', 'Client:', 'A leading trading firm in the USA', 'Industry Type:', 'Finance', 'Services:', 'Trading, Banking, Investment', 'Organization Size:', '100+', 'The Problem', 'Build ML/AI Model to predict next 15 min EMA cross on historical and live data by using indicators such as EMA, MACD, RSI etc.', 'Create a web app to show predicted EMA cross and other indicators movement', 'Our Solution', 'In stock market indicators such as EMA, MACD, RSI etc helps us to find cross by using historical price data. If we accurately predict cross earlier then it will help us in investment. So we have used 12data api to collect historical and live EUR/USD price data. We calculated EMA(12), EMA(26), MACD and RSI indicators based on price data.\xa0 After that we created labels of ema cross in historical data. When we have training data we used different classifier models for training. We predicted accuracy\xa0 with different models and the Logistic regression model gave 91% accuracy. This logistic regression is predicting the cross only for the next step. It means we will know only 15 minutes before that the cross will happen in the next 15 min but we need to know more earlier. For that we predicted the next 45 minutes price values using the LSTM model from historical price data. Based on these price values we have calculated EMA, MACD and RSI and\xa0 after that cross using logistic regression. So now we can predict the cross 1 hour earlier based on these 2 models.', 'To show cross and other\xa0 indicators movement we created a python flask web app and hosted it on AWS EC2 server. The process runs every 15 minutes\xa0 and checks the cross. If there is any cross in 1 hour it sends a telegram notification.', 'Deliverables', 'Flask web app', 'All the python code and machine learning models', 'Tools used', 'Pandas, numpy, scikit-learn, tensorflow, flask etc.', 'Language/techniques used', 'Data Analysis, Data Visualization, Machine learning, Deep learning, flask web app etc.', 'Models used', 'Logistic Regression, LSTM model', 'Skills used', 'Data Analysis, Data Visualization, Machine learning, Deep learning, flask, python etc.', 'Databases used', 'MongoDB', 'Web Cloud Servers used', 'AWS Ec2', 'What are the technical Challenges Faced during Project Execution', 'Main challenge in this project is to find the best model. Because we have time series data so we cannot change the orders to get better accuracy.', 'One machine learning model is only predicting the next 15 min cross but we need the ema cross 1 hour before.', 'How the Technical Challenges were Solved', 'We were using time series data so we cannot change the order to find better accuracy in every model.\xa0 So we have tried different models with the same order and evaluated the model. Only the logistic regression model worked best for the data it gave 91% accuracy on test data.', 'To get the next 1 hour prediction we first tried the same logistic regression to predict the next 3 steps but we failed because of poor accuracy. So we trained the LSTM model on price data and predicted the next 3 steps using the LSTM model.\xa0 After that we used logistic regression to predict ema cross.', 'Business Impact', 'It will help traders to predict the stock market earlier and get better returns from this project.', 'Project Snapshots', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Equity Waterfalls Model-Based SaaS Application for Real Estate Sector', 'Next article', 'An ETL solution for an Internet Publishing firm', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2064,https://insights.blackcoffer.com/equity-waterfalls-model-based-saas-application-for-real-estate-sector/,"Equity Waterfalls Model-Based SaaS Application for Real Estate Sector - Blackcoffer Insights-['Client Background', 'Client:', 'A leading real estate firm in the USA', 'Industry Type:', 'real estate', 'Services:', 'Property business, investment, real estate', 'Organization Size:', '100+', 'Project Objective', 'The objective is to create software that will calculate the equity waterfalls for different cases. And there should be 3 users admin, sponsor and investor. We need to create the equity waterfall calculation according to the csv file that is shared by the client. All users have their own UI portal.', 'Project Description', 'The project is created using python language, working on django rest framework and for frontend we use reactjs and the code deployed on google cloud app engine service. We need to create a software that will calculate the equity waterfalls. And there should be 3 users admin, sponsor and investor. We need to create the calculation according to the csv file that is shared by the client.', 'All users should have their own UI portal.', 'Sponsors can create deals and send deal invitations to all investors or specific investors.', 'Investors can see all the deals that are offered by the sponsor’s. After that Investors can subscribe that deal after subscription it is depending on sponsor that he will accept the investor subscription or not.', 'Our Solution', 'We have created api’s that will calculate the equity waterfall calculation according to the selection of the waterfall tiers.', 'Project Deliverables', 'Django rest framework api’s with frontend.', 'Github source code.', 'Working UI.', 'Tools used', 'Views.', 'Routers.', 'Serializers.', 'Serializer relations.', 'Settings.', 'Language/techniques used', 'Python', 'Django rest framework', 'ReactJS', 'JWT', 'SMTP', 'Skills used', 'SMTP', 'JWT', 'Databases used', 'Sqlite3 Database', 'Web Cloud Servers used', 'Google cloud platform', 'What are the technical Challenges Faced during Project Execution', 'The technical issues faced during the project is how to calculate the equity waterfall calculation for different tiers and different cases. And also invite the sponsors by admin or sponsors invite their investors.', 'How the Technical Challenges were Solved', 'We have used conditional statements in code and write different codes for different calculations. so that it will check which case we need to run and it will run accordingly.', 'Added the functionality in which admin can invite the sponsors to the website and sponsors can invite their investor through sending the invitation link to their email.', 'Project Snapshots', 'Project website url', 'https://stackshares.io/dashboard/add-new-deal', 'Project Video', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'AI Solutions for Foreign Exchange – An Automated Algo Trading Tool', 'Next article', 'AI-Based Algorithmic Trading Bot for Forex', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2065,https://insights.blackcoffer.com/ai-solutions-for-foreign-exchange-an-automated-algo-trading-tool/,"AI Solutions for Foreign Exchange - An Automated Algo Trading Tool - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'Financial Services', 'Services:', 'Trading, consulting, financial serivices', 'Organization Size:', '100+', 'The Problem', 'Our main objective in this project was to help with setting up with given Broker API using MT4 and extracting historical data from it, and solving different tasks which are related to extracting important values from the data. And tasks assigned by the client were related to working around the data, i.e. formatting, connecting with the IG trade broker, automating the Python script and scheduling the script accordingly.', 'Our Solution', 'During the initial phase, we were assigned to set up an MT4 with given Broker API access to extract historical prices, which was delivered to the client. In the second phase, the client requested to implement Profit/Loss, Spread Direction and Time in Trade. There were minute tasks related to the R script, which was duly completed. In the third phase, the client was assigned a task related to distinguishing the tickers according to cluster types which he provided and implemented code to distinguish the sell and buy spread for the given STD. In the fourth phase, I implemented the logic (Profit/Loss – (1% of 1st Currency + 1% of 2nd Currency)) into the existing code and worked on retrieving Historical prices from another Broker API and retrieving Watchlist given attributes by the client. Automated the Python script to retrieve yesterday’s market price of the given list', 'Deliverables', 'Successfully delivered set-up in MT4 for retrieving historical prices, Created logic for automating the profit and loss, Implemented code to distinguish the tickers according to the cluster type, Implemented code for distinguish the sell and buy spread for the given STD, Implemented the logic (Profit/Loss – (1% of 1st Currency + 1% of 2nd Currency)) into the existing code. Automated the Python script to retrieve yesterday’s market price.', 'Tools used', 'MT4, Jupyter Notebook, Excel, IG trade, Remote Desktop setup', 'Language/techniques used', 'MQL, Python, R', 'Skills used', 'Critical thinking, Logical Thinking', 'What are the technical Challenges Faced during Project Execution?', 'While setting up MT4 platform and its configurations', 'How the Technical Challenges were Solved', 'The above-mentioned challenges were resolved after many hours of effort and understanding.', 'Project Snapshots', 'Project Video', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'AI agent development and Deployment in Jina AI', 'Next article', 'Equity Waterfalls Model-Based SaaS Application for Real Estate Sector', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2066,https://insights.blackcoffer.com/ai-agent-development-and-deployment-in-jina-ai/,"AI agent development and Deployment in Jina AI - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in Europe', 'Industry Type:', 'IT', 'Services:', 'IT and Consulting', 'Organization Size:', '100+', 'The Problem', 'The client’s object was to create AI agents for his website, which the end-users will utilize for many tasks. The client had some recommendations on the models are utilized.', 'Our Solution', 'Created a feasible models list that complements the client’s requirement and when ahead and executed the Executor code for every model for compatibility with JinaAI deployment. After implementing Executor codes, I created a Flow to connect every executor and deployed it successfully.', 'Deliverables', 'Successfully delivered executable deployed models in Jina Ai', 'Tools used', 'Jina AI, VSCode, HuggingFace', 'Language/techniques used', 'Python', 'Models used', 'Whisper, Stable Diffusion, GPT3, Codex, YOLO, CoquiAI, PDF Segmentor', 'Skills used', 'Python, Model APIs', 'Databases used', 'JinaAI Cloud', 'What are the technical Challenges Faced during Project Execution', 'There were minute challenges, such as deployment issues and Execution issues', 'How the Technical Challenges were Solved', 'I resolved the issues effectively after long hours of understanding the concept because JinaAI is a new growing technology that does not have many forums to solve errors and issues.', 'Project Snapshots', 'Project Video', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Golden Record – A knowledge graph database approach to unfold discovery using Neo4j', 'Next article', 'AI Solutions for Foreign Exchange – An Automated Algo Trading Tool', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2067,https://insights.blackcoffer.com/golden-record-a-knowledge-graph-database-approach-to-unfold-discovery-using-neo4j/,"Golden Record - A knowledge graph database approach to unfold discovery using Neo4j - Blackcoffer Insights-['Client Background', 'Client:', 'A leading retail firm in the USA', 'Industry Type:', 'Retail', 'Services:', 'Retail business, consumer services', 'Organization Size:', '100+', 'The Problem', 'To use data ingested into Neo4j and use the nodes and relationships with its properties to determine which nodes are actually the same person. For eg: we have Person nodes in the data, now people might enter their names in different ways. Our main aim is to identify Person nodes that may have similar data and are actually the same person. This will be represented as a perfect match between the nodes. This single-person view is referred to as the Golden Record', 'Our Solution', 'Till date, we have loaded data into Neo4j and created relationships with score property which defines match strength. We have created some criterias by which we can determine what constitutes two nodes being the same and then based on them created ‘perfect match’ and ‘probable match’.', 'We have considered four properties for our criteria – full name, address, driver’s license, and passport number. We have relationships between nodes for these properties with scores, we use these in our perfect match and probable match creation.', 'We have also configured Graphlytics (a viz software) in the virtual machine which connects to the neo4j database and helps vizualize the nodes and relationships.', 'We have also worked on some algorithms using the GDS library in neo4j to produce more information on the graph, the common neighbors algorithm was used to produce scores based on node similarity and the higher the score the higher the similarity. Other algorithms were tried as well but since all the properties are of String format it did not work on it.', 'We have Resolved issues neo4j is facing when deleting a Large set of data and Provided steps to recover neo4j if it fails by going OutofMemory.', 'We have figured out the issues with the probable and perfect match cypher queries not working as intended and proposed a solution.', 'Solution Architecture', 'Deliverables', 'Created Perfect match and probable match queries.', 'Created queries that return the nodes (even if it does not have associated relationship) and it’s associated relationship.', 'A cypher query that return the result as a json object that can be mapped into a java oject.', 'A cypher query that will create the relationship if two node’s properties\xa0 have same value.', 'A cypher query that will delete one relationship from bidirectional relationship.', 'A python code for a sample neo4j query', 'Adjust the perfect and probable match queries so it would work for\xa0 current data.', 'Tools used', 'Neo4j', 'Language/techniques used', 'Cypher Query Language', 'Models used', 'The common neighbors algorithm', 'Skills used', 'CQL', 'Databases used', 'Neo4j', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Advanced AI for Trading Automation', 'Next article', 'AI agent development and Deployment in Jina AI', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2068,https://insights.blackcoffer.com/advanced-ai-for-trading-automation/,"Advanced AI for Trading Automation - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in Europe', 'Industry Type:', 'Banking & Finance', 'Services:', 'Trading, and financial services', 'Organization Size:', '100+', 'The Problem', 'Create an automated trading application with fully automated trading capabilities from selecting pair of assets to buying/selling assets. This application uses AI to decide what action to take while trading.', 'Our Solution', 'We have integrated coin_api with the application from which data is extracted. We have created the homepage for this application. We have changed the code structure of the front end to make it more fast and efficient.', 'Solution Architecture', 'An application, where the first automated top asset pair selection happens. If the coins are co-integrated, then only one indicator must be executed else trading starts based on 2 indicators.', 'The AI agent will take specific action to trade based on the algorithm.', 'Deliverables', 'We have removed the old API and integrated the new api with the application.', 'We have altered the code structure of the front end to make the code faster and more efficient.', 'Tools used', 'Visual studio code', 'Language/techniques used', 'Python', 'Skills used', 'Django', 'Databases used', 'SQlite', 'Web Cloud Servers used', 'Digital Ocean', 'What are the technical Challenges Faced during Project Execution', 'We faced an issue while integrating coin api with the application while retrieving the data. To retrieve the data using the coin api, we need to input a symbol id. This symbol id is a combination of exchange_name, symbol_type, currency_we_want_to_trade, and quote_currency. There are N coins that can be retrieved using coin api. There are more than multiple exchanges, multiple symbol types, and multiple quote currencies for ONE SINGLE COIN. This makes there is a huge no. Of combinations for one single coin. This made the execution of the api integration very slow.', 'How the Technical Challenges were Solved', 'We created one drop-down for exchange selection, one drop-down for symbol type selection, one drop for coin, and one drop-down for quote currency selection. The user selects these, and in the backend, a combination is created and is sent as input to the coin api code and the data is retrieved without slowing down the process.', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Create a Knowledge Graph to Provide Real-time Analytics, Recommendations, and a Single Source of Truth', 'Next article', 'Golden Record – A knowledge graph database approach to unfold discovery using Neo4j', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2069,https://insights.blackcoffer.com/create-a-knowledge-graph-to-provide-real-time-analytics-recommendations-and-a-single-source-of-truth/,"Create a Knowledge Graph to Provide Real-time Analytics, Recommendations, and a Single Source of Truth - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'Retail', 'Services:', 'Retail Business', 'Organization Size:', '100+', 'The Problem', 'The Client was using NoSql Database which was slow and did not provide real-time response for complex queries. The data had many Connections and it was difficult to represent them in NoSQL or Relational Databases.', 'Our Solution', 'Create a Knowledge Graph and Provide Real-time Analytics and Recommendations using Machine Learning.', 'Solution Architecture', 'Neo4j was Installed on a Cloud VM based on Linodes.', 'Deliverables', 'Knowledge graphs and Data Pipelines are used to Populate the Graph.', 'API’s to Perform CRUD operations in real-time.', 'Tools used', 'Neo4j', 'Postman', 'Language/techniques used', 'Python', 'JSON', 'Models used', 'Node-Relationship model', 'Skills used', 'Programming', 'Data Engineering', 'Data Analytics', 'Databases used', 'Neo4j', 'Web Cloud Servers used', 'Linode', 'What are the technical Challenges Faced during Project Execution', 'Integration of Firestore with Neo4j without any native integration method or driver.', 'How the Technical Challenges were Solved', 'The challenge was solved by using api to retrieve data from Firestore.', 'Project Snapshots', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Advanced AI for Thermal Person Detection', 'Next article', 'Advanced AI for Trading Automation', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2070,https://insights.blackcoffer.com/advanced-ai-for-thermal-person-detection/,"Advanced AI for Thermal Person Detection - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the Middle East', 'Industry Type:', 'Security', 'Services:', 'Security services', 'Organization Size:', '100+', 'The Problem', 'Detect a Person from thermal image and videos. Why this model was created was not told to us by the client.', 'Our Solution', 'Use Deeplearning Computer Vision to train the model on custom dataset and get the results.', 'Solution Architecture', 'Linux 22.04', 'Nvidiva RTX 3080', 'Deliverables', 'Trained model', 'Tools used', 'Labelimg', 'Yolov7', 'COCO2JSON', 'Language/techniques used', 'Python', 'Models used', 'Yolov7', 'Skills used', 'Deeplearning', 'Computer vision', 'Programming', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Advanced AI for Road Cam Threat Detection', 'Next article', 'Create a Knowledge Graph to Provide Real-time Analytics, Recommendations, and a Single Source of Truth', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2071,https://insights.blackcoffer.com/advanced-ai-for-road-cam-threat-detection/,"Advanced AI for Road Cam Threat Detection - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the Middle East', 'Industry Type:', 'Security', 'Services:', 'Security services', 'Organization Size:', '100+', 'The Problem', 'Detect the threat level of accidents between a Pedestrian and a Car.', 'Our Solution', 'Use Deeplearning Computer vision and logic to detect the threat level as defined by the Client.', 'Solution Architecture', 'Linux 22.04', 'Deliverables', 'Program which detects the threat level.', 'Pretrained model.', 'Tools used', 'Yolov7', 'DEEPSORT', 'Opencv', 'Language/techniques used', 'Python', 'Models used', 'Yolov7', 'Skills used', 'Programming', 'Computer Vision', 'Deep learning', 'What are the technical Challenges Faced during Project Execution', 'Integration of Object tracking algorithm with Object detection algorithm.', 'Writing of logic to detect the threat level.', 'How the Technical Challenges were Solved', 'The technical challenge was sorted by testing, experimenting and later on finding and modifying an already existing repository to use as a baseline for our code for integration.', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Advanced AI for Pedestrian Crossing Safety', 'Next article', 'Advanced AI for Thermal Person Detection', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2072,https://insights.blackcoffer.com/advanced-ai-for-pedestrian-crossing-safety/,"Advanced AI for Pedestrian Crossing Safety - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the Middle East', 'Industry Type:', 'Security', 'Services:', 'Security services', 'Organization Size:', '100+', 'The Problem', 'Traffic Signals are inefficient because even if there are no cars or no pedestrians on the road it still works on a timer and stops the traffic or pedestrian unnecessarily.', 'Our Solution', 'We provide a Computer vision-logic to Manipulate the traffic signal to work such that it turns red only when x number of pedestrians are waiting to cross the signal.', 'Solution Architecture', 'Yolov7 pose estimation', 'Opencv', 'Deliverables', 'The program Detects Pedestrians and Gives alerts to traffic Signals to turn Red or stay Green.', 'Yolov7 pose model weights', 'Tools used', 'Yolov7', 'Opencv', 'Language/techniques used', 'Python', 'Computer Vision', 'Models used', 'Yolov7 Pose Estimation', 'Skills used', 'Programming', 'Computer Vision', 'Deep Learning', 'What are the technical Challenges Faced during Project Execution', 'There was no existing solution and we had to create the logic from scratch.', 'How the Technical Challenges were Solved', 'Researching Computer Vision. Learning new Techniques and Experimentation.', 'Project Snapshots', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Advanced AI for Handgun Detection', 'Next article', 'Advanced AI for Road Cam Threat Detection', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2073,https://insights.blackcoffer.com/handgun-detection-using-yolo/,"Advanced AI for Handgun Detection - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the Middle East', 'Industry Type:', 'Security', 'Services:', 'Security services', 'Organization Size:', '100+', 'The Problem', 'Detecting Handguns in images and videos.', 'Our Solution', 'We use Yolov7 instance segmentation model to detect and provide coordinates for handguns.', 'Solution Architecture', 'Linux 22.04', 'Yolo', 'Deliverables', 'Trained model of yolov7 instance segmentation', 'Tools used', 'Openimages', 'Roboflow', 'Yolov7', 'Language/techniques used', 'Python', 'Models used', 'Yolov7_mask', 'Skills used', 'Deeplearning', 'Programming', 'What are the technical Challenges Faced during Project Execution', 'Retrieving handgun images in bulk from opensource.', 'How the Technical Challenges were Solved', 'Found Openimages dataset with good amount of required images', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Using Graph Technology to Create Single Customer View.', 'Next article', 'Advanced AI for Pedestrian Crossing Safety', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2074,https://insights.blackcoffer.com/using-graph-technology-to-create-single-customer-view/,"Using Graph Technology to Create Single Customer View. - Blackcoffer Insights-['Client Background', 'Client:', 'A leading retail firm in Newzealand', 'Industry Type:', 'Retail', 'Services:', 'Retail business', 'Organization Size:', '100+', 'The Problem', 'Companies face issue of having a Single customer under various rows with slightly different information in the same database. This causes unwanted duplication and inaccurate statistics. It also results in inaccurate ad targeting and financial loss.', 'Our Solution', 'We leverage graph technology to create a single customer view by using Complex cypher queries\xa0 and Graph Algorithms.', 'Solution Architecture', 'We have an Azure VM on which we have installed the Neo4j Database. Deployment architecture is a single Instance because of using the Community version of the software.', 'Deliverables', 'Populated Neo4j Database.', 'Required Cypher Queries.', 'Tools used', 'Neo4j', 'Graphlytics', 'Language/techniques used', 'Java', 'Cypher Query', 'Models used', 'Node-Relationship model', 'Skills used', 'Data Analytics', 'Data Engineering', 'Data Science', 'Databases used', 'Neo4j', 'Web Cloud Servers used', 'AZURE', 'What are the technical Challenges Faced during Project Execution', 'Only 1 Difficulty was faced in this Project and that was to migrate data from Elasticsearch to Neo4j.', 'How the Technical Challenges were Solved', 'Research and Experimentation.', 'Project Snapshots', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Car Detection in Satellite Images', 'Next article', 'Advanced AI for Handgun Detection', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2075,https://insights.blackcoffer.com/car-detection-in-satellite-images/,"Car Detection in Satellite Images - Blackcoffer Insights-['Client Background', 'Client:', 'A leading retail firm in the USA', 'Industry Type:', 'Retail', 'Services:', 'Retail business', 'Organization Size:', '100+', 'Project Objective', 'The objective of this project was to detect cars in satellite images and highlight them using a bounding box.', 'Project Description', 'The client, Steffen Schneider, approached us with a requirement to develop a Python project that dealt in the field of computer vision. The main aim of the project was to detect cars present in a satellite image and highlight them using a bounding box. To achieve this, we decided to use the Darknet model and train it on Yolov4 dataset of cars in satellite images.', 'Our Solution', 'We used Google Colab for coding and training the Darknet model. Kaggle was used to download the Yolov4 dataset of cars in satellite images. We preprocessed the dataset and trained the model on it. Once the model was trained, we tested it on sample satellite images and it worked perfectly fine. Finally, we created a script that detected the cars in an image and highlighted them using a bounding box.', 'Project Deliverables', 'The final deliverable was a ipython Notebook presented on Google Colab.', 'Tools used', 'Google Colab, Kaggle, Slack(For Communication)', 'Language/techniques used', 'Python', 'Models used', 'Darknet(CV Model)', 'Skills used', 'Python programming, AI/ML.', 'What are the technical Challenges Faced during Project Execution', 'The main challenge we faced was related to the pre-processing of the Yolov4 dataset of cars in satellite images. The dataset was large and had to be cleaned and formatted before it could be used for training the model.', 'How the Technical Challenges were Solved', 'We used Python programming skills and developed a script that automated the pre-processing of the dataset. This saved us a lot of time and allowed us to focus on training the model.', 'Business Impact', 'The project was a success and the client was very happy with the final product. The car detection model worked perfectly fine on sample satellite images and could be used for further development of an application that could detect cars in real-time.', 'Project website url', 'https://colab.research.google.com/drive/1AoeHdZdpi0lWLf3X2G800J0VT_7wJtnE', 'Project Video', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Building a Physics-Informed Neural Network for Circuit Evaluation', 'Next article', 'Using Graph Technology to Create Single Customer View.', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2076,https://insights.blackcoffer.com/building-a-physics-informed-neural-network-for-circuit-evaluation/,"Building a Physics-Informed Neural Network for Circuit Evaluation - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'Retail', 'Services:', 'Consulting', 'Organization Size:', '100+', 'Project Objective', 'The objective of this project was to build a Physics Informed Neural Network (PINN) using TensorFlow, which could evaluate circuits based on the parameters provided through a MATLAB simulation.', 'Project Description', 'Mohamed provided us with a dataset generated from a MATLAB simulation of a circuit, consisting of various input parameters and the corresponding circuit performance outputs. We were tasked with developing a machine learning model that could accurately predict circuit performance based on the input parameters, while also incorporating the underlying physics principles that govern circuit behavior.', 'Our Solution', 'Our team utilized Jupyter Notebook, Google Colab, Octave, and MATLAB to build the PINN. We used TensorFlow models to build the neural network and Microsoft Excel to clean and preprocess the data. Our team employed Python programming, TensorFlow, Pandas, and MATLAB skills to build the PINN. We did not use any databases for this project, nor did we use any web/cloud servers.', 'Project Deliverables', 'The final deliverable was a functional PINN capable of evaluating circuits based on the provided parameters.', 'Tools used', 'Our team used Jupyter Notebook, Google Colab, Octave, MATLAB, and Microsoft Excel.', 'Language/techniques used', 'The primary languages and techniques we used were Python programming, TensorFlow, and MATLAB.', 'Models used', 'We used TensorFlow models to build the neural network for the PINN.', 'Skills used', 'Our team utilized Python programming, TensorFlow, Pandas, and MATLAB skills to build the PINN.', 'Databases used', 'We did not use any databases for this project.', 'Web Cloud Servers used', 'We did not use any web/cloud servers for this project.', 'What are the technical Challenges Faced during Project Execution', 'The project was very challenging since our team did not have a background in electrical engineering. It was difficult to understand the physics behind the circuit evaluation, and we faced issues when using MATLAB to provide data for the project.', 'How the Technical Challenges were Solved', 'We worked with the client to gain a better understanding of the physics behind the circuit evaluation. We also worked with MATLAB experts to help us better understand how to provide data for the project.', 'Business Impact', 'The PINN we built for Mohamed Zamil allowed for efficient circuit evaluation and improved the overall accuracy of the evaluation process.', 'Project website url', 'https://colab.research.google.com/drive/1HX37MP4Jcb39SWJgkE_5z5n1gQwqWmV9', 'Project Video', 'Contact Details', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'Connecting MongoDB Database to Power BI Dashboard: Dashboard Automation', 'Next article', 'Car Detection in Satellite Images', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2077,https://insights.blackcoffer.com/connecting-mongodb-database-to-power-bi-dashboard-dashboard-automation/,"Connecting MongoDB Database to Power BI Dashboard: Dashboard Automation - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in Newzealand', 'Industry Type:', 'Retail', 'Services:', 'Retail business', 'Organization Size:', '100+', 'Project Objective', 'Brodie Johnco had a MongoDB Database that he wanted to connect to a Power BI Dashboard. However, ODBC connectors were not working for his level of subscription, so he needed a cheaper workaround.', 'Project Description', 'Brodie Johnco had a MongoDB Database containing a large amount of data that he wanted to visualize in a Power BI Dashboard. He initially tried to use ODBC connectors to connect his database to Power BI, but ran into issues due to his level of subscription. We were brought in to help find a cheaper workaround.', 'Our solution involved using Python to extract the relevant data from Brodie’s MongoDB Database. We used the Pandas library to create Dataframes, which we then uploaded to Azure Blob Storage as tables. We set up an Azure pipeline that ran a Python script every 30 minutes to update the tables with new data from the database.', 'Our Solution', 'We used Brodie’s MongoDB Database keys to extract relevant Data Clusters as Pandas Dataframes. We then added them as tables to Azure Blob Storage and set up a Python script to an Azure pipeline that refreshed every 30 minutes. This allowed us to keep the data in sync and provide Brodie with up-to-date information for his Power BI Dashboard.', 'Project Deliverables', 'The final deliverable was a readable CSV file that contained the converted data from the original JSON format.', 'Tools used', 'Jupyter Notebook, Google Colab, Power BI, MongoDB Compass, Microsoft Excel, Azure Blob Storage', 'Language/techniques used', 'Python, Pandas, Azure Cloud Storage', 'Skills used', 'Python programming, Azure Cloud Storage, data extraction and manipulation', 'Databases used', 'MongoDB Database', 'Web Cloud Servers used', 'Azure Blob Storage', 'What are the technical Challenges Faced during Project Execution', 'The main challenge we faced was finding a way to connect Brodie’s MongoDB Database to his Power BI Dashboard without using ODBC connectors. We overcame this challenge by using Python and Azure Blob Storage to extract and store the relevant data.', 'How the Technical Challenges were Solved', 'We solved the issue by using the client’s MongoDB Database keys to extract relevant Data Clusters as Pandas Dataframes. We then added these dataframes as tables to Azure Blob Storage and set the Python script to an Azure pipeline that refreshed every 30 minutes. This allowed the client to access the data in Power BI without the need for ODBC connectors.', 'Business Impact', 'Our solution allowed Brodie to visualize his data in a Power BI Dashboard without having to pay for expensive ODBC connectors. The Azure Blob Storage solution we implemented was much more cost-effective and provided him with up-to-date information every 30 minutes.', 'Project website url', 'https://github.com/AjayBidyarthy/Brodie-Johnco', 'Previous article', 'Data Transformation', 'Next article', 'Building a Physics-Informed Neural Network for Circuit Evaluation', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2078,https://insights.blackcoffer.com/data-transformation/,"Data Transformation - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'Retail', 'Services:', 'Retail business', 'Organization Size:', '100+', 'Project Objective', 'The objective of this project was to convert dirty JSON data present in a CSV file to a readable CSV file. The CSV file contained data in JSON format, which was split into columns in an Excel file, making it hard to read. The client wanted the data to be extracted and converted into a readable format to perform further analysis on it.', 'Project Description', 'Our client had provided us with a CSV file that contained data in JSON format, which was split into columns in an Excel file. The data was hard to read and understand, making it difficult to perform any analysis on it. Our objective was to extract the data, convert it to a readable format, and validate the JSON file to ensure that it was in a correct format. Finally, we had to convert the JSON data into a CSV file that could be easily read and analyzed.', 'Our Solution', 'To extract the data, we used Python programming language and Pandas library. We extracted every piece of text present in the Excel sheet using Pandas and converted it into a readable text format. We then validated the JSON file with a JSON validator website to ensure that it was in the correct format. Finally, we used Pandas again to convert the JSON data into a CSV file that could be easily read and analyzed.', 'To perform the conversion, we used Jupyter Notebook, Json Validator, and Microsoft Excel.', 'Project Deliverables', 'The final deliverable was a readable CSV file that contained the converted data from the original JSON format.', 'Tools used', 'Jupyter Notebook, Json Validator, and Microsoft Excel.', 'Language/techniques used', 'Python programming language and Pandas library.', 'Skills used', 'Python programming and Pandas data manipulation.', 'What are the technical Challenges Faced during Project Execution', 'The main technical challenge we faced during the project was dealing with dirty JSON data present in a CSV file that was split into columns in an Excel file. This made it hard to read and understand, and required extra effort to extract the data and convert it into a readable format.', 'How the Technical Challenges were Solved', 'We solved the technical challenges by using Python programming language and Pandas library to extract and manipulate the data. We validated the JSON data using a JSON validator website to ensure that it was in the correct format. Finally, we used Pandas to convert the JSON data into a readable CSV file that could be easily analyzed.', 'Business Impact', 'The business impact of this project was that the client was able to perform further analysis on the extracted data in a readable format, which was previously hard to read and understand.', 'Project website url', 'https://colab.research.google.com/drive/1yWDj8_HXu6hOYatrzWQ3ezqBxsUON3JY', 'Here are my contact details:', 'Email: ajay@blackcoffer.com', 'Skype: asbidyarthy', 'WhatsApp: +91 9717367468', 'Telegram: @asbidyarthy', 'For project discussions and daily updates, would you like to use Slack, or Skype or Whatsapp? Please recommend, what would work best for you.', 'Previous article', 'E-commerce Store Analysis – Purchase Behavior, Ad Spend, Conversion, Traffic, etc…', 'Next article', 'Connecting MongoDB Database to Power BI Dashboard: Dashboard Automation', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2079,https://insights.blackcoffer.com/e-commerce-store-analysis-purchase-behavior-ad-spend-conversion-traffic-etc/,"E-commerce Store Analysis - Purchase Behavior, Ad Spend, Conversion, Traffic, etc... - Blackcoffer Insights-['Client Background', 'Client:', 'A leading retail firm in the USA', 'Industry Type:', 'Retail', 'Services:', 'Retail business', 'Organization Size:', '100+', 'Project Objective', 'To create a well-designed and informative dashboard for Symbiome e-commerce website using data sourced from Bigquery Database, Google Ads, Google Analytics, and Facebook Ads.', 'Project Description', 'Our client, Arik Oganesian, approached us with a requirement to create a dashboard for his friend’s e-commerce website, Symbiome. The dashboard needed to be visually appealing and provide comprehensive insights into the website’s performance. We sourced data from various sources such as Bigquery Database, Google Ads, Google Analytics, and Facebook Ads. To create the dashboard, we used Google Data Studio and Google Sheets to link the data sources. We also used SQL language to extract data from Bigquery Database. The client specifically asked for cohort retention and cohort revenue charts to be included in the dashboard. With our expertise in data analytics, we were able to fulfill the client’s requirements and provide a dashboard that helped the client make data-driven decisions.', 'Our Solution', 'We used Google Data Studio to create the dashboard and Google Sheets to link the data sources. To extract data from Bigquery Database, we used SQL language. We created a set of charts including cohort retention and cohort revenue charts to fulfill the client’s requirements.', 'Project Deliverables', 'Symbiome E-commerce Dashboard', 'Tools used', 'Google Data Studio and Google Sheets', 'Language/techniques used', 'SQL for Bigquery', 'Skills used', 'Data analytics', 'Databases used', 'Bigquery Database', 'What are the technical Challenges Faced during Project Execution', 'One of the major challenges we faced was extracting data from Bigquery Database using SQL language. However, we were able to overcome this challenge by using our expertise in data analytics.', 'How the Technical Challenges were Solved', 'To solve this issue, we used Google Data Studio and Google Sheets to link the data sources. We also used SQL language to extract data from Bigquery Database. By using these tools, we were able to integrate the data from different sources and create a single comprehensive dashboard that met the client’s requirements.', 'Business Impact', 'The dashboard we created provided a clear view of the website’s performance and helped the client to make data-driven decisions. This resulted in an increase in website traffic and revenue.', 'Project Snapshots', 'Project website url', 'https://lookerstudio.google.com/u/1/reporting/c25c55ae-8052-4166-b363-347a2f8059da/page/SI6uC', 'Project Video', 'Previous article', 'KPI Dashboard for Accountants', 'Next article', 'Data Transformation', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2080,https://insights.blackcoffer.com/kpi-dashboard-for-accountants/,"KPI Dashboard for Accountants - Blackcoffer Insights-['Client Background', 'Client:', 'A leading accounting firm in the USA', 'Industry Type:', 'Finance and Accouting', 'Services:', 'Accounting and financial services', 'Organization Size:', '100+', 'Project Objective', 'The objective of the project was to create a simple and easy-to-use dashboard for the accounting firm Tech 4 Accountants to track their highest performers, target number of clients, current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances.', 'Project Description', 'Our client, Andrew Lassise, wanted a KPI dashboard for Tech 4 Accountants that would help them track their business performance easily. The dashboard needed to have various charts and tables that would display important KPIs in a visually appealing manner.', 'Our Solution', 'To achieve our client’s objectives, we used Google Data Studio and Google Sheets to create a visually appealing and easy-to-use KPI dashboard. We created various charts and tables that displayed the KPIs that our client wanted to track. We used Google Sheets to store the data and created visualizations using Data Studio.', 'Project Deliverables', 'We delivered a KPI dashboard for Tech 4 Accountants that included charts and tables for tracking the highest performers, target number of clients, current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances.', 'Tools used', 'Google Data Studio and Google Sheets', 'Skills used', 'Data Analytics', 'What are the technical Challenges Faced during Project Execution', 'There were no major technical challenges faced during the project execution as the data was stored in Google Sheets, and Data Studio allowed us to easily create visualizations using the data.', 'How the Technical Challenges were Solved', 'No major technical challenges were encountered, and the project was completed smoothly.', 'Business Impact', 'The KPI dashboard that we created for Tech 4 Accountants allowed them to track their business performance easily and make informed decisions. The dashboard helped them identify areas where they needed to improve and make changes to their business strategy accordingly.', 'Project Snapshots', 'Project website url', 'https://lookerstudio.google.com/u/1/reporting/fbf7879a-be79-4cb9-b7d4-783bf7447902/page/Hmg2C', 'Project Video', 'Previous article', 'Return on Advertising Spend Dashboard: Marketing Automation and Analytics using ETL and Dashboard', 'Next article', 'E-commerce Store Analysis – Purchase Behavior, Ad Spend, Conversion, Traffic, etc…', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2081,https://insights.blackcoffer.com/return-on-advertising-spend-dashboard-marketing-automation-and-analytics-using-etl-and-dashboard/,"Return on Advertising Spend Dashboard: Marketing Automation and Analytics using ETL and Dashboard - Blackcoffer Insights-['Client Background', 'Client:', 'A leading ad firm in India', 'Industry Type:', 'Ads', 'Services:', 'Ads, Marketing, and Promotions', 'Organization Size:', '100+', 'The Problem', 'The main problem that was addressed in this project was the manual calculation of Return on Advertising Spend (ROAS) due to the lack of a centralized platform for running ads. The client’s ads were spread across multiple revenue generating platforms, including Google Adsense, Adx, and Ezoic, while the spending was managed through the Google Ads Platform. At that time, the client lacked a centralized dashboard or website that could effectively calculate ROAS by integrating revenue and cost streams. This fragmentation made it challenging for the client to track and evaluate the effectiveness of their advertising campaigns. Therefore, a comprehensive solution was developed and implemented, providing a centralized platform for calculating ROAS, aligning revenue and cost data from various sources, and enabling informed decision-making regarding advertising investments.', 'Our Solution', 'We developed a comprehensive solution to address the challenges faced by the client in calculating Return on Advertising Spend (ROAS) and centralizing their advertising data. The solution involved collecting data from four different APIs: Google Ads API for spending data, Google Adsense API, Ad Manager API, and Ezoic data for revenue data. To ensure compatibility, we utilized an Extract, Transform, Load (ETL) tool to convert the data received from each API, which was in different formats, into a standardized format storing them Pandas Dataframe for both revenue and spending data.', 'The transformed data was then stored in a Postgres database for easy access and management. To automate the data extraction process, we implemented an ETL script that runs twice daily via cronjob on a Digital Ocean VM, ensuring the latest data is always available.', 'Moreover, we designed a backend API using the Flask framework. This API fetched the required data from the Postgres DB, allowing users to retrieve relevant information efficiently.', 'Finally, we implemented a ROAS Dashboard frontend to display the calculated ROAS using the fetched values. The dashboard provided a visually appealing and intuitive interface for users to track and monitor their advertising performance. With our solution in place, the client could now easily monitor ROAS over time, access consolidated data, and make informed decisions regarding their advertising investments.', 'Solution Architecture', 'The solution architecture involved a multi-step process to address the challenges faced by the client in calculating ROAS and centralizing their advertising data. Data was collected from various APIs, including Google Ads API, Google Adsense API, Ad Manager API, and Ezoic data, and transformed into a standardized format using an ETL tool.', 'The transformed data was stored in a Postgres database, and a backend API was developed using the Flask framework to fetch the required data. The calculated ROAS was then displayed on a Next Js Dashboard, providing users with an intuitive interface to track and analyze their advertising performance.', 'Deliverables', 'ETL Tool', 'Deployment on Digital Ocean', 'Backend API', 'Next js backend/ frontend', 'ROAS Dashboard', 'Tools used', 'Google Ads API', 'Google AdSense API', 'Adx API', 'Ezoic API', 'Python 3.9', 'Jupyter Notebook', 'Flask', 'Digital Ocean Droplet', 'Next Js frontend/backend Stack', 'Vuexy Template for ROAS Dashboard', 'Language/techniques used', 'Python 3.9', 'Flask API', 'DigitalOcean Droplet', 'Functional Programming in Python', 'ETL Tool', 'Skills used', 'Python', 'Git', 'Deployment', 'Data Engineering', 'Web Development using Next js', 'Databases used', 'We used', 'PostgreSQL', 'database for the project.', 'Web Cloud Servers used', 'Digital Ocean Droplet', 'What are the technical Challenges Faced during Project Execution', 'Some of the technical challenges encountered were:', 'Ensuring data integrity during the transformation process.', 'Deployment of Docker image on VM', 'Setting up an automated ETL pipeline.', 'Adding SSL certificate to backend API.', 'How the Technical Challenges were Solved', '1. Ensuring data integrity: Implemented checks, cleansing, and validation to maintain the accuracy and reliability of the data.', '2. Docker image deployment on VM: Configured VM to support Docker Image for ETL and deployed the image for seamless execution.', '3. Setting up automated ETL pipeline: Automated data extraction, transformation, and loading processes for efficient data management via cronjob.', '4. Adding SSL certificate to backend API: Secured backend API with SSL certificate, enabling encrypted communication for enhanced data protection.', 'Business Impact', 'The implemented solution had a significant positive impact on the client’s business. By providing a centralized platform for calculating ROAS and integrating data from multiple revenue-generating platforms, the client gained valuable insights into the effectiveness of their advertising campaigns. The availability of real-time, consolidated data enabled informed decision-making regarding advertising investments. The user-friendly interface of the RAOS Dashboard allowed the client to easily track and monitor their advertising performance, leading to improved campaign optimization and potentially higher returns on advertising spend. Overall, the solution streamlined the client’s advertising operations, resulting in increased efficiency and improved business outcomes.', 'Project Snapshots', 'Here are the project snapshots:', 'Login Screen', 'Landing page with first selected campaign in the list:', 'Using Date Picker', 'Search Functionality', 'Revenue Breakdown by Platform', 'Show/Hide Left Sidebar', 'Switching Site’s theme to Light Mode', 'Settings/Log Out Menu', 'Change Email/Password', 'Project Website URL:', 'https://roasing.com/', 'Project Video', 'Previous article', 'Grafana Dashboard – Oscar Awards', 'Next article', 'KPI Dashboard for Accountants', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2082,https://insights.blackcoffer.com/ranking-customer-behaviours-for-business-strategy/,"Ranking customer behaviours for business strategy - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Retail Firm in the USA', 'Industry Type:', 'Retail', 'Services:', 'Retail Business', 'Organization Size:', '100+', 'The Problem', 'Create\xa0an\xa0API\xa0service\xa0that\xa0will\xa0parse\xa0text,\xa0include\xa0comments,\xa0analyse\xa0the\xa0remarks,\xa0assign\xa0a\xa0score\xa0based\xa0on\xa0sentiment\xa0or\xa0other\xa0criteria,\xa0etc.\xa0Feed\xa0it\xa0comments,\xa0and\xa0it\xa0should\xa0analyse\xa0the\xa0syntax\xa0and\xa0sentiment\xa0of\xa0the\xa0comments\xa0as\xa0well\xa0as\xa0extract\xa0key\xa0terms\xa0to\xa0add\xa0to\xa0the\xa0extended\xa0meta\xa0data\xa0of\xa0that\xa0model.\xa0In\xa0order\xa0for\xa0us\xa0to\xa0know\xa0a\xa0user’s\xa0behaviour,\xa0personal\xa0information,\xa0and\xa0more\xa0meta\xa0data\xa0about\xa0their\xa0interests', 'Our Solution', 'Created a flask API, that will take comments as input and will textual analysis as follows:', 'Spell and Grammar Check', ': We have used', 'language tool python', 'for this ,', 'LanguageTool', 'is an open-source grammar tool, also known as the spellchecker for OpenOffice. This library allows you to detect grammar errors and spelling mistakes through a Python script or through a command-line interface.', 'Sentimental Analysis', ': For Sentimental Analysis we used FLAIR, Flair is a pre-trained embedding-based model. This means that each word is represented inside a vector space. Words with vector representations most similar to another word are often used in the same context. This allows us, to, therefore, determine the sentiment of any given vector, and therefore, any given sentence.', 'Keywords Extraction', ':', 'For keywords extraction we used', 'SPACY', 'which is newer than NLTK or Scikit-Learn, is aimed at making deep learning for text data analysis as simple as possible. The following are the procedures involved in extracting keywords from a text using spacy.', 'Split the input text content by tokens', 'Extract the hot words from the token list.', 'Set the hot words as the words with pos tag “', 'PROPN', '“, “', 'ADJ', '“, or “', 'NOUN', '“. (POS tag list is customizable)', 'Find the most common T number of hot words from the list', 'Solution Architecture', 'Deliverables', 'CommentScoringAPI that will take comments/reviews as input, and do the textual analysis on the given comment and will return the Comment Score based on counts of spell and grammar errors, sentiments, hot keywords.', 'Tools used', 'Numpy', ',', 'pandas', ',', 'flask', ',', 'NLTK', ',', 'Spacy', '(Keyword Extraction),', 'language tool python', '(spell and grammar check),', 'flair', '(Sentimental Analysis)', 'Language/techniques used', 'Python', 'Business Impact', 'Client have a user schema that contain all the information of users that have visited there platform, and he/she want to build a Script that will take all the reviews of a certain User as input and than will do textual analysis on all the comments of the user , by textual analysis we mean Spell and Grammar Check, Sentimental Analysis, and Keywords extraction. Based on these factors our Script scored each user and helped Client to understand his/her users well.', 'Previous article', 'Algorithmic trading for multiple commodities markets, like Forex, Metals, Energy, etc.', 'Next article', 'Rise of Chatbots and its impact on customer support by the year 2040', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2083,https://insights.blackcoffer.com/algorithmic-trading-for-multiple-commodities-markets-like-forex-metals-energy-etc/,"Algorithmic trading for multiple commodities markets, like Forex, Metals, Energy, etc. - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Trading Firm in the USA', 'Industry Type:', 'Finance', 'Services:', 'Trading, Consulting, Software', 'Organization Size:', '100+', 'The Problem', 'A Trading site will have all the required features, allowing users to trade in multiple commodities markets, like Forex, Agriculture, Metals, Energy etc.', 'Our Solution', 'Designed the website with technical indicators, and the ability to trade in live market, plus allows the user to create his/her own strategy to backtest. Functionalities like all types of technical indicators:', 'Trend following', 'mean reversion', 'relative strength', 'volume', 'momentum.', 'Strategies are specific scripts, which are able to send, modify, execute, and cancel buy or sell orders and simulate real trading right on your chart. Backtesting is the process of recreating the work of your strategies on historical data, essentially all of your past strategic work. Forward testing allows for the recreation of your strategy work in real time, all while your charts refresh their data.', 'Solution Architecture', 'Deliverables', 'A Fully functional trading\xa0platform that lets you customize technical indicators, create charts, and analyse financial assets. These indicators are patterns, lines, and shapes that millions of traders use every day. Platform designed is entirely browser-based, with no need to download a client. Allowing the user to use all types of indicators:', 'Trend following', 'mean reversion', 'relative strength', 'volume', 'momentum.', 'Tools used', 'Numpy', 'pandas', 'Language/techniques used', 'Python', 'Business Impact', 'Clients want a social media network, analysis platform, and mobile app for traders and investors. So we designed a website with all the client’s requirements, where traders, investors, educators, and market enthusiasts can connect to share ideas and talk about the market. By actively participating in community engagement and conversation, you can accelerate your growth as a trader, and your ability to trade in the live market, plus allows the user to create his/her own strategy to backtest. A Fully functional trading platform that lets you customize technical indicators, create charts and analyze financial assets. These indicators are patterns, lines, and shapes that millions of traders use every day. Platform designed is entirely browser-based, with no need to download a client. Allowing the user to use all types of indicators', 'Project Snapshots', 'Previous article', 'Trading Bot for FOREX', 'Next article', 'Ranking customer behaviours for business strategy', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2084,https://insights.blackcoffer.com/trading-bot-for-forex/,"Trading Bot for FOREX - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Trading Firm in the USA', 'Industry Type:', 'Finance', 'Services:', 'Trading, Consulting', 'Organization Size:', '100+', 'The Problem', 'Automate\xa0trading\xa0on\xa0the\xa0MT4\xa0terminal\xa0for\xa0forex\xa0when\xa0certain\xa0conditions\xa0are\xa0met,\xa0and\xa0end\xa0trade\xa0at\xa0the\xa0best\xa0exit\xa0point.', 'Save mt4 forex data for a instrument live for every tick.', 'Our Solution', 'Use PyTrader to log into trading system (mt4) for 2 brokers.', 'Use live prices to identify when prices diverge.', 'Buy one currency on broker 1, sell currency on broker 2.', 'Hold until prices come back together.', 'Coded a MQL4 script that will save tick data (bid, ask, open, high, low, close) for any instrument when active', 'Solution Architecture', 'Deliverables', 'Python Script to Automate the two Meta Trader 4 terminals, and trade when some conditions are true and break the trade at a exit point.', 'A MQL4 Sript that will Save the Live tick data (Bid, Ask, Spread, Open, High, Low, Close) in a CSV file.', 'Tools used', 'PyTrader', 'numpy', 'pandas', 'Language/techniques used', 'Python', '(Automation)', 'Mql4', '(To save tick data)', 'Business Impact', 'Client requirements were\xa0 to automate his forex trading strategy\xa0 on Meta Trader4 terminal, so that he doesn’t have to bother trading anymore, the Python script we designed to not only do it, plus it offers a safe exit point for Ongoing Trades, that saved the client’s money and time.', 'Previous article', 'Python model for the analysis of sector-specific stock ETFs for investment purposes', 'Next article', 'Algorithmic trading for multiple commodities markets, like Forex, Metals, Energy, etc.', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2085,https://insights.blackcoffer.com/python-model-for-the-analysis-of-sector-specific-stock-etfs-for-investment-purposes%ef%bf%bc/,"Python model for the analysis of sector-specific stock ETFs for investment purposes - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Investment Firm in the USA', 'Industry Type:', 'Finance', 'Services:', 'Investment, Consulting', 'Organization Size:', '100+', 'The Problem', 'Have an existing Python model that has been built for the analysis of sector-specific stock ETFs for investment purposes. Need to update the existing selection criteria to adjust the selection filter and add a screening criterion that drops off one or more of the proposed holdings, and to have the ability to adjust the parameters of the selection criteria to test different variables.', 'Our Solution', 'The 2 in 4 Fundamental model screens a fundamental ranking of stock market sectors, picks the top ranked holding and continues to hold that sector as long as it remains in the top four rankings.\xa0 The model holds two positions at a time.\xa0 The sector ranking data is in the wcm5.xlxs file.\xa0 We input data from the PRICES.CSV file to pull up monthly returns.\xa0 When I go to run the program, I use the 2_in_4_New.py and that give me the current rankings for both the fundamental and technical rankings.', 'Sometimes a sector is ranked as being fundamentally attractive because it has become cheaper because of problems going on within an industry.\xa0 What I would like to do is to test out a way of screening out a sector based upon poor performance over a lookback period.\xa0 Here is what the new model would do.', 'Screen for a the specific number of sectors, probably between three and five, based upon the fundamental ranking over an average time period (currently 3 weeks)', 'Choose either three, four, or five holdings', 'Exclude the holding that has the weakest performance over a specify lookback period, let’s start with 52 weeks, but I would like to be able to adjust this variable', 'compare the performance of various combinations, seeing the return on an annual basis if possible, as well as showing the maximum drawdown', 'Solution Architecture', 'Deliverables', 'An Updated, Optimised Python script that will filter and return Technical and Financial holdings, with a Price filter that will do price analysis on a certain lookback period.', 'Tools used', 'Numpy', 'pandas', 'itertools,', 'combinations', 'permutations', 'Language/techniques used', 'Python', 'Business Impact', 'The client now can get more than 2 Financial and technical holdings , up to maximum 5 holdings for both Technical and Financial, plus the holdings were more accurate because of the new added Price Filter that will Exclude the holding that has the weakest performance over a specify lookback period, default 52 weeks. It boosted the Client’s profit because of the more accurate and optimised functional filters.', 'Project Snapshots', 'Previous article', 'Rise of e-health and its impact on humans by the year 2030', 'Next article', 'Trading Bot for FOREX', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2086,https://insights.blackcoffer.com/medical-classification/,"Medical Classification - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Tech Firm in the USA', 'Industry Type:', 'IT Consulting', 'Services:', 'Software, Consulting', 'Organization Size:', '100+', 'Project Objective', 'Classify the medical research paper into 0 if the medical research paper cannot be used in future medical research and 1 if the medical research paper can be used in research based on some research-related phrases.', 'Train an ML/DL model on classified data.', 'Project Description', 'We have given an excel sheet of medical research paper text and provided some phrases to identify research papers that can be used for future medical research. If the phrase is not present in a research paper then it will not be used for research. After annotation, we need to find the best ML/DL model to train research data and evaluate the model on test data.', 'Our Solution', 'We have created a python script that can compare all medical research paper text to research phrases and annot 0 if research phrases are not present in a medical research paper and 1 if research phrases present in medical research paper.', 'After annotation we have trained different machine learning and deep learning models like Bert base uncased using Tensorflow, bert large, XGBoost Classifier, Random Forest Classifier and Logistic Regression. Among these models we have chosen the best accuracy\xa0 parameters model. In our case the bert-base model performed good and gave 95% test accuracy.', 'Project Deliverables', 'ML/DL model which is trained on medical research classification data to classify other medical research papers.', 'Tools used', 'Google Colab notebooks, Tensorflow, PyTorch, Transformers, MS Excel', 'Language/techniques used', 'Python, Machine learning, Deep learning, Data Science, Natural Language Processing (NLP).', 'Models used', 'Tensorflow-Bert model, PyTorch LSTM model, Random Forest Classifier, XGBoost Classifier, Logistic Regression.', 'Skills used', 'Machine Learning, Deep learning, NLP, Python programming.', 'Databases used', 'used ms excel data', 'What are the technical Challenges Faced during Project Execution', 'There are various technical challenges faced during project execution:', 'The research paper has a huge amount of text data so the model was giving space errors in colab notebooks.', 'Find the best threshold value which gives best test accuracy.', 'How the Technical Challenges were Solved', 'To solve space error we have trained the model with lower batch size so this solved the error.', 'To find the best threshold value we created the ROC AUC curve and Precision\xa0 Recall curve and checked best points where accuracy will be higher.', 'Previous article', 'Design & Develop BERT Question Answering model explanations with visualization', 'Next article', 'Playstore & Appstore to Google Analytics (GA) or Firebase to Google Data Studio Mobile App KPI Dashboard', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2087,https://insights.blackcoffer.com/design-develop-bert-question-answering-model-explanations-with-visualization/,"Design & Develop BERT Question Answering model explanations with visualization - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Tech Firm in the USA', 'Industry Type:', 'IT Consulting', 'Services:', 'Software, Consulting', 'Organization Size:', '100+', 'Project Description', 'We need to use a pre-trained bert question answering model and create a notebook that has explanations of model’s working with some visuals of bertviz, allennlp and gradient values.', 'Our Solution', 'We created a notebook first and explained the model with model view and head view visuals of bertviz library. It gives similarity between words so we can easily find related words.', 'We used the allennlp library and created bar charts and heatmaps to show higher and lower attention words. It means when it finds question related words in the context it gives higher value to those words and if words are not related it gives lower values.', 'We used a gradient based method to show higher and lower gradient values word according to question text and created bar charts and text color charts to show higher gradient values.', 'Project Deliverables', 'A notebook which has an explanation of the bert question answering model using some visualization.', 'Tools used', 'Google colab notebooks, Tensorflow, Bertviz, Allennlp, Transformers', 'Language/techniques used', 'Python programming language, Deep learning, NLP, Data Visualization', 'Models used', 'Pretrained bert-base-uncased model and distilbert model (both trained on squad2 dataset)', 'Skills used', 'Data visualization, Deep learning, NLP, python', 'What are the technical Challenges Faced during Project Execution', 'We need to use the best pre-trained model which can give good results on different questions and answers.', 'We were working on text data so we need to use charts which can clearly show differences between higher attention and lower attention value words.', 'How the Technical Challenges were Solved', 'For best pretrained we tried different Bert’s pretrained models like distilbert(trained on squad dataset), distilbert(trained on squad2), bert base uncased, bert large and roberta base.', 'Among these models we kept the best one.', 'For solving charts related issues we used heatmap chart, bar chart with dark and light colors and text coloring method.', 'Project Snapshots', 'Previous article', 'Design and develop solution to anomaly detection classification problems', 'Next article', 'Medical Classification', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2088,https://insights.blackcoffer.com/design-and-develop-solution-to-anomaly-detection-classification-problems/,"Design and develop solution to anomaly detection classification problems - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Tech Firm in the USA', 'Industry Type:', 'IT Consulting', 'Services:', 'Software, Consulting', 'Organization Size:', '100+', 'Project Description', 'We need to create a notebook with solutions to binary classification-related anomaly detection problems. We need to use machine learning and deep learning models which have greater than 90% accuracy.', 'Our Solution', 'We created a notebook for anomaly detection. We used 10 to 15 machine learning and deep learning models but only\xa0 3 different types of auto encoder models that were giving greater than 90% accuracy. We trained all 3 models on one classification data which have anomalies and evaluated trained models on test data.', 'Project Deliverables', 'A notebook that has solutions for anomaly detection related classification problems and accuracy should be above 90%.', 'Tools used', 'Google colab notebooks, Tensorflow, Google drive', 'Language/techniques used', 'Python programming language, Machine learning, Deep learning, Data analysis and Data visualization.', 'Models used', 'Auto Encoder and Variational Auto Encoder', 'Skills used', 'Python, Data Analysis, Data visualization, Machine learning, Deep learning.', 'Databases used', 'MS Excel', 'What are the technical Challenges Faced during Project Execution', 'Most of the anomaly detection models work with regression type data and this problem was classification problem so we need to deal with classification data.', 'Getting high accuracy is also a tough challenge for us because there are only a few models which work well on anomaly detection related classification problems.', 'How the Technical Challenges were Solved', 'So we have limited models for this problem so we used only classification models like Autoencoders, Isolation forest and one class svm.', 'Only Autoencoder was giving high accuracy so we worked with different types of autoencoders like variational autoencoder and normal autoencoder.', 'Project Snapshots', 'Previous article', 'An ETL Solution for Currency Data to Google Big Query', 'Next article', 'Design & Develop BERT Question Answering model explanations with visualization', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2089,https://insights.blackcoffer.com/an-etl-solution-for-currency-data-to-google-big-query/,"An ETL Solution for Currency Data to Google Big Query - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Tech Firm in the USA', 'Industry Type:', 'IT Consulting', 'Services:', 'Software, Consulting', 'Organization Size:', '100+', 'Project Objective', 'Fetch currency data from Pure-clear API and store it to Google cloud BigQuery.', 'Create a Google cloud function to automate the above process.', 'Project Description', 'We have given a pure-clear API and a google cloud account. We need to fetch currency data from that pure-clear API using python and need to store fetched data in Google Cloud Bigquery.', 'We also need to automate the above process like the process runs on a daily basis and update the currency data on Bigquery.', 'Our Solution', 'We have created a python program that can fetch pure-clear API data. The API data was in JSON format but we needed table format so we used python package pandas. We converted json data to tabular format using pandas. After that, we connected python code to google cloud using google’s authentication module and then stored data frame (table) directly to BigQuery using the “.to_gbq” method.', 'We also need to run the above process daily to update new data in BigQuery. For this Google cloud provides a “Cloud function” tool. In this, we can create a function and set up their running process. So we created a function and attached the above code to that function and set up a cloud function to run daily.', 'Project Deliverables', 'A Google cloud function that runs daily and updates data on Google BigQuery', 'Tools used', 'Cloud function, BigQuery of Google Cloud, Google Colab notebook, Python programming, Pandas', 'Language/techniques used', 'Python language and pandas module', 'Skills used', 'Python programming, Data handling, Google Cloud', 'Databases used', 'Google Cloud BigQuery', 'Web Cloud Servers used', 'Google Cloud Server', 'What are the technical Challenges Faced during Project Execution', 'Connecting google cloud to python code is challenging because Its credentials should be in a specified format otherwise it shows an authentication error.', 'How the Technical Challenges were Solved', 'To tackle this challenge we created a dictionary format (key-value pair) and stored all the authentication variables in the dictionary as a key value pair. Then we used google’s authentication library “google.auth” and passed a dictionary to the service_account method and stored it in different variables so we can store data from pandas dataframe to Google BigQuery.', 'Project Snapshots', 'Previous article', 'ETL and MLOps Infrastructure for Blockchain Analytics', 'Next article', 'Design and develop solution to anomaly detection classification problems', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2090,https://insights.blackcoffer.com/etl-and-mlops-infrastructure-for-blockchain-analytics/,"ETL and MLOps Infrastructure for Blockchain Analytics - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Blockchain Tech Firm in the USA', 'Industry Type:', 'AR/VR', 'Services:', 'Metaverse, NFT, Digital Currency', 'Organization Size:', '100+', 'Project Objective', 'Code for extraction of the price of cryptocurrency', 'Required real-time data of cryptocurrency and this is extracted from the cryptocurrency URL', 'Forecast code for prediction of the price', 'Built FastApi to reduce interaction complexity for the user', 'Project Description', 'ETL and MLOps Infrastructure for Blockchain Analytics this entire project completes in 4 outlines and stages. In the first segment data scraping for the price of the cryptocurrency. The second stage is, Loading the data into the Microsoft MYSQL server and Transforming data into the required shape for the automated process data Load into the Amazon RDS tool management service which knows as the Amazon relational database service, and creating DB instances (DB instance class – db.t3.small', ')', '.', 'In the fourth stage, built the FastAPI for the get data to the fingertips and easily accessible for the client because it reduces the time to fetch the price of a particular cryptocurrency with a single click, and increases the efficiency of understanding.', 'Our Solution', 'This Project Module develops according to the Client’s Requirements which involves Data extraction of Cryptocurrency data from a given URL by the Client, it also changes the data format, and attributes nomenclature according to the requirements. After extracting the data its loads into Microsoft MYSQL Server for the transformation of data and for full automation process, used Amazon RDS and built the FastAPI.', 'Project Deliverables', '–\xa0 Data Scraping code using Python', '–\xa0 ETL code for extracting, Transform and Loading into Microsoft MYSQL server', '–\xa0 AWS RDS (db.t3.samll) instances for storing data and for deployment', '–\xa0 Built FastAPI for getting the price of cryptocurrency', 'Tools used', '– VC code and Google Collab', '– Microsoft MYSQL server', '– AWS RDS services', 'Language/techniques used', 'Data Scraping using Python', 'ETL process to extract, load, and transform the data', 'FastAPI using Python', 'Amazon Cloud services', 'Skills used', '– Data scraping using python', '– ETL setup', '– Aws web services', '– FastAPI using Python', 'Databases used', '– Microsoft MYSQL server', '– Aws RDS (Amazon Relational Database services)', 'Web Cloud Servers used', '-AWS RDS services', 'What are the technical Challenges Faced during Project Execution', 'Data scraping speed does not meet the expected speed (events/sec)', 'API calls have their own limitation in requesting calls per sec', 'Storing the huge amount of data', 'How the Technical Challenges were Solved', 'Get the Premium service of API calls (20 calls/sec)', 'Used the AWS RDS for storing the data and for faster execution', 'Business Impact', 'This Project impact is directly responsible to the investors of the cryptocurrency.', 'To get the prices of cryptocurrency on fingers tips and use it for buying and investing money in the right corner of the cap market of finance.', 'It clearly impacts financially to the investors and helps them for investing purposes.', 'The scope impact of product service is worldwide for purchasing any cryptocurrency in the world.', 'To provide these impactful services, there is a tech team of Blackcoffer behind it.', 'Project Snapshots', 'Project website URL', '127.0.0.1:62190', 'Project Video', 'https://www.youtube.com/watch?v=xDeL5YggxDw&ab_channel=Blackcoffer', 'Previous article', 'An agent-based model of a Virtual Power Plant (VPP)', 'Next article', 'An ETL Solution for Currency Data to Google Big Query', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2091,https://insights.blackcoffer.com/an-agent-based-model-of-a-virtual-power-plant-vpp/,"An agent-based model of a Virtual Power Plant (VPP) - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Energy Firm in the USA', 'Industry Type:', 'Energy', 'Services:', 'Power, Energy, Distribution', 'Organization Size:', '5000+', 'Project Objective', 'To create an agent based model of a virtual power plant in Netlogo. To see the function of multiple such power plants that worked simultaneously. These power plants created and supplied energy based on a demand parameter that can be controlled by the observer', 'Project Description', 'The client defined specific requirements as to how he wanted the model to be.', 'The requirements were divided into 4 parts. Each successive part increased in complexity and required the model to be adjusted or configured to fit that part into it', 'The entire model when completed contained all the four parts defined by the client in the Statement of work.', 'Our Solution', 'Created the model according to requirements.', 'The clustering of multiple agents and their position is decided mathematically based on the total number of agents and the sum of their energies. The agents form a cluster based on the condition that the sum of their power is a figure that is above a certain threshold amount, the threshold amount is also decided by the observer.', 'Project Deliverables', 'https://github.com/AjayBidyarthy/Shingi-Samudzi-Build-Netlogo-ABM-for-simulating-Virtual-Power-Grid-economics', 'Above is the github link to every state of the model that was delivered to the client.', 'The uploads start from a basic model with only clustering of the agents', 'The final upload is a model that contains the full representation of a VPP for simulation.', 'Tools used', '-Netlogo', '– python', 'Language/techniques used', 'Netlogo uses a specific language that resembles the logo language but has it’s unique syntax and variations in the way variables are stored and how a list is parsed', 'Models used', 'Clustering', 'Skills used', 'Netlogo programming', 'What are the technical Challenges Faced during Project Execution', 'The major challenge was controlling the behavior of each agent in the model. The lack of understanding of the language and the available resources about it made it challenging to figure out the actual behavior of the agents and the overall model.', 'The decision to decide where exactly each agent will cluster on the grid was difficult primarily because each agent spawned on a random patch of the screen. This meant that each agent would have to be given a spot to land on and form a cluster with other agents.', 'The next challenge was deciding the condition on which the agents will cluster as their relative distance to each other couldn’t be used as a parameter as it wasn’t relevant to the model’s purpose.', 'How the Technical Challenges were Solved', 'The technical challenges were solved by extensive research and referring to several forums over the span of 2 months.', 'Project Snapshots', 'Project Video', 'https://www.youtube.com/watch?v=1fzCUzZ0q0Q&ab_channel=Blackcoffer', 'Previous article', 'Transform API into SDK library and widget', 'Next article', 'ETL and MLOps Infrastructure for Blockchain Analytics', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2092,https://insights.blackcoffer.com/transform-api-into-sdk-library-and-widget/,"Transform API into SDK library and widget - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Tech Firm in the USA', 'Industry Type:', 'IT', 'Services:', 'Consulting, Marketing, Healthtech', 'Organization Size:', '500+', 'Project Objective', 'Convert API documentation into SDK library and widget. Expected deliverables are SDK library and widgets for', 'Web apps', 'iOS apps', 'Android Apps', 'Project Description', 'API documentation is available for a tool that allows customers to type in their medication and find the cheapest price near them. For partners who want to have it on their own site, currently using the API documentation but would like to ultimately be able to send them an embeddable widget that incorporates the tool on their site', 'Our Solution', 'We created a flutter widget that uses\xa0 SDK libraries that allows the customer to type their medication and find the cheapest price near them.', 'This widget can be embedded in their web, android and IOS applications', 'Project Deliverables', '1)SDK Library/Widget', '2)Sample flutter application', 'Tools used', 'Flutter', 'Language/techniques used', 'Dart', 'Skills used', '1)Knowledge of dart language', '2)flutter app developing', 'What are the technical Challenges Faced during Project Execution', '1 )Problems while fetching details of drugs and pharmacies', '2) Showing details of drugs and pharmacies in the widget', 'How the Technical Challenges were Solved', 'All technical challenges are solved by proper communication with the client and by logical analyzing of data', 'Project Snapshots', 'Project Video', 'https://www.youtube.com/watch?v=MyNK_DPtsKA&ab_channel=Blackcoffer', 'Previous article', 'Integration of a product to a cloud-based CRM platform', 'Next article', 'An agent-based model of a Virtual Power Plant (VPP)', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2093,https://insights.blackcoffer.com/integration-of-a-product-to-a-cloud-based-crm-platform/,"Integration of a product to a cloud-based CRM platform - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Logistics Firm Worldwide', 'Industry Type:', 'Logistics', 'Services:', 'Import, Export, Supply Chain, Logistics, Trades', 'Organization Size:', '500+', 'Project Description', 'The main challenge faced by the team was the integration of the two systems themselves.', 'Since one-by-one entering of records into each module is a mundane task and a waste of valuable time we proposed the automation using APIs.', 'Our Solution', 'The challenge was divided into two milestones and sub-tasks for each.', '1. First was the ingestion of existing data into the cloud-based CRM platform.', '2. Second was the question of automating the process of adding newer records to the cloud platform.', 'Project Deliverables', 'The client has been provided with python scripting handling bulk data ingestion to CRM and also the script to handle daily synchronization of data.', 'Tools used', '– Python', '– MySQL Database', '– Postman', '– TeamViewer', 'Language/techniques used', '– Automation', '– 3', 'rd', 'party APIs', '– Authentication methods', '– Multi-Threading of function calls', '– bat Scripts for easier running of scripts for the client', 'Models used', 'Python Frameworks like requests to build own custom client for consumption of APIs.', 'Skills used', 'Python Programming, Mult-threading, APIs', 'Databases used', 'The client provided a MySQL instance.', 'Web Cloud Servers used', 'Zoho', 'What are the technical Challenges Faced during Project Execution?', '– Writing own client-side API-consumption code handling API calls from Authentication and Other Operations as per task requirements.', '– Debugging of API responses was messy.', 'How the Technical Challenges were Solved', '– Multiple alternatives were discussed and implemented in python like conditional refreshing of API tokens.', '– Automation of daily synchronization handled by use of time deltas.', '– Logging of all operations to efficiently handle errors in the future.', 'Business Impact', '– Automated workflow of the client', '– No need for dull tasks like data entry to CRM modules everything is taken care of using logic.', 'URL', 'https://www.exportgenius.in/', 'Previous article', 'A web-based dashboard for the filtered data retrieval of land records', 'Next article', 'Transform API into SDK library and widget', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2094,https://insights.blackcoffer.com/a-web-based-dashboard-for-the-filtered-data-retrieval-of-land-records/,"A web-based dashboard for the filtered data retrieval of land records - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Real Estate Firm in the USA', 'Industry Type:', 'Real Estate', 'Services:', 'Land, Infrastructure, Real Estate, Investment', 'Organization Size:', '100+', 'Project Description', 'The client’s own raw database needed to be converted into a dynamic web application with modern features like user management and subscription where users could explore land records as per their wish.', 'Our Solution', 'Created the web application as per client needs.', 'Added user functionality to handle signup/logins and added authorization middlewares to protect routes from unwanted access.', 'Transformed raw data into a meaningful NoSQL-based database with a proper schema being served as an instance on a cloud service named', '‘ MongoDB Atlas ‘.', 'Project Deliverables', 'Pushed code to the required GitHub repository.', 'Tools used', '– Vanilla javascript', '– Javascript Frameworks ( Nodejs, express , cors )', '– Postman', 'Language/techniques used', '– JavsScript', '– Backend Service setup ( express, cors , js )', '– Fronted logic setup ( HTML , CSS , JavaScript , Jquery )', 'Models used', 'Backend: An API service created to handle land records database and queries made by users.', 'Frontend: A frontend client is available as a web application where users can signup and access land records.', 'Skills used', 'JavaScript Programming, APIs, JavaScript Frameworks ( NodeJS, Express\xa0 , cors ) , Web Design, NoSQL querying in MongoDB.', 'Databases used', 'MongoDB (NoSQL)', 'Web Cloud Servers used', 'MongoDB Atlas', 'What are the technical Challenges Faced during Project Execution', '– UI component creation', '– User authorization middleware creation', '– Querying data in NoSQL', 'How the Technical Challenges were Solved', '– Created and extended UI components to handle filters like owners, date fields, and area ranges on land records.', '– API and Frontend are separately built for easier team management of tasks.', '– Using a cloud-based MongoDB instance provided support for teams to work without any problems with accessibility.', 'Business Impact', '– Created a platform for clients’ business.', '– Transformed his raw data into meaningful business applications.', 'Previous article', 'Integration of video-conferencing data to the existing web app', 'Next article', 'Integration of a product to a cloud-based CRM platform', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2095,https://insights.blackcoffer.com/integration-of-video-conferencing-data-to-the-existing-web-app/,"Integration of video-conferencing data to the existing web app - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Tech Firm in the USA', 'Industry Type:', 'IT & Consulting', 'Services:', 'Software, Business Solutions, Consulting', 'Organization Size:', '200+', 'Project Description', 'Integration of 3', 'rd', 'party APIs to client’s platform.Client required meeting/conference data from sites like gotomeeting/zoom.', 'Our Solution', 'Using APIs fetched data from different platform and rendered data into client’s application.', 'Modifed web application with a UI to handle form data accepting dates as a timeframe – which then makes a request to the API being handled at server end and returns the meeting data from the required source.', 'Project Deliverables', 'Pushed code to client’s github repository.', 'Tools used', '– Python', '– Postman', 'Language/techniques used', '– Automation', '– 3', 'rd', 'party APIs', '– Authenication methods', '– Multi-Threading of function calls ( authentication of api client )', '– UI component design to get dates from user-end', 'Models used', 'Python Framework- Django , requests', 'Skills used', 'Python Programming, APIs , Multi-threading , Web Developement', 'Databases used', 'Default project postgreSQL', 'Web Cloud Servers used', 'Heroku', 'What are the technical Challenges Faced during Project Execution', '– UI creation for handling form data', '– Managing and Validating form data to process request at server end', 'How the Technical Challenges were Solved', '– Created autmated functions as views in django to handle requests made to video-conferencing platform.', '– Which then returns meeting data as per user’s wish.', 'Business Impact', '– Instead of extracting meeting data and adding it to all users', 'any authorized user can get meeting data as his wish.', 'Project website url', 'https://www.codanalytics.net/', 'Previous article', 'Design & develop an app in retool which shows the progress of the added video', 'Next article', 'A web-based dashboard for the filtered data retrieval of land records', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2096,https://insights.blackcoffer.com/design-develop-an-app-in-retool-which-shows-the-progress-of-the-added-video/,"Design & develop an app in retool which shows the progress of the added video - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Tech Firm in the USA', 'Industry Type:', 'IT & Consulting', 'Services:', 'Software, Business Solutions, Consulting', 'Organization Size:', '200+', 'Project Description', 'The objective was to develop a progress bar that can help costumes to estimate the analytics of the video.', 'Our Solution', 'The client wanted a progress bar with the following filters:', 'Date filter: – Update the progress bar and count of the videos according to the date selected', 'Category filter: – Update the progress bar and the count of the videos according to the selected category', 'We have created a SQL query for getting a count of the videos from the full video table according to the filter selected in the app', 'In added video table some columns were missing to solve this we created a SQL query for joining the added video table to the other tables and return the count of the video according to the filter selected', 'Project Deliverables', 'App in retool', 'Tools used', 'Retool', 'Language/techniques used', 'SQL', 'Skills used', 'SQL', 'Databases used', 'SQL Database', 'What are the technical Challenges Faced during Project Execution', 'Client wanted date filter and a video category filter but this data was not there in added video table', 'How the Technical Challenges were Solved', 'We had to join multiple data so that we can get category column and date column for applying filter', 'Project Snapshots', 'Project Video', 'Previous article', 'Rise of Electric Vehicles and its Impact on Livelihood by 2040', 'Next article', 'Integration of video-conferencing data to the existing web app', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'Development of EA Robot for Automated Trading', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application']"
bctech2097,https://insights.blackcoffer.com/auvik-connectwise-integration-in-grafana/,"Auvik, Connectwise integration in Grafana - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Tech Firm in the USA', 'Industry Type:', 'IT & Consulting', 'Services:', 'Software, Business Solutions, Consulting', 'Organization Size:', '200+', 'Project Objective', 'Get statistics such as uptime,\xa0 availability, cpu throughput etc. from Auvik and Connectwise and make a dashboard from it in Grafana.', 'Project Description', 'Unlike many technologies for which plugins are readily available in Grafana, there are none for auvik and Connectwise. So our task was to device a solution through which all the data from Auvik and Connectwise can be fed to Grafana. This data then would be used to plot graphs in Grafana.', 'Our Solution', 'Setup Postgres on linux', 'Create appropriate databases, tables and users in it.', 'Use python to get data from Auvik and Connectwise and perform necessary preprocesing.', 'In the same python file, Connect to our postgres database.', 'Ingest this data into postgres database.', 'Setup Grafana.', 'Connect Grafana to postgres using the postgres plugin.', 'Query our postgres database in Grafana to get desired results.', 'Plot multiple graphs according to client’s requirement and make a dashboard from it', 'Project Deliverables', 'Setup Postgres', 'Setup Postrges in Grafana', 'Write Python code to get data from Auvik and Connectwise into Postrges', 'Plot graphs into Grafana according to client’s requirement', 'Make dashboards for all the graphs', 'Tools used', 'Grafana', 'Postgres', 'Vs Code', 'AWS', 'Postman', 'Language/techniques used', 'Python', 'bash', 'Skills used', 'Python', 'networking', 'Data visualisation', 'Databases used', 'Postgres', 'Web Cloud Servers used', 'Amazon Web Services (AWS)', 'What are the technical Challenges Faced during Project Execution', 'Since, the data received from Auvik was in Json fromat, our first approach was to use Grafana’s built-in Json plugin. But this wasn’t working since, the data received from Auvik was multi-dimensional when the Json plugin required One dimensional data.', 'How the Technical Challenges were Solved', 'The above challenge was addressed by transforming the multi- dimensional data into one dimensional when it was store in a python variable. This transformed data was then inserted into Postgres.', 'Project Snapshots', 'Project website url', 'https://github.com/AjayBidyarthy/Henry-Pardo', 'Project Video', 'https://www.youtube.com/watch?v=7CcbdfjkBzc&ab_channel=Blackcoffer', 'Previous article', 'Data integration and big data performance using Elasticsearch', 'Next article', 'Portfolio: Website, Dashboard, SaaS Applications, Web Apps', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2098,https://insights.blackcoffer.com/data-integration-and-big-data-performance-using-elk-stack/,"Data integration and big data performance using Elasticsearch - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Tech Firm in the USA', 'Industry Type:', 'IT & Consulting', 'Services:', 'Software, Business Solutions, Consulting', 'Organization Size:', '200+', 'Project Objective', 'Migrate existing databases from Postgres to elastic search since Elasticserach performs better in search operations. In addition to this, all of the backend javascript also needed to be changed in order to query the new elasticsearch database.', 'Project Description', 'The client’s website was a visualization tool. It also had GUI to add filters. To make the visualizations, at least 50,000 records needed to be pulled from the Postgres database whose size would be around 200mbs. This would take a lot of time (nearly 20-30 secs). Adding filters would take additional time. So our task was to move the entire database over to Elasticsearch from postgres since it is way more faster in search operations and also filtering data. Since the database was changed, we also had to write new backend code that would now query the Elasticsearch database.', 'Our Solution', 'Setup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.', 'Write a pipeline file (.conf file) which is used to ingest data from postgres to elasticsearch. The datatypes of cloumns, unique constraints, datetime formats etc., are all defined in this file. This is executed with the help of logstash.', 'Once the data is inserted, it can be queried in the kibana’s built in query compiler. Here we can check the veracity of the data.', 'Identify the code in the backend that needs to be changed.', 'Replace this code with new code that would now query elasticserach. We use elastic_query_builder module for this.', 'Testing Postgres and Elasticsearch performance.', 'Project Deliverables', 'Setup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.', 'Pipeline i.e; logstash file', 'New working backend code for elasticsearch', 'Commands to check elastic data.', 'Customizable logstash pipeline', 'Tools used', 'Elasticsearch', 'Postman', 'Kibana', 'Logstash', 'Python', 'Javascript', 'Amazon Web Services', 'Postgres', 'Docker', 'Git Bucket', 'Github', 'Language/techniques used', 'Javascript', 'Json', 'Domain-Specific Language for elasticsearch', 'bash', 'Skills used', 'Elasticsearch query knowledge', 'Postgres query knowledge', 'Networking', 'Javascript', 'Backend web stack', 'Databases used', 'Postgres', 'Elasticsearch', 'Web Cloud Servers used', 'Amazon Web Services (AWS)', 'What are the technical Challenges Faced during Project Execution', 'Sometimes for large responses from elasticsearch ( size above 500mb), time taken was above 30 secs.', 'How the Technical Challenges were Solved', 'To solve the above mentioned problem, we used gzip in the request url’s header. This significantly reduced the execution times.', 'Business Impact', 'Earlier postgres infrastructure which took around 20-30 secs now too consistently less than 10 secs to perform filter and search operations. This would contribute to a better user experience.', 'Project Snapshots', 'Previous article', 'Web Data Connector', 'Next article', 'Auvik, Connectwise integration in Grafana', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2099,https://insights.blackcoffer.com/web-data-connector/,"Web Data Connector - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Marketing Tech Firm in Australia', 'Industry Type:', 'Marketing', 'Services:', 'Marketing Solutions', 'Organization Size:', '50+', 'Project Objective', 'To make a software code that takes data from a source and ingests it into a database present on a server. The scripts should automatically execute after regular intervals of time.', 'Project Description', 'The client had several data sources that were updated with new data regularly. The client wanted software that triggers itself automatically and takes data from those data sources and ingests it into a database that is hosted on a Linode server. Also, the date parameters in the query should be changed dynamically using the current date. Further, we had to assist in setting up the Tableau BI tool on the client’s PC and connect the Postgres database to the tableau.', 'Our Solution', 'We setup a linux server on linode.', 'Install Postgres on this linux server.', 'Create a database and create a new user. Grant this new user all privileges on the database.', 'Create a table within the database. This table has columns with datatypes as specified by the client.', 'Write a python script that makes GET request to the client specified data source and store the response in json format.', 'Inside the python script itself, establish\xa0 a connection to our postgres database using the pscopg2 module and user credentials.', 'Ingest the data into postgres using INSERT query in python script.', 'Write code to get the today’s date using the datetime module. Using this, calculate yesterday’s date. Now we can use these as parameters inside our query to the data source.', 'Move these python files to our server.', 'Install and setup Cron on our server.', 'Add the task to run specified python files at regular intervals to Cron.', 'Repeat steps 4 to 11 for every new data source.', 'Project Deliverables', 'Python Script', 'Working linode server with cron installed', 'Tableau installation and connection to postgres', 'Project Documentation', 'Tools used', 'Linode server', 'VS Code', 'Language/techniques used', 'Python', 'Bash', 'PSQL.', 'Skills used', 'Python programming', 'Postgres SQL', 'Linux scripting', 'Databases used', 'Postgres', 'Web Cloud Servers used', 'Linode', 'What are the technical Challenges Faced during Project Execution', 'Avoiding duplicates was a challenge.', 'Since Client was living in Australia all the timezone (on server and in code) were changed to AEDT.', 'How the Technical Challenges were Solved', 'Used uniqueid Column to check for duplicates.', 'Used pytz module to change timezones.', 'Business Impact', 'This solution helps in maintaining a copy of all data sources inside our Postgres database. Also, the data is 24/7 available. Since data inside the Postgres is updated regularly, graphs in the tableau are also up to date.', 'Project Snapshots', 'Project website url', 'https://github.com/X360pro/Web-connector-for-tableu', 'Previous article', 'An app for updating the email id of the user and stripe refund tool using retool', 'Next article', 'Data integration and big data performance using Elasticsearch', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2100,https://insights.blackcoffer.com/an-app-for-updating-the-email-id-of-the-user-and-stripe-refund-tool-using-retool/,"An app for updating the email id of the user and stripe refund tool using retool - Blackcoffer Insights-['Client Background', 'Client:', 'A Leading Healthcare Tech Firm in the USA', 'Industry Type:', 'Healthcare', 'Services:', 'Healthcare Solutions', 'Organization Size:', '200+', 'Project Description', 'The client needed two apps in retool', 'Update the email id of the customer', 'Stripe refund app with two options full payment and partial payment', 'Our Solution', 'We create the following two apps in retool', 'Takes the old email id of the user and new email id of the user when the update email id is clicked then the old email id is updated with the new email id. For updating email id we have used stripe API', 'The user has to select the email id of the user and payment id of the user from the table the user get two options for a refund', 'Full payment: – This option refunds the whole amount to the customer', 'Partial payment: – This option refunds the partial amount entered by the user', 'Project Deliverables', 'Apps in retool', 'Tools used', 'Retool', 'Stripe', 'Language/techniques used', 'JavaScript', 'Models used', 'We have not used any models', 'Skills used', 'API', 'Databases used', 'Stripe database', 'What are the technical Challenges Faced during Project Execution', 'The main challenge was creating a full payment option using stripe API. If the customer has already received a partial amount then while performing a full refund the refund amount was always greater than the balance amount', 'How the Technical Challenges were Solved', 'To solve the full payment option issue, we calculate the balance amount and provided that amount to the full payment event in retool', 'Business Impact', 'Using this apps it’s easy for the client to update the email id of the customer and refund the customers client can refund into two option full payment and partial payment', 'Project Snapshots', 'Project website url', 'Previous article', 'An AI ML-based web application that detects the correctness of text in a given video', 'Next article', 'Web Data Connector', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2101,https://insights.blackcoffer.com/an-ai-ml-based-web-application-that-detects-the-correctness-of-text-in-a-given-video/,"An AI ML-based web application that detects the correctness of text in a given video - Blackcoffer Insights-['Client Background', 'Client:', 'A Design & Media firm in the USA', 'Industry Type:', 'Marketing', 'Services:', 'Consulting, Software, Marketing Solutions', 'Organization Size:', '100+', 'Project Objective', 'Create a python web application that detects the text and checks the spelling of written text in the videos and prints the count of wrong spelling in the end', 'Project Description', 'Developing a dockerized Django web application for detecting the text and checking the spelling of written text in the video and printing the count of wrong spelling in the end and deploying the application on google cloud', 'Our Solution', 'We have created a python web application with Django framework when user uploads the video the application run keras-ocr model on each frame of the video and keep the count of the wrong words at the end it provides the video with the bounding box around the words. For correct words it creates green bounding box and for wrong words it creates red bounding box and also it provides the summation of count of wrong words.', 'Project Deliverables', 'Deployed dockerized web application on google cloud which generate video with bounding box around texts', 'Tools used', 'Docker', 'Redis Server', 'Django', 'Celery', 'Nginx', 'Opencv', 'NLTK', 'Moviepy', 'Language/techniques used', 'Python', 'Html', 'CSS', 'JavaScript', 'Models used', 'We have used keras-ocr model for detecting the text form the video and creating the bounding box around the words', 'Skills used', 'Natural language processing,', 'Machine learning,', 'Image processing,', 'Web development,', 'Python programming', 'Databases used', 'Django Sqlite3,', 'Redis Server', 'Web Cloud Servers used', 'Google cloud', 'What are the technical Challenges Faced during Project Execution', 'Running model on each frame of the video', 'Show progress bar for the progress of the work', 'How the Technical Challenges were Solved', 'For running the model on each frame of the video we have used celery it runs the model in the backend of the application', 'We have used celery backend progressrecorder and updated it every time when model has detected the text from the frame of the video', 'Project Snapshots', 'Project website url', 'http://34.68.134.64/', 'Previous article', 'Website Tracking and Insights using Google Analytics, & Google Tag Manager', 'Next article', 'An app for updating the email id of the user and stripe refund tool using retool', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2102,https://insights.blackcoffer.com/website-tracking-and-insights-using-google-analytics-google-tag-manager/,"Website Tracking and Insights using Google Analytics, & Google Tag Manager - Blackcoffer Insights-['Client Background', 'Client:', 'A leading marketing firm in the USA', 'Industry Type:', 'Marketing', 'Services:', 'Consulting, Software, Marketing Solutions', 'Organization Size:', '400+', 'Project Objective', 'The project objectives are as follows:', 'Assisting the businesses with the setup for Google Analytics, Google Tag Manager which helps them in tracking the analytics of the website.', 'Setup pixels of Social Media platforms like LinkedIn and Facebook which assists users in tracking conversions.', 'Providing monthly insights on their website performance to analyse the businesses’ strengths and opportunities for growth.', 'Project Description', 'This project includes assisting businesses with digital analysis for their marketing.Digital analytics allows you to stand back, get the big picture, and see what is working and what isn’t in your overall strategy so you can adjust. The importance of digital analytics is that it allows for a data-driven approach to marketing, and as such it can produce better results.', 'The primary objective of the project is to help the businesses in knowing their target audience, understanding the trends in digital marketing, and providing insights on the analytics part of their website performance. Use the digital analytical data to determine if your business’ aims are in line with the customer’s wants and needs. As the picture of the customer’s needs unfolds, adjust the objectives accordingly.', 'Our Solution', 'The main aim of this project is to assist the businesses to improve their website performance with the use of technologies like Google Analytics, Google Tag Manager and dashboards built on Whatagraph.', 'Google Analytics:', 'Google Analytics is integral to tracking and measuring data from a number of digital platforms, but especially web metrics and customer behaviour. For example, through Google Analytics, you can see when people drop out of the buying process, perhaps they abandon while on the cart page, which would then inform your decisions on how to improve the check-out process.', 'Because Google Analytics measures traffic from a variety of devices and sources and integrates with other online platforms, such as Google Ads, it is a handy tool to get an overview of your business’s digital analytics.', 'Google Tag Manager:', 'Google Tag Manager is a tag management system (TMS) that allows you to quickly and easily update measurement codes and related code fragments collectively known as tags on your website or mobile app. Once the small segment of Tag Manager code has been added to your project, you can safely and easily deploy analytics and measurement tag configurations from a web-based user interface.', 'When Tag Manager is installed, your website or app will be able to communicate with the Tag Manager servers. You can then use Tag Manager’s web-based user interface to set up tags, establish triggers that cause your tag to fire when certain events occur, and create variables that can be used to simplify and automate your tag configurations.A Tag Manager container can replace all other manually-coded tags on a site or app, including tags from Google Ads, Google Analytics, Floodlight, and 3rd party tags.', 'Whatagraph Dashboards:', 'The whatgraph dashboards previews the important metrics related to the website including conversions, events, number of users and performance about ads and campaigns by the website. This dashboard helps in drawing some of the useful insights for the website notifying the strengths,gains and areas of improvement.', 'Project Deliverables', 'Main deliverables for the project are:', 'Setup the Google Analytics and Google Tag Manager for the website.', 'Tracking events on Google Analytics using Tags created in Google Tag Manager.', 'Monthly Reporting of Analytics for businesses on Whatagraph dashboards or via presentations.', 'LinkedIn and Facebook Pixel setup and validation for the website.', 'Setup Goal Conversions for the website to track the important and valuable metrics from the website.', 'Tools used', 'Google Analytics: To track events, goal conversions and analyse the traffic sources/medium, the top viewed pages and the top cities and countries.', 'Google Tag Manager: To set up the tags and triggers of button clicks, page visits as events in Google Analytics.', 'Whatagraph: To visually represent important metrics like impressions, clicks, goal completions and many more related to Ads management and Google Analytics.', 'Clickup: This tool is used to manage tasks given.', 'Skills used', 'Digital Analysis', 'Data Analysis', 'Digital Marketing', 'Google Analytics', 'What are the technical Challenges Faced during Project Execution', 'The main technical challenge faced was that any changes in Google Analytics are operational after 24 hrs. Thus, we can’t judge if the setup works as per required.', 'How the Technical Challenges were Solved', 'We had to wait for 24 hours to check the setup. We could use real-time report as well to check the setup on-the spot.', 'Business Impact', 'This analysis helps to improve website performance, understanding user behavior, understanding the impact of business campaigns and improvising the UI/UX to increase their potential users.', 'Having insight into your clients’ behaviour and demographics can help you make decisions about serving them the right products at the right time for maximum chances of a sale. Such data could include a client’s persona, such as their age, location, and areas of interest.', 'Some of the common metrics that are important in digital analytics include:', 'Dashboard metrics', ':', 'Some examples are pages per visit, bounce rate, and average duration of each visit.', 'Most exited pages:', 'Pages with an exit rate of 75–100% show that you need to examine the problem with the content and improve upon it.', 'Most visited pages:', 'These pages will make the customers either exit or explore the website further.', 'Referring websites:', 'These are other websites that link to your website.', 'Conversion rate:', 'This indicates whether the goal of your website was achieved, be it a sale of a product, a free giveaway, or a subscription to a newsletter.', 'Frequency of visitors:', 'This tells you about the loyalty of the customers.', 'Days to the last transaction:', 'This refers to the time lapse between the first visit and the sale. The shorter the time taken, the better it is for your business.', 'Project Snapshots', 'Figure 1: Google Tag Manager Domains', 'Figure 2: Google Tags', 'Figure\xa0 3: Google Analytics', 'Figure 4: Google Analytics', 'Figure 5: Tracking Facebook Pixels for a website', 'Figure 6: Whatagraph dashboard', 'Figure 7: Whatagraph Dashboard(Conversions)', 'Project website url', 'https://unite.ca/', 'https://livelike.com/', 'http://essencelle.ca/', 'https://www.decorium.com/', 'https://www.everafterfest.com/2022-tickets/', 'https://winagetaway.com/', 'Previous article', 'Dashboard to track the analytics of the website using Google Analytics and Google Tag Manager', 'Next article', 'An AI ML-based web application that detects the correctness of text in a given video', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2103,https://insights.blackcoffer.com/dashboard-to-track-the-analytics-of-the-website-using-google-analytics-and-google-tag-manager/,"Dashboard to track the analytics of the website using Google Analytics and Google Tag Manager - Blackcoffer Insights-['Client Background', 'Client:', 'A Automobile firm in India', 'Industry Type:', 'Automobile', 'Services:', 'Retail, Automobile', 'Organization Size:', '1000+', 'Project Objective', 'The project objectives are as follows:', 'Assisting the client with the setup for Google Analytics, Google Tag Manager which helps them in tracking the analytics of the website.', 'Dashboards on website analysis presenting the important metrics and analysis related to websites.', 'Project Description', 'This project includes assisting the client to study the user flow and behaviour flow of the users on the websites. It had one main website and three other sub websites to analyse the button clicks, impressions and understanding the user’s behaviour on the website. Many events were to be tracked and converted to a dashboard in Google Data Studio to make it simpler to understand.', 'This project was created to give this data in a way that companies can readily understand through the use of visualisations. The graphs will show the increase/decrease in any of the metrics, as well as the manner in which the increase/decrease occurs. It will display all of the crucial data monthly or even by date range to help you keep track of the changes that occur.', 'Our Solution', 'The main aim of this project is to display the event flow, user flow and behaviour flow through dashboards and analyse them to work on the areas of improvements.', 'Google Analytics:', 'Google Analytics is integral to tracking and measuring data from a number of digital platforms, but especially web metrics and customer behaviour. For example, through Google Analytics, you can see when people drop out of the buying process, perhaps they abandon while on the cart page, which would then inform your decisions on how to improve the check-out process.', 'Because Google Analytics measures traffic from a variety of devices and sources and integrates with other online platforms, such as Google Ads, it is a handy tool to get an overview of your business’s digital analytics.', 'Google Tag Manager:', 'Google Tag Manager is a tag management system (TMS) that allows you to quickly and easily update measurement codes and related code fragments collectively known as tags on your website or mobile app. Once the small segment of Tag Manager code has been added to your project, you can safely and easily deploy analytics and measurement tag configurations from a web-based user interface.', 'When Tag Manager is installed, your website or app will be able to communicate with the Tag Manager servers. You can then use Tag Manager’s web-based user interface to set up tags, establish triggers that cause your tag to fire when certain events occur, and create variables that can be used to simplify and automate your tag configurations.A Tag Manager container can replace all other manually-coded tags on a site or app, including tags from Google Ads, Google Analytics, Floodlight, and 3rd party tags.', 'Google Data Studio Dashboards:', 'The dashboards preview the important metrics related to the websites using graphs, tables to understand the trends, patterns in the users.', 'The following steps were carried out for the project:', 'Get the important metrics for website performance like the number of users visiting the websites, the average session duration, graphs related to the user acquisition like number of new users vs the returning users. This is related to the main website.', 'For the sub websites, track the number of users clicking on specific buttons. Through this I understand the user flow. Compare between the number of users entering the website and those clicking on buttons.', 'Track the metrics related to goal conversion like goal completions, goal conversion rate, goal completion rate and different goals and present it using visualisations.', 'Provide data insights in the end providing scope of improvements and recommendations.', 'Project Deliverables', 'The main deliverable for this project were dashboards on Google Data Studio depicting important metrics related to website performance. There were three sub websites for which there were two types of views each. Each of the views had several buttons related to the product. The project was about finding the user flow and event flow on the views.', 'Tools used', 'Google Analytics: To track events, goal conversions and analyse the traffic sources/medium, the top viewed pages and the top cities and countries.', 'Google Tag Manager: To set up the tags and triggers of button clicks, page visits as events in Google Analytics.', 'Google Data Studio: To visually represent important metrics like impressions, clicks, goal completions using Google Analytics.', 'Skills used', 'Digital Analysis', 'Data Analysis', 'Data Visualisations', 'Google Analytics', 'What are the technical Challenges Faced during Project Execution', 'The main technical challenge faced was that there were multiple events setup in Google Analytics for one event and thus identifying a particular one was difficult.', 'How the Technical Challenges were Solved', 'We had to communicate with the client to clarify about the event names. Although this took some time but it was necessary since accurateness of data is very essential for the project.', 'Business Impact', 'This analysis helps to improve website performance, understanding user behavior, understanding the impact of business campaigns and improvising the UI/UX to increase their potential users.', 'Having insight into your clients’ behaviour and demographics can help you make decisions about serving them the right products at the right time for maximum chances of a sale. Such data could include a client’s persona, such as their age, location, and areas of interest.', 'Some of the common metrics that are important in digital analytics include:', 'Dashboard metrics', ':', 'Some examples are pages per visit, bounce rate, and average duration of each visit.', 'Conversion rate:', 'This indicates whether the goal of your website was achieved, be it a sale of a product, a free giveaway, or a subscription to a newsletter.', 'Source/Medium Analysis:', 'This analysis helps in understanding the traffic sources and medium on the website. This helps the businesses to work on strengthening the traffic sources to get better reach to the target audience.', 'Traffic Analysis:', 'The overall traffic analysis for the website provides information regarding the important metrics like users,avg. session duration and goal completions according to different source/medium. This will help the\xa0 business to analyse different traffic channels performances.', 'Project Snapshots', 'Figure 1: Tracking of Buttons for Triber Virtual Studio', 'Figure 2: Triber Goal Conversions', 'Figure 3: Kiger 360 Experience Website Tracking', 'Figure 4: Traffic Medium Analysis', 'Figure 5: Overview of Dashboard Metrics', 'Figure 6: Kiger Studio Experience Website', 'Project website url', 'Website URL:', 'https://www.renault.co.in/', 'Dashboard URL:', 'Previous article', 'Power BI Dashboard on Operations, Transactions, and Marketing Data, embedding the Dashboard to Web App', 'Next article', 'Website Tracking and Insights using Google Analytics, & Google Tag Manager', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2104,https://insights.blackcoffer.com/power-bi-dashboard-on-operations-transactions-and-marketing-embedding-the-dashboard-to-web-app/,"Power BI Dashboard on Operations, Transactions, and Marketing, Anaytics-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT Services', 'Services:', 'Consulting, Software, Marketing Solutions', 'Organization Size:', '100+', 'Project Objective', 'Create a dashboard with Assets Performance With react App. So users can evaluate with Key metrics from data analytics and forecasting.', 'Project Description', 'The client requires two pages:', 'Screening Asset Performance', 'Portfolio Investing', 'according to criteria and sector-based.', 'Our Solution', 'By using Power BI We can achieve this requirement without any additional stack. It requires a subscription to enhance the report.', 'Using Page Navigation and bookmarks to create reports like Web Application with React App.', 'Project Deliverables', 'Asset Report Page', 'Investor Page', 'Tools used', 'Power BI', 'Azure AAD', 'Mongo DB BI Connector', 'ODBC Connector', 'DAX Studio', 'Language/techniques used', 'STAR SCHEMA', 'Skills used', 'DATA MODELLING.', 'Performance Analyser.', 'Vertipaq Analyser.', 'Databases used', 'Mongo DB', 'Web Cloud Servers used', 'AZURE', 'What are the technical Challenges Faced during Project Execution', 'Time for loading pages is increased due to raw data.', 'Cold start of Report taking more time than usual', 'How the Technical Challenges were Solved', 'From Snowflake to Star Schema\xa0 achieved performance of Report', 'By using Performance Analyser debugging resolved many glitches and where it is happening.', 'Extraction, Transformation makes data less complex and removing unwanted data from a website perspective makes data shrink and achieved 75% of Data Reduction.', 'Business Impact', 'Less coding with Power BI speeds the development process and achieves Best UX with less time.', 'Project Snapshots', 'Project website url', 'https://digital.bctriangle.com', 'Project Video', 'Previous article', 'NFT Data Automation (looksrare), and ETL tool', 'Next article', 'Dashboard to track the analytics of the website using Google Analytics and Google Tag Manager', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2105,https://insights.blackcoffer.com/nft-data-automation-looksrare-and-etl-tool/,"NFT Data Automation (looksrare), and ETL tool - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT Services', 'Services:', 'Blockchain, NFT', 'Organization Size:', '10+', 'Project Objective', 'To scrape all the desired information regarding the NFTs from a website and store them in a database to be accessed later on.', 'Project Description', 'Matthew Brown – extract all events, all time from this', 'https://looksrare.org/explore/activity', '. We can then pay you weekly to keep them up to date. You can choose any technology you like, as long as it’s updated into an SQL database. Additional tasks may be to make an alert or dashboard from data, later access API when it becomes available.', 'Our Solution', 'We provided a robust solution which returned the NFT data every 8 hours into the google big query database. To do this we used selenium web driver to scrape all events as the website was dynamic and did not have a format data structure to scrape data using AJAX POST calls. After automating the scarper the data was manipulated and constructed into a desired format into pandas dataframe, which was later used to push the dataframe into the google big query database using Google cloud api and credentials. The data was getting collected every day and about 50M distinct rows were created.', 'Project Deliverables', 'Webcrawler and database', 'Tools used', 'Python', 'Selenium', 'GBQ', 'Language/techniques used', 'Python', 'Selenium web scraper', 'Pandas', 'Google big query', 'Parallel processing.', 'Databases used', 'SQL', 'Google BigQuery', 'Web Cloud Servers used', 'Google BigQuery', 'What are the technical Challenges Faced during Project Execution', 'The only technical challenge faced during this project was that the website used to keep changing the elements on their webpage and used to cause error. Though it did not use to happen regularly, it happened 3 times in 5 weeks. Also AJAX calls were not proper.', 'How the Technical Challenges were Solved', 'Identifying the elements solved the issue. Also remote access to a better desktop enabled me to keep working as well as keep the code running all the time.', 'Business Impact', 'Supplied upto 50 million rows data regarding NFTs.', 'Provided a python solution with optimal functions and code to be used and automate them to save the data into a database on a daily basis.', 'Caused a huge influx of data which can be used to make many insightful decisions regarding the nft market.', 'Project Snapshots', 'Project website url', 'https://looksrare.org/explore/activity', 'Previous article', 'Optimize the data scraper program to easily accommodate large files and solve OOM errors', 'Next article', 'Power BI Dashboard on Operations, Transactions, and Marketing Data, embedding the Dashboard to Web App', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2106,https://insights.blackcoffer.com/optimize-the-data-scraper-program-to-easily-accommodate-large-files-and-solve-oom-errors/,"Optimize the data scraper program to easily accommodate large files and solve OOM errors - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in India', 'Industry Type:', 'IT Services', 'Services:', 'SAAS services, Marketing services, Business consultant', 'Organization Size:', '100+', 'Project Description', 'Building a large data warehouse that houses projects and tenders data from all over the world that is to be collected from official government websites, multilateral banks, state and local government agencies, data aggregating websites, etc.', 'Our Solution', 'We had tried multiple solutions to prevent the program from running out of memory. We used python pandas techniques to control the use of memory which worked for some files and did not work for others. Provided more solutions using vaex ,dask module and datatables.', 'Project Deliverables', 'Desired changes to the code and committing them to github.', 'Tools used', 'Vscode', 'Python', 'Github', 'Slack', 'Language/techniques used', 'Chunking', 'dask Dataframe', 'vaex', 'datatable', 'python.', 'Skills used', 'Cloud', 'Python', 'Time complexity', 'What are the technical Challenges Faced during Project Execution', 'System specs requirement was the main issue during this project because the RAM available was too less and got used up quickly.', 'How the Technical Challenges were Solved', 'Team viewer to use remote desktop which had higher specs would be sufficient enough to solve the problem.', 'Business Impact', 'Provided various techniques to solve memory issues.', 'Suggested parallel programming to decrease the execution time by 12% making getting the tender data at a much faster rate.', 'Project Snapshots', 'Project website url', 'https://github.com/Taiyo-ai/opentenders-eu', 'https://opentender.eu', 'Previous article', 'Making a robust way to sync data from airtables to mongoDB using python – ETL Solution', 'Next article', 'NFT Data Automation (looksrare), and ETL tool', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2107,https://insights.blackcoffer.com/making-a-robust-way-to-sync-data-from-airtables-to-mongodb-using-python-etl-solution/,"Making a robust way to sync data from airtables to mongoDB using python - ETL Solution - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT Services', 'Services:', 'SAAS services, Marketing services, Business consultant', 'Organization Size:', '100+', 'Project Description', 'Equilo is a social impact start-up focused on gender equality and social inclusion. We need to link data in Airtable (1 million+ records spread across 20+ bases) to MongoDB (v3.x.x).', 'Most of the data is backend data for our app, in which case the flow is only AT to MDB.', 'Need to create a code that can calculate a scores by pulling from indicators in many different bases and putting result in new database.', 'Our Solution', 'Used Python and MongoDB module along with Airtable API to fetch all the data from airtables and push them to the database. Stayed in touch with the client through slack and asana completing daily tasks and applying a cronjob for the program to run on a scheduled time.', 'Project Deliverables', 'Python code for sync into their staging server and then to production.', 'Tools used', 'VScode', 'MongoDB', 'Airtable API', 'Slack', 'Asana', 'Github', 'Language/techniques used', 'Python', 'MongoDb', 'SQL', 'Skills used', 'Data extraction', 'Data handling', 'Data storage', 'Computational data queries', 'Databases used', 'Airtables', 'MongoDB', 'Web Cloud Servers used', 'Airtable', 'What are the technical Challenges Faced during Project Execution', 'Main challenge faced was regarding the new concept of Airtables and syncing up the data into mongodb in a very complex schema as proposed by the client. Dissimilar columns in mongoDB and Airtables for 100s of tables took lot of time.', 'Also insufficient information provided by client while coding and the previous versions codes that had been written only to discover them on a later stage caused a lot of problem.', 'Not proper code management which could help next coders like me to complete the remaining stuff quickly.', 'How the Technical Challenges were Solved', 'These issues were solved by lot of self study and evaluation and then asking the exact question to client which they would then answer. For eg: whereabouts of the previous codes and people who run that code.', 'Business Impact', 'Helped them immensely making their backend to frontend integration seamless.', 'Sped up their product development by 20% to calculate various different scores and visualize them on the frontend.', 'Project Snapshots', 'Project website url', 'https://www.equilo.io/', 'Previous article', 'Google Local Service Ads LSA API To Google BigQuery to Google Data Studio', 'Next article', 'Optimize the data scraper program to easily accommodate large files and solve OOM errors', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2108,https://insights.blackcoffer.com/incident-duration-prediction-infrastructure-and-real-estate/,"Incident Duration Prediction - Infrastructure and Real Estate - Blackcoffer Insights-['Client Background', 'Client:', 'A leading research institution in the middle east', 'Industry Type:', 'Research', 'Services:', 'R&D', 'Organization Size:', '1000+', 'Project Objective', 'To complete a Research Paper draft by training various Machine Learning models which can predict the Incident Duration based on various parameters given in the dataset and summarising the results.', 'Project Description', 'Given a set of researches, need to analyse and compare various machine learning and deep learning models to predict the Incident Duration for the given dataset. The dataset contained Short durations as well as Long durations. Build models for each set of durations, compare and get the best out of all.', 'Our Solution', 'Here, we had to predict the traffic incident duration with some machine learning tools and techniques i.e. XGBoost, SVR and Deep Learning algorithm using tensor flow. First two models were run on Python Interpreter whereas Deep learning model was run on R studio, all the three with the same dataset and then we had compared these models based on their MAE (mean absolute error). Initially, we had done a preliminary analysis of the collected incident duration data, to collect the statistical characteristics of all the variables used in our research.', 'Project Deliverables', 'Python Script for each model.', 'Documentation for Research Work.', 'Tools used', 'Python Interpreter', 'Language/techniques used', 'Language Used: Python', 'Libraries Used: pandas, sklearn, numpy, keras, pickle', 'Models used', 'XGBRegressor', 'SVR', 'SGDRegressor', 'Sequential', 'DecisionTreeRegressor', 'Skills used', 'Programming, Statistical Analysis', 'Project Snapshots', 'Previous article', 'Statistical Data Analysis of Reinforced Concrete', 'Next article', 'How does Metaverse work in the Financial sector?', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2109,https://insights.blackcoffer.com/statistical-data-analysis-of-reinforced-concrete/,"Statistical Data Analysis of Reinforced Concrete - Blackcoffer Insights-['Client Background', 'Client:', 'A leading research institution in the middle east', 'Industry Type:', 'Research', 'Services:', 'R&D', 'Organization Size:', '1000+', 'Project Objective', 'Conducting statistical data analysis on the data provided for different types of reinforced concrete (using 3 different fibers – Steel, Date Palm and Polypropylene fibers) and also helping in preparing good research paper based on laboratory data.', 'Project Description', 'The project had two phase:', 'Phase 1:', 'In this phase, we had to do a comprehensive\xa0analysis on the data given and finally build statistical models for the variables present. The main motive was to understand the behaviour of concrete based on various parameters – Compressive strength, Flexural strength, water absorption capabilities of the concrete and many more. The analysis should include, but was not limited to:', 'Comparison of Mo (control mix) with all mixes at 28 days for each parameter test', 'Comparing all parameters for all specimens (all concrete mixes) with 28 days and also 6 months heat-cool and wet-dry', 'all other expected analysis we could see you and do', 'Phase 2:', 'In this phase, we had to develop a structure for the research paper based on the results and analysis. The paper included sections – Abstract, Introduction ( literature, background and objective), Experimental program ( materials and methods), Results and discussion ( analysis and interpretation) and Conclusion ( summary, insights and remarks).', 'Our Solution', 'Providing a Comprehensive analysis for the concrete data – showcasing the key insights from it based on the parameters (compressive strength, etc). On the basis of results from the analysis, research paper was drafted which included all the deliverable.', 'Project Deliverable', 'A manuscript (drafted article) with the following:', 'Abstract', 'Introduction ( literature, background and objective)', 'Experimental program ( materials and methods)', 'Results and discussion ( analysis and interpretation)', 'Conclusion ( summary, insights and remarks)', 'References', 'Tools used', 'Tools used:', 'Jupyter – Notbebook (Python)', 'Numpy', 'Pandas', 'Sklearn', 'Matplotlib', 'Seaborn', 'MS Excel', 'Google spreadsheets', 'Language/techniques used', 'Python', 'Statistical Modelling', 'Statistical Inference', 'Models used', 'Statistical models – linear, polynomial, exponential and logarithmic models build for showcasing behavior of concrete mixes due to mixing of different fiber content and its effect on different parameters specified above.', 'Skills used', 'Coding – Python', 'Performing statistical analysis – extracting inferences', 'Building statistical models – through python or through Excel and its counterparts.', 'Databases used', 'No database was used.', 'Web Cloud Servers used', 'No Cloud server was used.', 'What are the technical Challenges Faced during Project Execution', 'The Challenges faced during project execution are:', 'Getting statistical models from seaborn libraries, there is no direct way to get the models from the graphs created from data.', 'Building models in excel and validating it (didn’t know how, had to learn it before applying it).', 'How the Technical Challenges were Solved', 'I had to use different libraries for building the models, later on turned to MS excel and spreadsheet because they were building models and were also able to showcase it on the data itself. For this, I learned how to build models on the aforementioned software through YouTube and blogs.', 'Project Snapshots', 'Project Video', 'Previous article', 'Database Normalization & Segmentation with Google Data Studio Dashboard Insights', 'Next article', 'Incident Duration Prediction – Infrastructure and Real Estate', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2110,https://insights.blackcoffer.com/database-normalization-segmentation-with-google-data-studio-dashboard-insights/,"Database Normalization & Segmentation with Google Data Studio Dashboard Insights - Blackcoffer Insights-['Client Background', 'Client:', 'A leading marketing firm in the USA', 'Industry Type:', 'Market Research', 'Services:', 'Marketing, Consultancy', 'Organization Size:', '60+', 'Project Objective', 'To combine the different datasets.', 'To make dashboards for each and every dataset individually.', 'Project Description', 'Phase – 1: In this project first of all we have to combine different datasets individually to make single file for each source.', 'Phase – 2: Make Good looking reports for each file individually.', 'Our Solution', 'We used pandas dataframe to combine different files to make single file for each source. We used Google Data Studio to make good looking and better reports with good UI.', 'Project Deliverables', 'We have provided a Google Data Studio report file as deliverable for the project.', 'Tools used', 'Python, Google Data Studio, Google Chrome', 'Language/techniques used', 'Python Programming and SQL queries editor.', 'Models used', 'SDLC model used in this project. We have used the SDLC model as analysis, design, implementation, testing and maintenance.', 'Skills used', 'Data cleaning, Data Pre-processing, Data Visualisation are used in this project.', 'Databases used', 'We have used the traditional file systems as database storage.', 'What are the technical Challenges Faced during Project Execution', 'Combining Data sets into single file.', 'Making good looking UI dashboards.', 'How the Technical Challenges were Solved', 'I used pandas dataframe to combine different datasets and made a single file of every individual source. I used Google Data Studio to make dashboard for the project.', 'Project Snapshots', 'Project Video', 'Previous article', 'Power BI dashboard to drive insights from complex data to generate business insights', 'Next article', 'Statistical Data Analysis of Reinforced Concrete', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2111,https://insights.blackcoffer.com/power-bi-dashboard-to-drive-insights-from-complex-data-to-generate-business-insights/,"Power BI dashboard to drive insights from complex data to generate business insights - Blackcoffer Insights-['Client Background', 'Client:', 'A leading marketing firm in the USA', 'Industry Type:', 'Market Research', 'Services:', 'Marketing, Consultancy', 'Organization Size:', '100+', 'Project Description', 'Phase – 1: In this project first of all we have made heatmap between two columns named Author and Data Source. Then after two combining two tables named NY_data and nodeid_views made the report of all of the data.', 'Phase – 2: Success of story was given by if pageviews is more than 35000, if pageviews lies between 3500-35000 the story was labelled as needs improvement and if it was below 3500 the story was labelled as failure.', 'Phase – 3: The powerbi report was made to find different insights in the data like different tables were drawn between different attributes of data like pie chart, time series chart, comparison charts. The data is updated every week and the report is generated automatically.', 'Our Solution', 'We provided them Phase 1 in the powerbi sql editor by combining two tables using sql queries. For phase 2 we just used the power bi program tool and written a script in Python to calculate the success of story. For Phase 3 we used the internal features of Power BI to find insights of the data.', 'Project Deliverables', 'We have provided a PowerBI report file as deliverable for the project.', 'Tools used', 'Python, PowerBI, Google Chrome', 'Language/techniques used', 'Python Programming and SQL queries editor.', 'Models used', 'Waterfall model used in this project.', 'Skills used', 'Data cleaning, Data Pre-processing, Data Visualisation are used in this project.', 'Databases used', 'We have used the traditional file systems as database storage.', 'What are the technical Challenges Faced during Project Execution', 'Drawing heatmap in the PowerBI.', 'Combining two tables on the basis of the pageviews.', 'Converting the time series to data to 5 minute format.', 'How the Technical Challenges were Solved', 'We installed a new add on in the PowerBI to draw heatmap for the project and used the SQL editor to combine the tables on the basis of page views. We used python programming to convert the time series data to 5 minute time gap format.', 'Project Snapshots', 'Project Video', 'Previous article', 'Real-time dashboard to monitor infrastructure activity and Machines', 'Next article', 'Database Normalization & Segmentation with Google Data Studio Dashboard Insights', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2112,https://insights.blackcoffer.com/real-time-dashboard-to-monitor-infrastructure-activity-and-machines/,"Real-time dashboard to monitor infrastructure activity and Machines - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in Europe', 'Industry Type:', 'IT', 'Services:', 'Software Services', 'Organization Size:', '30+', 'Project Objective', 'For the current project, we hope to develop a real-time dashboard (* it updates every several minutes). Currently, we have multiple Ubuntu machines that are sending messages every minute to Apache Pulsar.', 'Project Description', 'Developing a realtime updating dashboard to display the metadata of various machines on a server from pandio queue.', 'The dahboard must display the count of “inactive” , “active” and “down” servers with a table displaying the details of all the machines in different color scheme for each type of server/machine.', 'Our Solution', 'We used Django framework to develop the dashboard as it didn’t require the ec2 instance to be active on machine which was the problem with using streamlit.', 'For communication between webpage and fetched data we used django channel .', 'We used django background task module to make the fetching run forever in background.', 'Project Deliverables', 'Real time updating Dashboard with separate color scheme for different types of machines.', 'Storing the historical data in sqlite3 db.', 'Tools used', 'Django', 'Web Channels', 'D3 js', 'Reddis server', 'Skills used', 'Python', 'Django Framework', 'Django web channels', 'HTML/CSS + JS', 'Databases used', 'Django sqlite3 database.', 'Web Cloud Servers used', 'AWS', 'What are the technical Challenges Faced during Project Execution', 'Making the dashboard run forever using streamlit', 'Data updation in realtime when using django channels', 'How the Technical Challenges were Solved', 'Switched the entire dashboard to django framework', 'We redirected data to channels on local reddis server.', 'Project Snapshots', 'Project website url', 'Development hosted URL', 'Previous article', 'Electric Vehicles (EV) Load Management System to Forecast Energy Demand', 'Next article', 'Power BI dashboard to drive insights from complex data to generate business insights', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2113,https://insights.blackcoffer.com/electric-vehicles-ev-load-management-system-to-forecast-energy-demand/,"Electric Vehicles (EV) Load Management System to Forecast Energy Demand - Blackcoffer Insights-['Client Background', 'Client:', 'A leading energy consulting firm in the USA', 'Industry Type:', 'Energy', 'Services:', 'Energy solutions, Consultancy', 'Organization Size:', '100+', 'Project Objective', 'Create a Machine learning solution to manage electricity for electric vehicles.', 'Main Tasks:', 'Percentage probability of\xa0 user plugin his vehicle today by user’s plugin date history', 'Reduce the probability of plugin time according to user’s plugin time history', 'Project Description', 'We need to calculate the date and time probability that the user will plugin his vehicle today based on his plugin date and plugin time history. We also need to decrease time probability based on the user’s past time range.', 'Our Solution', 'We converted the user’s plugin data into binary values like 0 if the user hasn’t plugged-in his vehicle on that day and 1 if he plugged-in. We identified the driven distance based on the amount of charge used between two plug-in times. Then we trained the Ridge Regression ML model for identifying each day driven kilometer. From these kilometres we have identified the probability that user’s will plug-in today and it will increase day by day till the user does not plug-in his vehicle.', 'For time probability we have used Probability Distribution Function (PDF) and Cumulative Distribution Function\xa0 (CDF). These functions will decrease probability according to the user’s time range.', 'Project Deliverables', '2 python scripts to:', 'Train regression model every day.', 'Use model weights to generate probability values.', 'Tools used', 'Google Colab, VS Code, Google Drive, and MS Excel.', 'Language/techniques used', 'Python programming language, Data Analytics with numpy and pandas, Data Visualization with matplotlib, Statistics and Mathematics, Machine learning with SKlearn.', 'Models used', 'Ridge Regression Model', 'Skills used', 'Data Analytics, Data Visualization, Machine learning, Python, Statistics', 'Databases used', 'local data from MS Excel Sheet', 'What are the technical Challenges Faced during Project Execution', 'There are a lot of challenges faced during project execution', 'At the start, we have only imaginary data so need to convert in a good format to apply machine learning models.', 'Find the best machine learning model for the data.', 'Decrease the time probability according to user’s time range', 'How the Technical Challenges were Solved', 'We have converted the data into weekday’s binary values like marked 0 if not plugged-in vehicle on that day and 1 if plugged and calculated driven distance by amount of charge used between two plugin dates.', 'Tried different regression based machine learning models like Random Forest Regressor, XGBoost Regressor, Ridge Regression and checked accuracies of all models and choosed best one.', 'For decreasing time probability we used Probability Distribution Function (PDF) and Cumulative Distribution Function (CDF). These functions decrease probability according to the user’s time range.', 'Project Snapshots', 'Previous article', 'Power BI Data-Driven Map Dashboard', 'Next article', 'Real-time dashboard to monitor infrastructure activity and Machines', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2114,https://insights.blackcoffer.com/power-bi-data-driven-map-dashboard/,"Power BI Data-Driven Map Dashboard - Blackcoffer Insights-['Client Background', 'Client:', 'A leading marketing firm in the USA', 'Industry Type:', 'Market Research', 'Services:', 'Marketing, Consultancy', 'Organization Size:', '60+', 'Project Objective', 'Change bubble colors dynamically.', 'Make table and charts linked. If a user clicks on tables values, then the bubble chart on the map should be highlighted that relates to the table.', 'Project Description', '“I have a map visual. I would like to dynamically change the colours of some of the bubbles.”', 'The report page has several filters and KPI Dashboard, whose metrics change dynamically when the user clicks a certain element. Similarly the map should also change dynamically relative to the filter.', 'Our Solution', 'Added the website data from Details table to the map visualization, it makes the bubbles get coloured dynamically according to the requirement for websites data.', 'Project Deliverables', 'The Power BI ( .pbix ) file updated with solution', 'Tools used', 'Power BI', 'Skills used', 'Power BI', 'Data Visualization', 'Data Analysis', 'Databases used', 'The database that came in with the Power BI file received from client', 'What are the technical Challenges Faced during Project Execution', 'The map was not linked', 'Map Bubbles were not dynamic', 'How the Technical Challenges were Solved', 'Refactoring the data model and using appropriate keys to link the data together', 'That made Map to change according to Slicers/Filters', 'To Change the colour, Bookmark buttons were used in the dashboard to bring up the dynamic colour changing with slicing (works after being published)', 'Project Snapshots', 'Project Video', 'Previous article', 'AI Conversational Bot using RASA', 'Next article', 'Electric Vehicles (EV) Load Management System to Forecast Energy Demand', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2115,https://insights.blackcoffer.com/google-local-service-ads-lsa-leads-dashboard/,"Google Local Service Ads (LSA) Leads Dashboard - Blackcoffer Insights-['Client Background', 'Client:', 'A leading law firm in USA', 'Industry Type:', 'Law', 'Services:', 'Law practice', 'Organization Size:', '40+', 'Project Objective:', 'For a better understanding, provide visualisations of the data on the LSA Dashboard.', 'Learn how to enhance Rank and push the Ad to potential consumers by gaining data insights.', 'Project Description', 'Local Service Ads is a newer program by Google that allows advertisers to achieve a “Google Guaranteed” status in search engines when a visitor makes a search. Advertisers who participate in Google Local Service Ads will receive a larger ad space with their competitor’s local services ads and they will be able to feature their local businesses throughout organic search queries.', 'There are various aspects that firms must concentrate on in order to win the Google services ad and so raise their ranking. These enhancements may be implemented if companies obtain current data about their leads and analyse it in order to take appropriate actions in the future.', 'This project was created to give this data in a way that companies can readily understand through the use of visualisations. The graphs will show the increase/decrease in any of the metrics, as well as the manner in which the increase/decrease occurs. It will display all of the crucial data monthly or even by date range to help you keep track of the changes that occur.', 'Our Solution', 'The solution for the project includes data insights through visualisations which will help businesses to better analyse the available data. This solution will help the businesses in improvising the factors to increase their potential customers and raise their respective ranks.', 'It is divided into two parts: databases and data dashboard. The databases will store the important data retrieved from the LSA dashboard and use them to calculate some important metrics. The data dashboard will represent those metrics in form of graphs and data in form of tables.', 'Project Deliverables', 'The project deliverables can be divided into two parts:', 'Data in databases: The data is divided into three parts: Historical Account Data, Historical Phone Lead and Historical Message Lead. Using these three data, we calculate and store other important metrics like Cost per Acquisition, Conversion Rate, number of booked leads, number disputed leads, pending leads and approved leads.', 'Google data studio dashboard: The dashboard will show the count of important metrics like total number of records, total interactions and different types of leads. It will represent different types of graphs portraying different kinds of information and tables containing major data like Lead data combined and Net monthly spent on Ads.', 'Tools used', 'For extracting the data from the LSA Dashboard, we have made our own tool by python scripts. The automation tool will store data in the excel sheets and google bigquery for respective businesses on a day to day basis. PyCharm for compiling and running the code. JsonViewer for processing', 'Language/techniques used', 'We have used the LSA API to extract data from the LSA Dashboard. Google Sheets API to store data in excel sheets. Bigquery API for storing data in google bigquery. The scripts for the automation tool were written in the Python programming language.', 'Models used', 'Software Model: RAD(Rapid Application Development model) Model', 'In the RAD paradigm, less emphasis is placed on planning and more emphasis is placed on development activities. It aims to create software in a short period of time.', 'Advantages of RAD Model:', 'Changing needs can be addressed.', 'Progress may be quantified.', 'Increases component reusability.', 'Encourages responses from consumers.', 'Integration from the start solves a lot of integration concerns.', 'Skills used', 'API Data Abstraction', 'Data Visualisation', 'Automation of tools', 'Exception Handling from Python', 'Data Preprocessing', 'Data Wrangling', 'Databases used', 'Two types of databases: Google excel sheets and google bigquery.', 'Web Cloud Servers used', 'Google BigQuery Cloud Database with up to 1 TB of free storage is being used.', 'What are the technical Challenges Faced during Project Execution', 'Some minor technical challenges were faced for clients with minimum data. For those, plotting graphs became difficult.', 'How the Technical Challenges were Solved', 'We tried to process the data, remove the blank data spaces and plotted the graph with available data.', 'Business Impact', 'It’s undeniable that Google’s Local Services ads (LSA) have changed the way home service businesses advertise online.', 'The pay per lead system designed to provide the end-user with a quick, clean and trusted experience, gives small and medium-sized businesses a better shot at competing with national brands and massive budget operations.', 'To win with the Local Services the businesses need to take care of some factors where data comes to help.', 'Dialling in your service area, Profile and Budget: The data from the message and phone leads help to know whether they are potential customers. If they are potential customers, their location and profile can help you in charging them or not charging the leads.', 'Mark your JOBS as Booked: The dashboard will display the number of archived leads and booked leads. This count can help you analyse your performance and how you can work to increase your potential customers.', 'Deal with disputes: The dashboard will also represent the disputed disputes and approved disputes which will help you to deal with the disputes.', 'Net Monthly Ad Spend: This is an important metric which helps the firms to make better decisions for their expenditure. They can have an efficient control over their expenditure once they have proper data available. Other metrics related to finances include Cost per lead, Cost per Acquisition and Conversion rate.', 'Project Snapshots', 'Fig.1: Data Dashboard for individual businesses-1', 'Fig.2: Data Dashboard for individual businesses-2', 'Fig.3: Consolidated Dashboard', 'Fig.4: Historical Account Data', 'Fig.5: CPA and CPL datasheet', 'Fig.6: Lead Dispute Status', 'Previous article', 'How Metaverse will change your life?', 'Next article', 'AI Conversational Bot using RASA', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2116,https://insights.blackcoffer.com/aws-lex-voice-and-chatbot/,"AWS Lex Voice and Chatbot - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in USA', 'Industry Type:', 'IT', 'Services:', 'eCommerce', 'Organization Size:', '40+', 'Project Objective', 'Create a Voice and chatbot using AWS lex which can book flights, hotels, cars and book some fun activities in a city.', 'Project Description', 'We need to create a voice and chatbot using AWS lex and lambda function. The bot should book a flight, a hotel, and a car by asking some relevant questions to the user like destination, origin, date, etc. We also need to create a combination of all these which can plan the whole trip, flight, hotel, car and book some fun activities.', 'Our Solution', 'We have created aws lex intents and lambda functions for all bookings. Intents manage front ends like utterances (user can ask to the bot) and slots (bot replies with relevant questions). Lambda functions manage backend parts like which intent should be triggered if the user says “ book a flight” or “book a hotel” or “book a car”. For search results we have used some external APIs like Amadeus for flight, sabre for hotels and blablacar for car booking.\xa0 We have modified search results by using Data Analytics (for getting the cheapest and good star flight and hotel), Machine learning (for getting user’s preferences by analyzing user’s history) and NLP (Differentiate search results by text analysis) techniques so users can get the best search results.', 'Project Deliverables', 'An aws lex voice and chatbot which can book flight, hotel, car and fun activities. This can be integrated with IOS applications.', 'Tools used', 'AWS Lex, AWS Lambda, AWS Cognito, AWS EC2, Google colab, VS code, FAST API, Uvicorn.', 'Language/techniques used', 'python, machine learning, data analytics, NLP.', 'Models used', 'TfIdf-Vectorizer and cosine similarity', 'Skills used', 'Data Analytics, Machine learning, NLP, Python, AWS, REST APIs.', 'Databases used', 'MySQL', 'Web Cloud Servers used', 'AWS', 'What are the technical Challenges Faced during Project Execution', 'The first challenge we have faced is the integration of AWS lex and lambda functions.', 'Amadeus and Sabre APIs data was not in a good format so we have to clean some data and organize it in a usable format.', 'We need to make some APIs so we can pass flight or hotel parameters and the APIs will give flight or hotel related data.', 'Create a book button in the bot for booking flights, hotel,s and car.', 'How the Technical Challenges were Solved', 'So the integration of AWS lex and lambda function was very tough for us. Because lex uses some intentes to show responses from the lambda function. So we have created different lex intents to pass messages to lex bot from lambda function. And put some good coding to the lambda function so different messages can be handled by different intents.', 'For flight, hotel and car search results we were using some external apis like amadeus, sabre and blablacars apis. These APIs have a lot of data and are not in a format we need.\xa0 So first we cleaned data and then sorted data according to cheaper and best ratings results. We have used the best two results among all the results.', 'We cannot use all the machine learning and data analytics part in aws lambda function. So what we did was we created some REST APIs which can handle all the data analytics and machine learning part and we hosted these APIs on AWS EC2 instance. We used these APIs in our lambda functions.', 'So Creating a button in a chat bot or voice bot is always so different from providing text messages. For creating a button we used a response card structure in lambda function which can handle button and button related responses.', 'Project Snapshots', 'Project Video', 'Previous article', 'MetaBridges API Decentraland Integration – AR, VR', 'Next article', 'AI/ML and Predictive Modeling', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2117,https://insights.blackcoffer.com/metabridges-api-decentraland-integration/,"MetaBridges API Decentraland Integration - AR, VR - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in the USA', 'Industry Type:', 'IT', 'Services:', 'Consulting, Software, Blockchain, Metaverse', 'Organization Size:', '20+', 'Project Objective', 'To integrate with Metaverse environments with the help of EC2, S3 bucket and the Decentraland SDK.', 'Project Description', 'Move 3D model files from EC2 instance to S3 bucked using aws-sdk.', 'Our Solution', 'Configure\xa0 s3 bucket in aws account, create an user for s3 bucket api keys, and', 'api secret. Put the api key, aapi secret, bucket name and bucket region in', 'environment variable to use them in app. Install aws-sdk to implement s3 bucket.', 'Create a function to send file from nodejs server to s3 bucket.', 'Project Deliverables', 'Aws ec2 instance credentials, s3 bucket credentials. Code used in the project', 'Tools used', 'vs code editor, git bash terminal, google chrome web browser.\xa0 Metamask wallet, cryptocurrency, blockchain, bitcoin,  metamask, metaverse, VR, AR, Virtual Reality, Augmented Reality', 'Language/techniques used', 'Javascript language is used.\xa0 Metamask wallet, cryptocurrency, blockchain, bitcoin,  metamask, metaverse, VR, AR, Virtual Reality, Augmented Reality', 'Models used', 'dcl SDK (Decentraland sdk for nodejs), aws-sdk, awscli.', 'Skills used', 'Node js project setup, Dcl sdk setup, Aws ec2 instance setup with aws cli,', 'S3 bucket connection with aws-sdk. cryptocurrency, blockchain, bitcoin, metamask, metaverse, VR, AR, Virtual Reality, Augmented Reality', 'Databases used', 'No database is used', 'Web Cloud Servers used', 'AWS cloud server is used', 'What are the technical Challenges Faced during Project Execution', 'Making the application port in ec2 instance available globaly.', 'How the Technical Challenges were Solved', 'Search few blogs and videos for the solution. And make it done by doing some change in', 'Security group in ec2 instance.', 'Business Impact', 'As Decentraland is a platform based of NFT so main part of business is related to NFT and cryptocurrency.', 'Project Snapshots', 'Project Video', 'Previous article', 'Microsoft Azure chatbot with LUIS (Language Understanding)', 'Next article', 'AWS Lex Voice and Chatbot', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2118,https://insights.blackcoffer.com/microsoft-azure-chatbot-with-luis-language-understanding/,"Microsoft Azure chatbot with LUIS (Language Understanding) - Blackcoffer Insights-['Client Background', 'Client:', 'A leading retail firm in the USA', 'Industry Type:', 'Retail', 'Services:', 'e-commerce, retail business', 'Organization Size:', '100+', 'Project Objective', 'To create an advanced chatbot using Microsoft Azure cognitive service to take orders from customer on behalf of a pizza restaurant and give order summary as end result to the user.', 'Project Description', 'The project uses MS Azure LUIS service for language understanding to receive order details from a customer and provide an order summary. Also display various menu options to the customer in a dynamic method.', 'Our Solution', 'Our solution is to create a chatbot on MS Azure platform using their LUIS service in bot-framework composer environment. Use dynamic hero cards to display menu so that user can get a better experience.', 'Project Deliverables', 'Chatbot', 'Tools used', 'Bot Framework composer', 'Bot emulator', 'MS Azure LUIS services', 'Language/techniques used', 'Bot framework composer', 'Natural language processing', 'Models used', 'MS Azure LUIS', 'MS Azure QnA', 'MS Azure speed SDK', 'Skills used', 'Deep learning', 'Web development', 'Cloud tech', 'Web Cloud Servers used', 'Microsoft Azure web platform', 'What are the technical Challenges Faced during Project Execution', 'Monthly quota for LUIS authoring service was reached', 'Tracking multiple items ordered by user', 'Accessing relevant images for each menu item', 'How the Technical Challenges were Solved', 'Switching to a more suitable pricing tier which would have to eventually switch to when move onto production phase', 'Creating custom functions and intents for different trackers', 'Using open license images from internet', 'Project Snapshots', 'Project website url', 'Demo', 'Previous article', 'Do All Social Media Is Owned By Meta?', 'Next article', 'MetaBridges API Decentraland Integration – AR, VR', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2119,https://insights.blackcoffer.com/impact-of-news-media-and-press-on-innovation-startups-and-investments/,"Impact of news, media, and press on innovation, startups, and investments - Blackcoffer Insights-['Client Background', 'Client:', 'A leading research institution in the word', 'Industry Type:', 'Research, R&D', 'Services:', 'R&D', 'Organization Size:', '1000+', 'Project Objective', 'Make data ready for predictive modelling.', 'Making Google Data Studio dashboard.', 'Project Description', 'Phase – 1: In this project first of all we have to clean the data as the data was very noisy, we have to filter out only the needed columns of the data.', 'Phase – 2: Finding co-relation between the pitchbook data and the other output files.', 'Phase – 3: Making dashboard in Google Data Studio for the project.', 'Our Solution', 'We used pandas and numpy to clean the data and make useful for it to be used in predictive modelling. We have found the co-relation between the tempa msa pitchbook data and the output files like textual file, ai_ml_tm file etc. We have made the dashboard using the Google Data Studio.', 'Project Deliverables', 'We have provided a excel file consisting of clean data and the Google Data Studio report.', 'Tools used', 'Python, Google Data Studio, Google Chrome', 'Language/techniques used', 'Python Programming', 'Models used', 'Waterfall model used in this project.', 'Skills used', 'Data cleaning, Data Pre-processing, Data Visualisation are used in this project.', 'Databases used', 'We have used the traditional file systems as database storage.', 'What are the technical Challenges Faced during Project Execution', 'Cleaning the data was the major challenge faced while executing the project. The data has a lot of noise. It was difficult to find which data was useful and which data is not useful in this project. Secondly the co relation between the output files and pitchbook data. There was nothing common between both the datasets. So was difficult to find co-relation between them.', 'How the Technical Challenges were Solved', 'We used pandas dataframe to clean the data and make it ready for predictive modelling and used the Google Data studio to find insights between the different datasets.', 'Project Snapshots', 'Project Video', 'Previous article', 'AWS QuickSight Reporting Dashboard', 'Next article', 'How Metaverse is Shaping the Future?', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2120,https://insights.blackcoffer.com/aws-quicksight-reporting-dashboard/,"AWS QuickSight Reporting Dashboard - Blackcoffer Insights-['Client Background', 'Overview', 'As a Singapore and Australia based startup, Drive lah (known as Drive mate in Australia) is a peer-to-peer car sharing platform where you can rent a large variety of cars, always nearby at great value. All trips on Drive lah are comprehensively insured through our insurance partners so car owners don’t have to worry about their insurance. The idea is simple: car ownership is expensive in Singapore (per month yet only use the car 5% of the time – cars are mostly parked. With Drive lah you can reduce the cost of ownership by renting it out when you don’t need it in a safe way. Renters can rent those cars when they are not used by their owners at good value.', 'In a fast-growing non-ownership economy where taxi, food, beauty is available on-demand, Drive lah is envisioning to take the lead in distance travel and simplifying car access', 'Website', 'http://www.drivelah.sg', 'Company size', '11-50 employees', 'Founded', '2019', 'Project Objective', 'Automating the process to get updated Metrics every week.', 'Evaluate the following Performance Metrics which will be used on AWS Quick Sight for Performance Evaluations:', 'Total Cancellations', 'Cancellations by Host', 'Weekly Guest Success Rate.', 'Monthly Active User’s {MAUs}', 'Monthly Active Listings {MALs}', 'Total Approved & Live Listings', 'Approved & Live InstantBookings', 'Approved & Live Dl Go', 'Delivery Booking Listings', 'Weekly Active Listings {WALs}', 'Successful HDM', 'Unsuccessful HDM', 'Booking Acceptance Rate', 'Total Requested Trips', 'New Listings Made Live', 'Percentage of Live Listings Made Active', 'Map Location Metrics Table with Postal Districts.', 'DL Live Cars & DL L3M Active Cars', 'Host Experience Team Weekly Dashboard', 'New Weekly Listings Dashboard', 'Two Transaction Metrics', 'Build Code for extracting Daily Agent Activity Report on Daily Basis.', 'Our Solution', 'For Performance Metrics, we suggested that we will Code for each Metric & will store them in a Table on AWS RDS which will be directly synced to the AWS Quick Sight for Performance Evaluations.', 'For Automating the process to get updated Tables of Metrics every week, we suggested to use a Virtual Machine on which we can upload all code files & can run a Cron Job for each file to automatically get updated on specified time every week.', 'Tools used', 'Jupyter Notebook', 'PyCharm', 'MySQL Workbench', 'AWS Quicksight', 'Language used', 'Python', 'Database Used', 'Amazon Relational Database Service (RDS)', 'What are the technical Challenges Faced during Project Execution?', 'Tried with AWS Lambda Function to update tables on AWS RDS but Lambda Function was unable to run complete code.', 'How the Technical Challenges Were Solved?', 'Suggested to use a Virtual Machine on which we can upload our Code Files & can run Cron Job for automatically updating tables on regularly basis.', 'Project Snapshots', 'Metrics from Listings Table:', 'Host Experience Metric:', 'New Live Listings of Last 7 Days:', 'Line Chart of Total Cancellations & Cancellations by Host:', 'Line Chart of Monthly Active Users (MAU’s):', 'Area Chart of Percentage of Live Listings Made Active:', 'Line Chart of Number of DL GO Listings & Number of Instant Booking Listings:', 'Line Chart of Monthly Active Listings (MAL’s):', 'Line Chart of New Listings Made Live:', 'Vertical Bar Chart of Total Approved & Live Listings:', 'Project Video Link', 'Previous article', 'Google Data Studio Dashboard for Marketing, ads and Traction data', 'Next article', 'Impact of news, media, and press on innovation, startups, and investments', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2121,https://insights.blackcoffer.com/google-data-studio-dashboard-for-marketing-ads-and-traction-data/,"Google Data Studio Dashboard for Marketing, ads and Traction data - Blackcoffer Insights-['Client Background', 'Overview', 'Bankiom – the super banking app for MENA on a mission to make managing your finances easier.', '☞ Open an account on your phone and get a virtual card in 3 minutes or less', '☞ Manage all your bank accounts from one app and one control panel', '☞ Save money and grow your wealth', 'Website', 'http://www.bankiom.com', 'Company size', '2-10 employees', 'Founded', '2019', 'Specialties', 'Banking, Financial Services, Card Payments, Mobile Payments, Digital Bank, and FinTech', 'Project Objective', 'Build a dashboard unifying all the platforms that we use: Google Ads, FB ads, Appsflyer, Mixpanel', 'Project Description', 'We want to be able to track everything in the funnel from traffic source to total installs (paid, organic and by channel):', '– App settings in Appsflyer', '– SDK Installation, test it (+ instruction for devs)', '– Ad sources setup in ad accounts (Facebook, Google Ads, etc)', '– Ad sources setup in Appsflyer', '– In-app conversions mapping', '– Conversion set up in ads sources', '– One link, smart script, and deep link setup', '– SKAD Network for IOS app', 'Our Solution', 'Built dashboard for each data source like Google Ads, Facebook Ads for tracking installs, channel spend, cost per install for both Android and IOS.', 'Then, we made a dashboard for tracking the retention rates of customers and other events that they execute on the app like transfer money, user registration, connect banks. The data for these events was fetched from MixPanel.', 'These dashboards were made using Google Data Studio.', 'Project Deliverables', 'We need to deliver dashboards for tracking the ads data from Google and Facebook and to track the events which the users perform on their app and for this data was collected from MixPanel.', 'Tools used', 'Following Tools were used for successful execution of the project', 'Google Data Studio', 'Adveronix', 'Mixpanel Api', 'BigQuery', 'GCP', 'Language/techniques used', 'Code was written to create the pipeline to fetch MixPanel data through mixpanel Api and store it in bigquery. So, the code was written in Python.', 'Skills used', 'Following Skills were used to complete the project', 'Data Preparation', 'Data Visualization', 'Python', 'API', 'BigQuery', 'Google Cloud Platform', 'Databases used', 'For storing the data of the project Google Sheets and Google BigQuery were used.', 'Web Cloud Servers used', 'Web Cloud server used in this project was Google Cloud Platform.', 'What are the technical Challenges Faced during Project Execution?', 'Technical Challenges faced during the execution of the project was to understand how the api of the mixpanel works and how to connect it to Google BiqQuery. Another technical challenge that we faced was to find a free resource to connect the facebook ads data to data studio.', 'How the Technical Challenges were Solved', 'To solve the technical challenges we went through the documentation of the mixpanel api to get a understanding of how the things work. Based on that we built the pipeline to connect the mixpanel data to big query. The other technical challenge of finding a free resource to connect the facebook ads to datastudio for free was solved by researching for the various connectors available and we found an add on named ‘Adveronix’ which could connect the facebook ads data to google sheets which can eaily be connected to data studio.', 'Project Snapshots', 'Project website url', 'https://datastudio.google.com/reporting/8af163c1-b328-4ed3-91fc-cf8a026d0d9f', 'Project Video', 'Previous article', 'Gangala.in: E-commerce Big Data ETL / ELT Solution and Data Warehouse', 'Next article', 'AWS QuickSight Reporting Dashboard', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2122,https://insights.blackcoffer.com/gangala-in-e-commerce-big-data-etl-elt-solution-and-data-warehouse/,"Gangala.in: E-commerce Big Data ETL / ELT Solution and Data Warehouse - Blackcoffer Insights-['Client Background', 'Client:', 'A leading eCommerce firm in the USA, Columbia, India, and Latin America', 'Gangala', 'promotes local shops selling a wide variety of products at great prices. Easily find the best offers using our price comparison tool. It’s a WIN WIN for\xa0…', 'Industry Type:', 'eCommerce', 'Services:', 'e-commerce, retail business', 'Organization Size:', '100+', 'Project Title', 'Gangala.in: E-commerce site gathering data of different products from various sources and providing it on a single platform', 'Project Objective', 'Provide up-to-date data of any given product on the website along with 3-5 prices of that product from different sites for the customer to compare and buy.', 'Project Description', 'A platform in which users can get price data of any product from multiple sites. The client provided us with raw data. We were tasked with building a pipeline for the data, build API’s to get product data such as price and update them and make sure that all the data is available for the front end team to access.', 'Our Solution', 'We built them a pipeline to process and clean the raw data provided. We built API’s to fetch the updated data of the products. Neo4j was used as the intermediary data and mongoDB was used as our primary database. We also process the images of each product and remove any unwanted texts from it and add the client’s watermark.', 'Project Deliverables', 'A fully-updated database with up to date data on all the products and each product having atleast 3-5 prices from different sites.', 'Tools used', 'Numpy package', 'Json package', 'csv package', 'concurrent futures package (for multithreading)', 'Py2neo package (to connect to neo4j using python)', 'Language/techniques used', 'Python', 'Cypher Query Language (CQL)', 'APOC Queries', 'Databases used', 'Neo4j', 'MongoDB', 'Dataiku', 'Odoo', 'DSS', 'Web Cloud Servers used', 'Linode cloud servers', 'What are the technical Challenges Faced during Project Execution', 'We were asked to process 3million products per day and this was a challenge as the VM’s we used were not able to handle the load.', 'How the Technical Challenges were Solved', 'We were able to overcome the challenge by using Asynchronous processing of the data thereby increasing the speed of the processing reducing the cost on the client side as well', 'Project website url', 'https://gangala.in/', 'Previous article', 'Big Data solution to an online multivendor marketplace eCommerce business', 'Next article', 'Google Data Studio Dashboard for Marketing, ads and Traction data', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2123,https://insights.blackcoffer.com/big-data-solution-to-an-online-multivendor-marketplace-ecommerce-business/,"Big Data solution to an online multivendor marketplace eCommerce business - Blackcoffer Insights-['Client Background', 'Client:', 'A leading eCommerce firm in the USA, Columbia, India, and Latin America', 'Gangala', 'promotes local shops selling a wide variety of products at great prices. Easily find the best offers using our price comparison tool. It’s a WIN WIN for\xa0…', 'Industry Type:', 'eCommerce', 'Services:', 'e-commerce, retail business', 'Organization Size:', '100+', 'Project Objective', 'To give User experience of easy and convenient Shopping by searching all the products like any medicines , Clothes , Gadgets etc in a single Website without going through all the E-Commerce Sites and make shopping easy and get the most affordable and best product.', 'Project Description', 'It’s an E-Commerce Sites that’s helps customer to compare different\xa0 products that were available on different E-Commerce Sites like Flipkart , Amazon , Netmeds etc.It’s helps the user to visit only one sites to get what they need and find the perfect product without visiting all the sites.The gives the user a great and friendly Experience in Buying any Products.It’s Also have some Unique Similar Products Recommendation Based on user search and also have a ChatBot That’s solves User Query .It’s uses Big data and Rest API that’s help the projects for regular updates and regular fetching of the new products.', 'Our Solution', 'In BlackCoffer We create the flow of the Big Data and all Backend Solution That is requires for this futuristic E-Commerce Sites.We Create Pipelines for the data of all the products and their price and url fetch from different E-Commerce Sites using Custom made APIs And perform many data cleaning, data transformation and data validation techniques to make sure the standard of data to be used by Our Sites .We also get Additional Feature from the scraped data By using Different APIs . We also create automation and custom python scripts that helps us to achieve some outstanding data related tasks.', 'Project Deliverables', 'Python script for performing ETL and Cypher Query for big data Handling.', 'Tools used', 'Jupyter Notebook', 'DSS', 'VS Code', 'Language/techniques used', 'Python', 'No SQl', 'Cypher', 'ETL', 'Models used', 'Similar Price API', 'Whatsapp Chat API', 'Similarity Server to get similar products', 'Skills used', 'Data Engineering', 'Data Analysis', 'Python Programming', 'Rest APIs', 'Databases used', 'DSS', 'NEO4J', 'MongoDB', 'Web Cloud Servers used', 'Linode', 'AWS', 'What are the technical Challenges Faced during Project Execution', 'Data Cleaning : -The Scraped that will be used by our sites is coming from different sources and also it’s not’s that clean to be used by sites .This is the very first problem every data scientist faced during the whole process.', 'Data Merging :-\xa0 The data is scraped from around 140 sources that’s why it’s very difficult to maintain the attributes that should be used by all the sources and we can get a clean and sufficient amount of data to process.', 'Data Validation :-\xa0 There are many records that have null values and missing values that disturb the users experience a lot .That should be handle with very care.', 'How the Technical Challenges were Solved', 'Data Cleaning : – For Data cleaning we used Python Data Frame and Pandas and data structure and handles the data cleaning and optimize our data for get correct data format and useful data.', 'Data Merging : –\xa0 For data Merging and data transformation we used pandas that help to get the appropriate data that can used further And also make Python pipelines for future updation.', 'Data Validation :- For that data validation we use some fundamental property and feature selection that’s help us to make the appropriate data format and records to be used In our sites.', 'Project Snapshots', 'Project website url', 'https://gangala.in/', 'Previous article', 'Creating a custom report and dashboard using the data got from Atera API', 'Next article', 'Gangala.in: E-commerce Big Data ETL / ELT Solution and Data Warehouse', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2124,https://insights.blackcoffer.com/creating-a-custom-report-and-dashboard-using-the-data-got-from-atera-api/,"Creating a custom report and dashboard using the data got from Atera API - Blackcoffer Insights-['Client Background', 'Client:', 'A leading Marketing firm in USA', 'Industry Type:', 'Marketing', 'Services:', 'Marketing, consulting, ads, business solutions', 'Organization Size:', '20+', 'Project Description', 'Atera.com is used as our RMM, we have an agent on every machine. Which tracks the if a machine goes down, initial response time etc.., The website doesn’t provide any standard reports, So we needed to create a custom report.', 'Our Solution', 'Importing the data from Atera API into Jupyter', 'Using Web Scraping download the JSON data', 'Convert the JSON data to Data Frame and download it into PC.', 'Clean the data with only required columns', 'Upload the data into google sheets.', 'Connect google sheets and google data studio', 'Create the dashboard with the data', 'Tools used', 'Python (Pandas, requests)', 'Google Sheets', 'Google Data Studio', 'Skills used', 'Analytics', 'Programming Language', 'Databases used', 'Contacts.csv', 'Customers.csv', 'Tickets.csv', 'Alerts.csv', 'What are the technical Challenges Faced during Project Execution?', 'I found it difficult on downloading the data.', 'How the Technical Challenges were Solved', 'Once I figured I have been using the wrong Authorization key to login I was able to solve the issue, and convert the curl command into python', 'Project Snapshots', 'Project website url', 'https://datastudio.google.com/reporting/5e61aecb-a420-41cc-afba-d0ca37f69132', 'Project Video', 'Previous article', 'Azure Data Lake and Power BI Dashboard', 'Next article', 'Big Data solution to an online multivendor marketplace eCommerce business', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2125,https://insights.blackcoffer.com/azure-data-lake-and-power-bi-dashboard/,"Azure Data Lake and Power BI Dashboard - Blackcoffer Insights-['Client Background', 'Overview', 'Stone is a video bibliographic tool for journalists and other researchers.', 'It allows users to capture, annotate and share their journeys through digital and physical space, producing verifiable logs and generating monetizeable video highlight reels that can be embedded in digital and other media – showcasing key moments and telling the story behind the story.', 'Our mission is to address distrust and disinformation with transparency and authenticity, while simultaneously tilting the information ecosystem in favour of quality original work.', 'Research is valuable. Make it Visible.', 'Write In Stone.', 'Website', 'http://www.writeinstone.com', 'Company size', '2-10 employees', 'Headquarters', 'Blackheath, New South Wales', 'Founded', '2017', 'Specialties', 'Research Transparency, Trust, Video Content, Journalism, Proof Of Work, and Bibliographic Standards', 'Project Objective', 'Working on Microsoft Azure Analytics Services', 'Verifying that indicators are being gathered in an intended manner, in line with GDPR provisions', 'Building and analyzing dashboards and, specifically, conversion funnels', 'Project Description', 'To determine whether the already implemented indicators in are in intended fashion (separated by where these indicators are placed in the currently constituted funnel)', 'Implement New Indicators', 'Research Logged', 'Average Number of Highlights per Project', 'Total Hours of Content Produced', 'Total Hours of Content Watched', 'Daily unique visitors engaging with Stone, including the landing page, public research page(s), and the research portal', 'Assess the dashboard set up in Azure, refine the existing dashboard, and determine whether an alternative is preferable.', 'Review, refine, and optimize the WIS conversion funnel(s)', 'Our Solution', 'Built a Power BI dashboard as per the requirement. Also built a separate dashboard for the metric data from Azure.', 'Project Deliverables', 'Power BI dashboard which contains indicators funnels, new indicators(Research logged, Average number of Highlights per projects, Total hours of content watched etc), visualizations\xa0 extracted from metric data.', 'Tools used', 'Power BI', 'Azure', 'Language/techniques used', 'Power BI', 'DAX', 'Kusto Query', 'Azure', 'Skills used', 'Data collection', 'Data Analysis', 'Data cleaning', 'Feature engineering', 'Querying', 'Visualization', 'Databases used', 'Azure database', 'Web Cloud Servers used', 'Azure', 'What are the technical Challenges Faced during Project Execution', 'Difficulty in data collection.', 'Previous article', 'Advantages and Disadvantages of E-learning during the COVID-19 for students and teachers', 'Next article', 'Creating a custom report and dashboard using the data got from Atera API', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2126,https://insights.blackcoffer.com/google-data-studio-pipeline-with-gcp-mysql/,"Google Data Studio Pipeline with GCP/MySQL - Blackcoffer Insights-['Client Background', 'Client:', 'A leading IT firm in Europe', 'Industry Type:', 'IT', 'Services:', 'e-commerce, retail business, marketing, Consulting', 'Organization Size:', '100+', 'Project Objective', 'Creating a Data Pipeline to sync live data from FieldPulse to Google Data Studio using GCP/MySQL.', 'Project Description', 'There is a Virtual Machine up and running and MySQL in Google Cloud(GCP). Get the following live data from FieldPulse to Google Data Studio(GDS) for making Business Dashboard in GDS –', 'Job Data', 'Tag Data', 'Team Member Data', 'Team Data', 'Such that if data changes in FieldPulse , GDS Dashboard should update automatically.', 'Our Solution', 'For fetching data from FieldPulse –', 'Data Pipeline (FieldPulse to GCP MySQL)', ':\xa0 We have created a Data Pipeline that uses web scraping to fetch data from FieldPulse. It makes a GET request to the FieldPulse API , and the API returns raw data. Convert this into json format then in Dataframe. Now , create new tables in GCP MySQL and insert/update the data accordingly.', 'Insertion & Updation of Data :', 'Insertion :', 'If any data fetched from Fieldpulse is not present in their respective database table , then\xa0 insert that data in the table.', 'Updation :', 'If any data fetched from Fieldpulse is present in their respective database table , then update that data in the table.', 'Deploy the above Data Pipeline in GCP VM instance', ':\xa0 Deploy the above data pipeline in GCP VM so that data gets updated every hour from FieldPulse to MySQL.', 'For getting data from GCP MySQL to Google Data Studio(GDS) :', 'Connecting GCP MySQL to Google Data Studio', ':\xa0 Connect GCP MySQL to GDS as follows –', 'Open a new report', 'Click on add data', 'Choose MySQL connector', 'Enter following fields :', 'Host Name or IP\xa0 :', 'xxx.xxx.xxx.xxx', 'Database \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 :', 'xyz', 'Username\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 :', 'xyz', 'Password \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 :', '**********', 'Enable SSL', 'Upload', 'server-ca.pem', 'certificate', 'Upload', 'client-cert.pem', 'certificate', 'Upload', 'client-key.pem', 'certificate', 'Click Authenticate', 'Add whatever table you want', 'Build Visualization', 'Project Deliverables', 'Below are the services that we provided to client after completion of this project –', 'Deployed Data Pipeline in GCP', ':\xa0 A Data Pipeline connecting FieldPulse(', 'https://webapp.fieldpulse.com/', ') to GCP MySQL that is deployed on a client’s GCP VM instance. It updates the data in MySQL every hour. It extracts the following data tables from FieldPulse –', 'Job Data', 'Tag Data', 'Team Member Data', 'Team Data', 'Maintaining a log file in Google Cloud :', 'There is a log file in cloud to resolve unexpected error quickly if any , log file stores following details –', 'last pipeline synced time', 'Error type if any', 'Error location if any', 'Work Order Data :', 'Job id', 'Work order no.', 'Tags titles', 'Start_time', 'Job_type', 'Created By', 'Status', 'Invoice_status', 'Assigned teams name', 'Project_id', 'Assignment_count', 'Assignable_type', 'Notes', 'Customer_notes', 'Customer_first_name', 'Customer_last_name', 'Location', 'Assigned_team_members name', 'End_time', 'created_at', 'Job Tag Data :', 'Tag ids', 'Company_id', 'Mongo_id', 'Title (Tag name)', 'Type', 'Color', 'Created_at', 'Updated_at', 'deleted_at', 'Setup to Connect GCP MySQL to Google Data Studio(GDS)', ':\xa0 Provided a setup to connect GCP MySQL to GDS easily. Client can access his live data from MySQL to GDS and make visualizations out of it.', 'Tools used', 'Google Colab', 'Language/techniques used', 'Python', 'Web Scraping', 'MySQL', 'Skills used', 'Programming in Python', 'Data Structure & Algorithm', 'Web Scraping', 'File Handling', 'Google Cloud', 'Google Data Studio', 'Databases used', 'MySQL', 'Web Cloud Servers used', 'Google Cloud Platform (GCP)', 'What are the technical Challenges Faced during Project Execution', 'Getting Data from FieldPulse :', 'As there is no open source package/library in Python for accessing Fieldpulse API , we struggled a lot to get the desired data from FieldPulse.', 'Setting Up Connection from GCP MySQL to GDS :', 'Due to firewall and VPN , connection was not set up as IP address changes while using VPN. It was showing an error every time someone tries to connect to MySQL from their Google Studio account.', 'How the Technical Challenges were Solved', 'Getting Data from FieldPulse :', 'We did use web scraping for this. We explored all the API addresses. We connected to each possible address and got the data then explored the data. Made a list of addresses which contains data of our interest. Also data is stored in a scattered and cascaded manner in FieldPulse with ids. So , we had to fetch a lot of extra tables and then join multiple tables to get a desired data table.', 'Setting Up Connection from GCP MySQL to GDS :', 'To resolve this issue , we did as below –', 'set up the IP address in GCP MySQL security to 0.0.0.0 , so that any system can access it. (VPN issue resolved)', 'Enabled the SSL in GCP MySQL. (to prevent it from unauthorized access)', 'Project Video', 'Previous article', 'QuickBooks dashboard to find patterns in finance, sales, and forecasts', 'Next article', 'AI and its impact on the Fashion Industry', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2127,https://insights.blackcoffer.com/quickbooks-dashboard-to-find-patterns-in-finance-sales-and-forecasts/,"QuickBooks dashboard to find patterns in finance, sales, and forecasts - Blackcoffer Insights-['Client Background', 'Client:', 'A leading marketing firm in the USA', 'Industry Type:', 'Marketing', 'Services:', 'e-commerce, retail business, marketing', 'Organization Size:', '100+', 'Project Objective', 'Build a fully Integrated BI Platform in PowerBI using native connectors and APIs(QuickBooks and Airtable) to pull real time data from many sources.', 'Project Description', 'For building a fully integrated BI Platform , the data has to come from the following sources to feed it to PowerBI –', '·', 'QuickBooks :', 'An accounting software that accepts real-time business payments ,\xa0 manage and pay bills, manage organisation’s deposits/expenses , customers ,and payroll functions. The following data/tables has to be fetched from Quickbooks –', 'o \xa0 Customer', 'o \xa0 Invoices', 'o \xa0 Product & Services', 'o \xa0 Payments', 'o \xa0 Expenses', 'o \xa0 Deposits', 'o \xa0 Accounts', 'o \xa0 Vendors', 'o \xa0 Departments', 'o \xa0 Classes', '·', 'Airtable :', 'An online database hybrid platform for creating and sharing relational databases with friendly user interfaces. The following databases with multiple data table has to be fetched from Airtable –', 'o \xa0 Marketing Data Analytics Base (Google Ads , Facebook Ads)', 'o \xa0 Payroll Tracking (Payroll , Hours Log)', 'This Quickbook and Airtable real time data has to go to the powerBI service (', 'https://app.powerbi.com', '). Then create useful visualisation and dashboards based on plan and feedback from the executive team. All visuals in dashboards should automatically update without any intervention to make it fully integrated.', 'Our Solution', 'Collecting data tables from data sources :', 'Data Pipeline(QuickBooks to Airtable)', '– We have built a Data Pipeline in Python that uses quickbooks API(', 'https://pypi.org/project/python-quickbooks/', ') to get raw data tables from QuickBooks and uses Airtable API (', 'https://api.airtable.com/v0/base_key/Table_name?api_key=YOUR_API_KEY', ') to write/update data in Airtable. It fetches all the below raw tables after making requests to QuickBooks API –', 'Customers , Invoices , Expenses , Deposits', 'Accounts , Departments , Vendors etc.', 'After getting these raw data tables , pipeline converts it into DataFrame , then writes/updates it into Airtable.', 'The Pipeline is deployed in a server that runs every night , it fetches the data from QuickBooks API and writes/updates to Airtable.', 'Airtable to PowerBI', '– As there is no connector available to sync data from Airtable to PowerBI. We have used pagination using DAX queries to get data from Web Sources i.e. Airtable API. Pagination fetches the data page by page\xa0 using a source and offset technique set by the Airtable API developers. It successfully fetches all the below bases from Airtable API –', 'Marketing Data Analytics Data (Google Ads , Facebook Ads)', 'Payroll Data (Payroll , Hours Log)', 'Scheduled Refresh :', 'To refresh visualization/dashboard (If incoming data from Airtable API has updated) , set refresh time in powerBI service.', 'Preprocessing of\xa0 Data –', 'We have used DAX queries to prepare and process the raw data coming from Airtable like –', 'Split data , typecast data', 'Filter data (fill missing values , delete irrelevant rows etc.)', 'Create visualizations/Dashboards', '– We have used following techniques to create visualizations –', 'Used M code queries to extract useful/desired data', 'Used measure to perform calculations on data', 'Use a calculated table to create a relationship between two tables.', 'Used data joining (Union , Intersection) to get desired data', 'Project Deliverables', 'Below are the services that we provided to client after completion of this project –', 'Deployed Data Pipeline', ':\xa0 A Data Pipeline connecting QuickBooks to Airtable to sync in the following data tables –', 'Customers', 'Invoices', 'Product & Services', 'Expense', 'Deposits', 'Payments', 'Accounts', 'Vendors', 'Departments', 'Classes', 'QuickBooks Data Dashboard', ': It contains following visualizations –', 'KPIs –', 'Total Revenue', 'Total Spend', 'Total Profit', 'Profit Margin', 'No. of Customer', 'Line Charts –', 'Revenue/Expense over days', 'Bar Charts –', 'Revenue & Expenses by Businesses', 'Profit/loss by Businesses', 'Revenue & Expense by Class', 'Profit/loss by Class', 'Pie Chart', 'Expenses by Category', 'Paid/Unpaid Invoices', 'Tables –', '[Class , Business , Revenue , Spend , Profit , Profit Margin)', '[Customer , Balance , Due(in days)]', '[Customer , Balance , OverDue]', '[Account , QuickBooks Balance]', 'Filters/Slicer –', 'Transaction Date', 'Business', 'Class', 'Marketing Analytics (Facebook Ads) Dashboard', '–', 'KPIs –', 'All Impressions', 'Total Reach', 'Total Link Clicks', 'Average CPM', 'Amount Spent on Ads', 'Total Budget', 'Budget Left', 'Line Charts –', 'Avg. Frequency Over Days', 'Avg. CPC over days', 'Impressions , Reach and Page Engagement over days', 'Link Clicks by day and Account Name', 'Results , Cost per Results over days', 'Ad set Budget and Amount Spent Over days', 'Bar Charts –', 'Ad set Budget and Amount Spent by Account Name', 'Gauge –', 'Daily Avg. Links', 'Count of Ongoing Campaigns', 'Tables –', 'Top Compeigns [Account name , Compeign name , Link Clicks , Impressions , Reach , Avg. Frequency , Social Impressions]', 'Filters/Slicer –', 'Account name', 'Date Range', 'Marketing Analytics (Google Ads) Dashboard', '–', 'KPIs –', 'Total Impressions', 'Total Clicks', 'Total Conversions', 'Total Cost', 'Daily Avg. Cost', 'Daily Avg. CTR', 'Daily Avg. Conversion Rate', 'Daily Avg. Cost per Conversion', 'Line Charts –', 'Clicks and Conversions over days', 'Avg. CPC over days by Day and Google Ad Account', 'Clicks per Impressions by Day and Google Ad Account', 'Impressions by Day and Google Ad Account', 'Cost by Day and Google Ad Account', 'Clicks by Day and Google Ad Account', 'Gauge –', 'Avg. Daily new Conversions', 'Pie Chart –', 'Count of Google Ad Accounts', 'Tables –', 'Top Ads [Ad name , Ad Group , Conversions]', '[Google Ad Account , Impressions , Clicks , Conversions]', 'Filters/Slicer –', 'Date Range', 'Google Ad Account Name', 'Payroll Dashboard', '–', 'KPIs –', '$ Total Payroll', '$ Avg. Rate', 'Count of Invoice', 'Total Payroll Time (in hrs.)', 'Avg. TurnArroundTime (in Days)', 'Total Hours', 'Line Charts –', 'Avg. Rate over Days', 'Avg. daily Pay Amount', 'Bar Chart –', 'Payroll time by Employee', '$ Payroll by Employee', 'Hours by Entity', 'Total hours by Employee', 'Pie Chart –', 'Paid/Unpaid Invoices', 'Tables –', 'Payroll [Employee , Count of Invoice , Total Due , Paid Before/After Due Date]', 'Filters/Slicer –', 'Date Range', 'Employee name', 'Entity name', 'Tools used', 'PowerBI', 'Language/techniques used', 'Python', 'Pagination', 'Skills used', 'Programming in Python', 'Data Structure & Algorithm', 'API Integration (QuickBooks , Airtable)', 'File Handling', 'PowerBI(with DAX , M code queries)', 'Data Analytics', 'What are the technical Challenges Faced during Project Execution?', 'QuickBooks Refresh Token Expired Issue', ': As stated in QuickBooks Developer Guide , we need refresh token to access QuickBooks API and it expires after 101 days. But that is not the case , it usually expires within 2 to 4 days depending on how frequently we access the API. In that case our deployed Pipeline does not work if the token expired.', 'Getting data from Airtable to PowerBI', ': As PowerBI has no Airtable data source connector to fetch data from Airtable , we did use Web Source connector using Airtable data web links. It only fetches the 1st page that is 100 rows from Airtable base because Airtable API gives only 100 rows/request.', 'Dynamic Data Source Refresh Issue', ':\xa0 As the URL of Airtable bases data changes based on the size of data. PowerBI recognizes it as Dynamic Data Source , hence it gives the error “Dynamic Data Source Refresh Error” in PowerBI Service.', 'How the Technical Challenges were Solved', 'QuickBooks Refresh Token Expired Issue', ':\xa0 As the token may expire anytime after 2 days , so to resolve this we have added a gui element in Pipeline so that if token expires a pop up will appear asking for a new refresh token , until the consumer enters a valid new token from their QuickBook developer account , a pop up will keep coming and pipeline will be paused. Once the user enters a new token , the pipeline will continue working.', 'Getting data from Airtable to PowerBI', ': To resolve this issue , we have used Pagination technique as below –', 'First request Airtable API with proper URL , api_key and blank_offset (data_url?API_KEY=api_key?OFFSET=blank_offset)', 'This request returns first 100 rows of data and a new offset value', 'Now replace the previous offset value with a new offset value in the URL , and again make an API request.', 'This request will return the next 100 rows of data and a new offset.', 'Do this until you get a null offset (null offset means , all data has been fetched)', 'This is how we get all the data of any size from Airtable bases.', 'Dynamic Data Source Refresh Issue', ': The above mentioned Pagination technique converts dynamic URLs of Airtable bases data into static URLs. So PowerBI gives no error as it has been converted to a static data source. Now the client can refresh the dashboard manually by clicking the refresh button or can set automatic refresh daily at some given time.', 'Project Snapshots', 'Project Video', 'https://www.youtube.com/watch?v=iemcyRtWPNU&ab_channel=Blackcoffer', 'Previous article', 'Marketing, sales, and financial data business dashboard (Wink Report)', 'Next article', 'Google Data Studio Pipeline with GCP/MySQL', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2128,https://insights.blackcoffer.com/marketing-sales-and-financial-data-business-dashboard-wink-report/,"Marketing, sales, and financial data business dashboard (Wink Report) - Blackcoffer Insights-['Client Background', 'Client:', 'A leading retail firm in Australia', 'Industry Type:', 'Retail', 'Services:', 'e-commerce, retail business, marketing', 'Organization Size:', '100+', 'Project Objective', 'Bringing in data from many sources(Google Analytics , ServiceM8 and Xero etc.) and making Business Dashboard KPIs in Wink Report.', 'Project Description', 'For building Business Dashboards in Wink Report , collect data from the following sources –', 'ServiceM8', 'Xero', 'Facebook', 'Google Ads', 'Communiqa', 'Explore/analyze the underlying data tables from each Data Source. Make useful reports using different tables from different data sources based on client’s requirement. Set up formulas in each report to calculate desired fields. Add a custom visualization to each report for making dashlets. Add dashlets to newly created dashboards.', 'Our Solution', 'For collecting the data from the sources (ServiceM8 , Xero , Facebook , Google Ad) native connectors have been used , available in the Wink Report. It fetches the following data/tables from around the given data sources –', 'ServiceM8 Connector –', 'Assets', 'Client', 'Invoices', 'Job Allocations', 'Jobs', 'Materials', 'Payments', 'Xero Connector –', 'Bank Transaction Items', 'Budget Vs Actual', 'Employees', 'Payments', 'Payslip', 'Products', 'Purchase Orders', 'Purchase Invoices', 'Sales Invoices', 'Transaction', 'Facebook Connector –', 'Facebook Ad Insights', 'Google Ads Connector –', 'Ad Insights', 'Google Analytics Connector –', 'eCommerce Campaign', 'Totals', 'Data Pipeline', ': For collecting data from Communiqa website (', 'https://www.communiqa.com.au/', ') , web scraping has been used as there is no connector available for Communiqa to Wink Report. By scraping Communiqa , we get the following data –', 'Account , Date , Total calls , Total unanswered calls , Total engaged calls , Total answered calls , Total minutes etc.', 'Then , we have merged different tables from different sources to get desired reports. Store all reports belonging to the same dashboard in a separate folder. Do this for all the dashboard , then setup formula for calculating desired fields. Add appropriate visualization to each report for each folder. Then , finally add all dashlets belonging to the same folder to a newly created dashboard.', 'Project Deliverables', 'Below are the services that we provided to client after completion of this project –', 'Data Pipeline(Communiqa to Wink Report)', ':\xa0 A Data Pipeline connecting Communiqa to Wink Report to sync in the following data tables –', 'CSR calls [Account , Date , Total calls , Total unanswered calls , Total engaged calls , Total answered calls , Total minutes etc]', 'Company Performance Dashboard', ': It contains following visualizations –', 'KPIs –', 'Sales This Month', 'Leads Booked Today', 'Sales Today', 'Revenue This Month', 'Cash Payment This Month', 'Conversion Rate', 'Open Warranty Jobs', 'Bar Charts –', 'Scheduled Jobs by Category', 'Sales by Month', 'Revenue by Month', 'Tables –', 'Open Jobs from Last month[Job Id , Opened Date , Status, Invoice Amount, Amount Paid]', 'Filters/Slicer –', 'Date Range', 'Job Status', 'Date Grouping(Daily/Monthly/Yearly)', 'Lead Generation Dashboard', '–', 'KPIs –', 'Total Website Traffic this month', 'Average Daily Website Traffic this month', 'No. of Conversion this month', 'Total Marketing Investment this month', 'Marketing Budget Tracking', 'Cost per Acquisition', 'Line Charts –', 'Link Clicks and conversion by month', 'Total marketing spend by month', 'Bar Chart –', 'Lead Generation Count by Source', 'Pie Chart –', 'Lead Generation Source by Invoice Amount', 'Filters/Slicer –', 'Date Range', 'Job Status', 'Lead Conversion Dashboard', '–', 'KPIs –', 'All Employees monthly Sales Target', 'All Employees monthly Conversion Rate', 'Filters/Slicer –', 'Date Range', 'Job Status', 'Company Leads/Target Dashboard', '–', 'KPIs –', 'Total Hi-pages Lead today', 'Total Hi-pages Lead this month', 'Total OneFlare Lead today', 'Total OneFlare Lead this month', 'Total Google Ads Lead today', 'Total Google Ads Lead this month', 'Total Facebook Ads Lead today', 'Total Facebook Ads Lead this month', 'Company Daily Sales Target', 'Company Monthly Sales Target', 'Filters/Slicer –', 'Date Range', 'Job Status', 'Tools used', 'Wink Report', 'Language/techniques used', 'Python', 'Web Scraping', 'Skills used', 'Data Analytics', 'Data Visualization', 'Programming in Python', 'Data Structure & Algorithm', 'Web Scraping', 'File Handling', 'What are the technical Challenges Faced during Project Execution', 'Merging reports from different data sources', ': Faced the issue of making the cross report from different data sources.', 'Take live parameter input daily in Dashboards from User', ': Taking live user parameter input daily to feed in Wink report Dashboard. So that dashboard KPIs can change accordingly.', 'How the Technical Challenges were Solved', 'Merging reports from different data sources', ': Resolved this issue by using merge report configuration. Using this we were able to join tables from different data sources like – Left join , Right join , Union etc.', 'Take live parameter input daily in Dashboards from User', ': To resolve this issue , we added a custom field in reports with input tag. Users can enter their parameter in this custom field and all dashlets in the dashboard would update automatically.', 'Project Snapshots', 'Project Video', 'Previous article', 'React Native Apps in the Development Portfolio', 'Next article', 'QuickBooks dashboard to find patterns in finance, sales, and forecasts', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2129,https://insights.blackcoffer.com/react-native-apps-in-the-development-portfolio/,"React Native Apps in the Development Portfolio - Blackcoffer Insights-['Here are the list of react native apps developed by the team and the resources:', 'https://itunes.apple.com/us/app/truckmap-truck-gps-routes/id1198422047?mt=8', 'https://play.google.com/store/apps/details?id=com.truckmap.truckmap', 'https://play.google.com/store/apps/details?id=com.verifai.standalone', 'https://apps.apple.com/nl/app/verifai/id1504214033', 'https://apps.apple.com/de/app/meetlist-lokale-aktivit%C3%A4ten/id1439183715', 'https://play.google.com/store/apps/details?id=de.mlug.meetlist', 'https://play.google.com/store/apps/details?id=com.payroo.employee', 'https://play.google.com/store/apps/details?id=com.vahcare', 'https://play.google.com/store/apps/details?id=com.candorivf', 'Previous article', 'A Leading Law Firm in the USA, Website SEO & Optimization', 'Next article', 'Marketing, sales, and financial data business dashboard (Wink Report)', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2130,https://insights.blackcoffer.com/a-leading-firm-website-seo-optimization/,"A Leading Law Firm in the USA, Website SEO & Optimization - Blackcoffer Insights-['Client Background', 'Client:', 'A leading marketing firm in the USA', 'Industry Type:', 'Marketing', 'Services:', 'Marketing consulting', 'Organization Size:', '100+', 'Project Objective', 'Connect website to Search Console, Google Analytics and Facebook Pixel through Google Tag Manager.', 'Fix SEO of the website.', 'Project Description', 'Connecting website to Google Search Console, Google Analytics and Facebook Pixel through Google Tag Manager.', 'Fixing SEO of the website.', 'Our Solution', 'Website connected to Google Search Console, Google Analytics and Facebook Pixel successfully.', 'Fixed the', 'meta description error', 'broken link error', '404 error, etc.', 'Tools used', 'Squarespace', 'Google Tag Manager', 'Google Analytics', 'Google Search Console', 'Language/techniques used', 'JavaScript', 'Skills used', 'Squarespace', 'Google Tag Manager', 'Google Analytics', 'Google Search Console', 'JavaScript', 'Project Snapshots', 'Project website URL', 'https://www.keepingorlandomoving.com/', 'Previous article', 'A Leading Hospitality Firm in the USA, Website SEO & Optimization', 'Next article', 'React Native Apps in the Development Portfolio', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2131,https://insights.blackcoffer.com/a-leading-hospitality-firm-in-the-usa-website-seo-optimization/,"A Leading Hospitality Firm in the USA, Website SEO & Optimization - Blackcoffer Insights-['Client Background', 'Client:', 'A leading marketing firm in the USA', 'Industry Type:', 'Marketing', 'Services:', 'Marketing consulting', 'Organization Size:', '100+', 'Project Objective', 'Working On-page SEO of the pages to make it user-friendly and feasible for crawlers to make the site indexing better.', 'Project Description', 'Firstly, exploring the Liverez as it was a new platform then, performing intermediate SEO like page titles and description, completing word count, alt. text and removing duplicate page title and description.', 'Our Solution', 'To increase the organic traffic of the site and improve the insights.', 'Project Deliverables', 'There was a bit of improvement in the traffic of the site.', 'Tools used', 'Brightlocal.com, Yoast SEO, Grammarly', 'Language/techniques used', 'Basic HTML', 'Skills used', 'ON-page SEO', 'Project Snapshots', 'Project website url', 'https://www.missionbeach.com/', 'Previous article', 'A Leading Firm in the USA, Website SEO & Optimization', 'Next article', 'A Leading Law Firm in the USA, Website SEO & Optimization', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2132,https://insights.blackcoffer.com/a-leading-firm-in-the-usa-website-seo-optimization/,"A Leading Firm in the USA, Website SEO & Optimization - Blackcoffer Insights-['Client Background', 'Client:', 'A leading marketing firm in the USA', 'Industry Type:', 'Marketing', 'Services:', 'Marketing consulting', 'Organization Size:', '100+', 'Project Objective', 'Fixing On-Page SEO of the website', 'Project Description', 'Fixing On-Page SEO contains things like title, meta description, image-alt text, broken links, 404 error page, multiple h1 tag in one page, duplicate title/description, dynamic URL, sparse content page (word count <500), etc.', 'Our Solution', 'Fixed all the possible solutions that we can do for improving the SEO health score.', 'Fixed, image-alt text error, title, meta description, broken links, dynamic URL, 404 error page, sparse content pages, contact information on all pages, connecting website with Google search console.', 'Tools used', 'Ahrefs', 'WordPress', 'Google Search Console', 'Language/techniques used', 'HTML', 'Redirection plugin', 'Skills used', 'HTML', 'WordPress', 'Google Search Console', 'Project Snapshots', 'Project website URL', 'URL', 'https://www.jupiteroutdoorcenter.com/', 'Home', 'Previous article', 'A Leading Musical Instrumental, Website SEO & Optimization', 'Next article', 'A Leading Hospitality Firm in the USA, Website SEO & Optimization', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2133,https://insights.blackcoffer.com/a-leading-musical-instrumental-website-seo-optimization/,"A Leading Musical Instrumental, Website SEO & Optimization - Blackcoffer Insights-['Client Background', 'Client:', 'A leading marketing firm in the USA', 'Industry Type:', 'Marketing', 'Services:', 'Marketing consulting', 'Organization Size:', '100+', 'Project Objective', 'Connect website to Google Tag Manager.', 'Remove error.', 'Project Description', 'Remove all previously added code and add new code for connecting to Google Tag Manager.', 'Remove 5xx error from the website.', 'Our Solution', 'Website connected to Google Tag Manager successfully.', 'Removed 5xx error.', 'Tools used', 'Google Tag Manager', 'WordPress', 'Language/techniques used', 'JavaScript', 'Skills used', 'WordPress', 'Google Tag Manager', 'Project website URL', 'URL:', 'https://www.hamiltonpianoco.com/', 'Previous article', 'A Leading Firm in the USA, SEO and Website Optimization', 'Next article', 'A Leading Firm in the USA, Website SEO & Optimization', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2134,https://insights.blackcoffer.com/a-leading-firm-in-the-usa-seo-and-website-optimization/,"A Leading Firm in the USA, SEO and Website Optimization - Blackcoffer Insights-['Client Background', 'Client:', 'A leading marketing firm in the USA', 'Industry Type:', 'Marketing', 'Services:', 'Marketing consulting', 'Organization Size:', '100+', 'Project Objective', 'Connect website to Search Console. Add Call Rail Code', 'Project Description', 'Connecting website to Google Search Console through Google Tag Manager.', 'Connect website with CallRail.', 'Our Solution', 'Website connected to Google Search Console successfully.', 'Added CallRail code to the website.', 'Tools used', 'kvCore', 'Google Tag Manager', 'Google Search Console', 'CallRail', 'Language/techniques used', 'JavaScript', 'Skills used:', 'kvCore', 'Google Tag Manager', 'Google Search Console', 'CallRail', 'Javascript', 'Project Snapshots', 'Project website URL:', 'https://www.12stonesnwa.com/', 'Previous article', 'Immigration Datawarehouse & AI-based recommendations', 'Next article', 'A Leading Musical Instrumental, Website SEO & Optimization', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2135,https://insights.blackcoffer.com/immigration-datawarehouse-ai-based-recommendations/,"Immigration Datawarehouse & AI-based recommendations - Blackcoffer Insights-['Client Background', 'Client:', 'A leading business school worldwide', 'Industry Type:', 'R&D', 'Services:', 'R&D, Innovation', 'Organization Size:', '100+', 'Project Objective', 'Objective of this project is to research and collect news article data sourcing from Canada, based on the keyword.', 'Project Description', 'There were 3 phases of the project.', 'Phase 1', '– Data collection and selection', 'Data related to anyone coming to Canada (new comers)', 'Data related to anyone coming to Canada (new comers)', 'Canadian policy to new comers', 'i.e. from any country to Canada', 'Data containing News, press, think tanks, government policy documents, or research institutions releasing the news or press about', 'The news source should be limited to Canada only', 'Time span- 2005 to 2021', 'Output- Excel having URLs or the documents along with the source type, keywords, and date on which that article is posted.', 'Phase 2', '– Documents text data extraction', 'Develop tool to collect and extract data from each URL.', 'Clean and save the texts in the text documents', 'Phase 3', '– Textual Analysis', 'Sentiment Analysis', 'Analysis of readability', 'Topic modelling', 'Our Solution', 'We provide them with completed Phase 1 in an excel sheet and ongoing samples for Phase 2. Also work for Phase 3 has been started in between to complete the Project as soon as possible in a best way.', 'Project Deliverables', 'There is a file containing excel sheet and a word file containing a summary of the dataset and folders of text files containing samples of data from Phase 2.', 'Tools used', 'Python, PyCharm, Jupyter Notebook, Microsoft Excel, Google Chrome is used to complete different phases of this project', 'Language/techniques used', 'Python programming language is used to do Web Scraping, Automation, Data Engineering in this project.', 'Models used', 'SDLC is a process followed for a software project, within a software organization. It consists of a detailed plan describing how to develop, maintain, replace and alter or enhance specific software. The life cycle defines a methodology for improving the quality of software and the overall development process.', 'We are using Iterative Waterfall SDLC Model as we have to follow our development of software in phases and we also need feedback on every step of the development of our project so as to keep track of the occurring changes with every step.', 'Figure 1 SDLC Iterative Waterfall Model', 'Skills used', 'Data scraping, cleaning, pre-processing and creating data pipelines are used in this project.', 'Databases used', 'We used the traditional way of storing the data i.e file systems.', 'What are the technical Challenges Faced during Project Execution', 'There were a lot of challenges we faced during the project execution.', 'As on the internet, raw data is available to us. So, to search for the important data specifically related to Canada only, with a lot of keywords was a challenging part for us.', 'Then, if we somehow manage to do the task by automating it upto some extent only, we are required to find the dates of the articles, news, think tanks, documents etc, that was also a challenging part.', 'While working on Phase 2, we need to scrape the data from the URLs, so sometimes, the news articles were removed from the website, which we earlier took in our datasets which cause problems in extracting the data.', 'Then cleaning the webpages was also challenge for us, because this project is for research, so data is important to us. So, it was difficult to take only that data from website which we require and are most important.', 'How the Technical Challenges were Solved', 'Below are the points used to solve the above technical challenges-', 'We used sitemaps of websites to find different articles that we require according to the keywords, manual research was done to find out which URL will solve the purpose. Manual checking of results of automation tools, that we created, was done.', 'To find the dates of the articles, we wrote multiple regular expressions, that will find the match for the dates that we need, also manual checking was done after that.', 'To scrape removed webpages, we used WayBack machine or google archives, which stores all the deleted webpages.', 'To clean the data, we filtered out various HTML tags, classes, ids by using regex, manual research.', 'Project Snapshots', 'Previous article', 'Lipsync Automation for Celebrities and Influencers', 'Next article', 'A Leading Firm in the USA, SEO and Website Optimization', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2136,https://insights.blackcoffer.com/lipsync-automation-for-celebrities-and-influencers/,"Lipsync Automation for Celebrities and Influencers - Blackcoffer Insights-['Client Background', 'Client:', 'A leading tech firm in India', 'Industry Type:', 'Entertainment', 'Services:', 'B2C', 'Organization Size:', '100+', 'Project Objective', 'To change the lipsing of the original video with the new replaced audio.', 'Project Description', 'We needed to create an output video that will have the new lipsing according to the new replaced audio. Also we will have to change the actual audio with the new audio with automated editing.', 'Our Solution', 'We have created two different files which will perform 2 different operations 1', 'st', 'will replace the original audio with new and extract only video from original. 2', 'nd', 'will take the muted video and replaced audio and we will get the output of the new replaced audio lipsync. This is done by pre-defined model Wav2Lip on github.', 'Project Deliverables', '2\xa0 google colab notebooks', 'Tools used', 'github', 'Google drive', 'Language/techniques used', 'Python 3.6', 'moviepy', 'ffmpeg', 'Models used', 'Wav2lip', 'Skills used', 'Python programming', 'Data science', 'Databases used', 'Provided by the company (Hrithik Roshan video files)', 'Project Snapshots', 'Project website url', 'https://colab.research.google.com/drive/18mlREpLmV9hj-uDfufkGJ_-m_E37Hct9?usp=sharing', 'https://colab.research.google.com/drive/1FZHvcVKyJxOUkUFI2auPt3vTOu4jh09K?usp=sharing', 'Previous article', 'Key Audit Matters Predictive Modeling', 'Next article', 'Immigration Datawarehouse & AI-based recommendations', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2137,https://insights.blackcoffer.com/key-audit-matters-predictive-modeling/,"Key Audit Matters Predictive Modeling - Blackcoffer Insights-['Client Background', 'Client:', 'A leading business school worldwide', 'Industry Type:', 'R&D', 'Services:', 'Research & Innovation', 'Organization Size:', '10000+', 'Project Objective', 'Do regression modeling on the data provided, cross-country determinants of Key Audit Matters (KAMs) and its usefulness to Investors and Debt Market Participants', 'Project Description', 'USEFULNESS TO EQUITY MARKETS', 'Examine whether the number and content of KAMs varies with country-level determinants.', 'Explore whether the usefulness of KAMs to investors varies with country level variables such as type of law, enforcement etc.', 'Examine whether adoption of the expanded auditor’s report associated with change in audit quality? Examine whether the content in the auditor’s report improves audit quality. Does this vary across countries?', 'Is the adoption of the expanded auditor’s report associated with a change in audit fees?', 'Explore whether the content of auditor report moderates the usefulness of KAMs to investors (also check by country-level variables)', 'Can the number and content of KAMs be used to predict restatements (2017 onwards)?', 'In order to do the analysis and hypothesis testing, create a mapping to divide the audits into sub category and category according to the sub category and category provided in the question document. Clean the data before proceeding and calculate variables ABRET, ABVOL, CAR and CAAR according to the description provided.', 'Our Solution', 'Created a mapping for key audit matters to label the sub category and category of the audit for further analysis and merging with other datasets on the basis of the unique keys to create a final dataset we can use to calculate and do the hypothesis testing.', 'Calculation of variable ABRET and ABVOL is proceeded by firstly arranging the data by unique key and then the date of the data to get the sorted data. Cleaning is done on the data by removing the repetitive entries from the dataset and then selected the data around the date for which the variable is to be calculated. Similarly, calculated ABVOL in which extracted the data around the annual report filing date and mean value for 40 days interval that ends 21 days before earning announcement dates.', 'Couldn’t proceed because dataset provided by the client was incomplete in order to calculate ABRET.', 'Language/techniques used', 'R language to create mapping for the key audit matters and save data set for question 1.', 'Python pandas library to deal with dates and extract data around annual report filing date.', 'Skills used', 'Data mapping, data cleaning, data manipulation, debugging', 'Databases used', 'Key audit matter', 'GDP rule law', 'Audit fee', 'Trading data', 'Earning date', 'Report filing date', 'What are the technical Challenges Faced during Project Execution', 'Dataset provided by the client was too big and made my system slow when the data is loaded in the environment. Too many datasets and variables made it bit difficult to understand and time taking.', 'How the Technical Challenges were Solved', 'Calculated the number of unique identifiers in the large dataset and sorted those. Then selected the data for 1 unique identifier and sorted dates for it and append it to the dataframe and saved group of such unique identifiers to reduce the size of the dataset and performed the calculations in loop.', 'To tackle the difficulty of understanding the data I made a document tracking all the columns or variables present in the data.', 'Previous article', 'Splitting of Songs into its Vocals and Instrumental', 'Next article', 'Lipsync Automation for Celebrities and Influencers', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2138,https://insights.blackcoffer.com/splitting-of-songs-into-its-vocals-and-instrumental/,"Splitting of Songs into its Vocals and Instrumental - Blackcoffer Insights-['Client Background', 'Client:', 'A leading Entertainment firm in the USA', 'Industry Type:', 'Entertainment', 'Services:', 'Music', 'Organization Size:', '100+', 'Project Objective', 'The objective of this project is to split a song into its vocals and instrumental.', 'Project Description', 'The project aims at taking a Hindi language song as input and separating the vocals(lyrics) from the instrumental music of the song. Save both the vocals and instrumental files separately as output.', 'Our Solution', 'I have used Python programming language for this project. The use of a Python library called Spleeter developed by Deezer has been made to achieve our goal.', 'Spleeter', 'is\xa0Deezer\xa0source separation library with pretrained models written in Python\xa0and uses\xa0Tensorflow. It makes it easy to train source separation model (assuming you have a dataset of isolated sources), and provides already trained state of the art model for performing various flavor of separation :', 'Vocals (singing voice) / accompaniment separation (', '2 stems', ')', 'Vocals / drums / bass / other separation (', '4 stems', ')', 'Vocals / drums / bass / piano / other separation (', '5 stems', ')', '2 stems and 4 stems models have\xa0high performance\xa0on the', 'musdb', 'dataset.', 'Spleeter', 'is also very fast as it can perform separation of audio files to 4 stems 100x faster than real-time when run on a GPU.', 'Project Deliverables', 'Python tool that takes Hindi song as input and gives two audio files as output: vocals file and instrumental file.', 'Language/techniques used', 'Python', 'Models used', '2 Stems model', 'Skills used', 'Advanced Python programming', 'Project Snapshots', 'Previous article', 'AI and ML technologies to Evaluate Learning Assessments', 'Next article', 'Key Audit Matters Predictive Modeling', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2139,https://insights.blackcoffer.com/ai-and-ml-technologies-to-evaluate-learning-assessments/,"AI and ML technologies to Evaluate Learning Assessments - Blackcoffer Insights-['Client Background', 'Client:', 'A leading EduTech firm in the USA', 'Industry Type:', 'EduTech', 'Services:', 'Educations. Training', 'Organization Size:', '1000+', 'Project Objective', 'Confirmation / Identification of data that can be used / obtained without bias.', 'Understanding the Actions that are required to be performed post analytics.', 'Converting the data into metrics using formulae that can be used to conduct the analysis.', 'Project Description', 'It is a culture management platform that uses learning as the fundamental mode of communication. The platform requires an Analytics portion that captures a variety of data related to the interaction of the learner with content, assessments, engagements and forums to create personalized learning plans for each user to increase the effectiveness of learning and its retention which together make an impact on the overall productivity of the learner and the organization.', 'Our Solution', 'We helped the client in deciding the data required for the analysis process. We came up with the appropriate models for various tasks and interpretations of how the data will be collected and analysed for the initial response, final response, retention, proficiency, and learning intent of the user. We designed the models in such a way that one can perform seamlessly grading for each question type (based on difficulty level) and at a different hierarchical level (sub-section, section, training, and so on). We knew that each user has its unique aptitude level (basic, intermediate, and advanced) and keeping that in my mind, we incorporated those aptitude levels in our analytics too. Moreover, we integrated the grade and time factor into the analysis so that more points are allotted for comparatively tough questions and quick responses, respectively.', 'Project Deliverables', 'MS Excel sheet, Google spreadsheets with proper tables and visualizations.', 'Tools used', 'Jupyter notebook, MS Excel, Google Spreadsheets.', 'Language/techniques used', 'Python.', 'Skills used', 'Data science and analytics.', 'Databases used', 'Generated our data through data simulation.', 'What are the technical Challenges Faced during Project Execution?', 'Data analytics is all about analysing and finding patterns in the data that already exist or are getting generated in real-time. However, this project is in the budding stage, and we had no data to start our analysis. Moreover, this project is novel, and the dataset that meets our requirements was nearly impossible to find online.', 'How the Technical Challenges were Solved', 'We performed data simulation techniques and tried to generate the data as authentic as possible using some libraries in python and random functions in spreadsheets. We also generated the data manually at a small scale, but we made sure that we are including every human factor in it.', 'Project Snapshots (Minimum 10 Pictures)', 'Previous article', 'Datawarehouse, and Recommendations Engine for AirBNB', 'Next article', 'Splitting of Songs into its Vocals and Instrumental', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2140,https://insights.blackcoffer.com/datawarehouse-and-recommendations-engine-for-airbnb/,"Datawarehouse, and Recommendations Engine for AirBNB - Blackcoffer Insights-['Client Background', 'Client:', 'A leading hotels chain in the USA', 'Industry Type:', 'Real Estate, Hospitality', 'Services:', 'Hostpitality', 'Organization Size:', '1000+', 'Project Objective', 'To download the data from the servers using Cyberduck on the daily basis and perform data engineering on it.', 'Project Description', 'Firstly, download the property and forward files from the server', 'Secondly, From the property master file a new data set was created with the conditions that the Bedrooms from Property file should be 5 or more or Max Guests from Property File should be 16 or more and City from Property File should be Sevierville or Pigeon Forge or Gatlinburg.', 'In the forward file only those with status = R were kept and the other data was removed.', 'Finally, forward file was merged with the new data set on ‘Property ID’ i.e., keeping only those forward data with the common ‘Property ID’ and City, Bedrooms, Max Guests columns from the new dataset was added to the forward file.', 'Our Solution', 'We created a Python Script which performs the task and create property and forward master files, which we deliver to client on weekly basis.', 'Project Deliverables', 'Two csv files named property master file and forward master file to be delivered weekly after applying various steps.', 'Tools used', 'PyCharm, PowerBi, Cyberduck, Microsoft Excel.', 'Language/techniques used', 'Python Programming Language is used to create scripts performing Data Manipulation in different files.', 'Models used', 'SDLC is a process followed for a software project, within a software organization. It consists of a detailed plan describing how to develop, maintain, replace and alter or enhance specific software. The life cycle defines a methodology for improving the quality of software and the overall development process.', 'We are using Iterative Waterfall SDLC Model as we have to follow our development of software in phases and we also need feedback on every step of the development of our project so as to keep track of the occurring changes with every step.', 'Figure 1 SDLC Iterative Waterfall Model', 'Skills used', 'Skills such as Data Pre-processing, cleaning, and data manipulation are used in this project.', 'Databases used', 'We used traditional way of storing the data i.e file systems.', 'Web Cloud Servers used', 'Cyberduck, which is a libre server and cloud storage browser for Mac and Windows with support for FTP, SFTP, WebDAV, Amazon S3 etc, was used in this project with Amazon S3 servers.', 'What are the technical Challenges Faced during Project Execution?', 'Data to be processed was very big in size, so space complexity was a challenge in this project', 'How the Technical Challenges were Solved', 'To solve the space complexity issues, we tried PowerBi, but now time complexity arises.', 'Then we did processing in chunks, by reducing file sizes to avoid memory errors.', 'Project Snapshots (Minimum 10 Pictures)', 'Previous article', 'Real Estate Data Warehouse', 'Next article', 'AI and ML technologies to Evaluate Learning Assessments', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2141,https://insights.blackcoffer.com/real-estate-data-warehouse/,"Real Estate Data Warehouse - Blackcoffer Insights-['Client Background', 'Client:', 'A leading Real Estate firm in the EU', 'Industry Type:', 'Real Estate', 'Services:', 'Real Estate', 'Organization Size:', '1000+', 'Project Objective', 'The objective of this project is to build a data warehouse from a website given search and filter criteria.', 'Project Description', 'The objective of this project is to collect data from a website given search and filter criteria.', 'Data Brief:', 'Crawl all the information for the property adverts once a week and store them in a database.', 'Data language: English', 'Filters:', 'Federal States', 'Contains a list of the federal states in Germany to Crawl:', 'https://en.wikipedia.org/wiki/States_of_Germany', 'Categories to Crawl', 'Mieten Wohnung', 'Kaufen Wohnung', 'Kaufen Anlageobjekte', 'Kaufen Grundstück', 'Our Solution', 'We have developed a Python tool that crawls and scrapes all the apartment listings for all the states in Germany under each category namely: mieten wohnungen, kaufen wohnungen, kaufen anlageobjekte and kaufen grundstuck. The Scrapy library has been used to crawl and scrape. Beautiful soup could have also been used for the scraping purpose, but for the sake of consistency, Scrapy has been used for both purposes.', 'Scrapy is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.', 'Even though Scrapy was originally designed for', 'web scraping', ', it can also be used to extract data using APIs (such as', 'Amazon Associates Web Services', ') or as a general purpose web crawler.', 'Four Spiders have been created for each category to be scraped. Every spider crawls all the states in Germany and scrapes all the apartment listings for important data. Every spider creates a separate JSON file to store all its data. This data is then converted to CSV using another python script called “conversion”.', 'The python tool has been completely automated and only needs the “Controller” script to be run. The script also has the capability of running every two weeks automatically.', 'Project Deliverables', 'Four CSV files (one for each category):', 'Mieten Wohnungen.csv', 'Kaufen Wohnungen.csv', 'Kaufen Anlageobjekte.csv', 'Kaufen Grundstuck.csv', 'Language/techniques used', 'Python', 'Web Crawling & Scraping', 'Skills used', 'Data Scraping', 'Data Crawling', 'Advanced Python programming', 'Project Snapshots', 'Previous article', 'Traction Dashboards of Marketing Campaigns and Posts', 'Next article', 'Datawarehouse, and Recommendations Engine for AirBNB', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2142,https://insights.blackcoffer.com/traction-dashboards-of-marketing-campaigns-and-posts/,"Traction Dashboards of Marketing Campaigns and Posts - Blackcoffer Insights-['Client Background', 'Client:', 'A leading Marketing firm in the USA', 'Industry Type:', 'Marketing', 'Services:', 'Marketing consulting', 'Organization Size:', '100+', 'Project Objective', 'For the LinkedIn posts that received the highest engagement, which keywords, phrases, and hashtags were most commonly used and also view the data according to Impressions and Likes?', 'Project Description', 'We are testing AWS Comprehend. I performed a key phrase analysis of our LinkedIn posts. We have an output file. Now we need your help to visualize the data so that we can interpret it.', 'I also have the original export file from LinkedIn. I want to answer this business question:', 'For the LinkedIn posts that received the highest engagement, which keywords, phrases, and hashtags were most commonly used?', 'I want to match up Engagement Rate with key phrase analysis. The business question is this: For the LinkedIn posts that received the highest engagement, what were the most common keywords, phrases and hashtags used?', 'Beyond matching to Engagement Rate, please check if there is a way to also view the data according to Impressions and Likes.', 'Our Solution', 'Data Driven Dashboards which will give the summary of Most used words, keywords, Phrases and also Analysis of Posts as per their interaction with their audience.', 'Project Deliverables', 'Two Dashboard Links in which', 'First dashboard', 'represents Key Phrase analysis of the output by AWS Comprehend.', 'Second Dashboard', 'represents the Linked In data Analysis', 'Tools used', 'Python, Google Data studio', 'Language/techniques used', 'Python', 'Skills used', 'Python and Data Studio', 'Databases used', 'MongoDB', 'Web Cloud Servers used', 'Google Data Studio', 'What are the technical Challenges Faced during Project Execution', 'One of the major problem was to match the output of AWS Comprehend data with the data of excel sheet to find out which posts received maximum interactions and make a dashboard out of it.', 'How the Technical Challenges were Solved', 'Working on the output.json file in code editor and comparing it to the Linked In data sheet to check the accuracy of the output file with each post.', 'Project Snapshots (Minimum 10 Pictures)', 'Project website Url', '1 Key Phrase Analysis Dashboard', 'https://datastudio.google.com/reporting/efbabbff-55ba-4326-8133-78ae304aeb99', '2 Linked IN Data Analysis Dashboard', 'https://datastudio.google.com/reporting/3525e1c1-6c4f-4613-b260-d6e975fe1652', 'Previous article', 'Google Local Service Ads (LSA) Data Warehouse', 'Next article', 'Real Estate Data Warehouse', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2143,https://insights.blackcoffer.com/google-local-service-ads-lsa-data-warehouse/,"Google Local Service Ads (LSA) Data Warehouse - Blackcoffer Insights-['Client Background', 'Client:', 'A leading Marketing firm in the USA', 'Industry Type:', 'Marketing', 'Services:', 'Marketing consulting', 'Organization Size:', '100+', 'Project Objective', 'Automated tool to extract daily review data from Local Service Ads dashboard for all clients.', 'Project Description', 'Extracts data from a company’s Google LSA page for the last 24 hours', 'The data is uploaded to the Bigquery database called “LSA_Review_db”.', 'The script runs once a day and is deployed to Heroku by the name “lsa-daily-reviews”.', 'The script runs for all companies in the Google sheet “LSA Review Automation master file”.', 'The following data is uploaded:', 'Date', 'Company Name', 'Location', 'Total Reviews', 'Verified Reviews', 'Overall Star', 'Reviewer Name', 'Review Date', 'Reviewer Star', 'Reviewer Comment', 'Our Solution', 'Get list of companies to monitor along with their LSA URL', 'Use Selenium automated browsing to open the review page for each company.', 'Web scrape the data from the review page', 'Prepare report', 'Upload to database', 'Project Deliverables', 'An automated tool that runs daily and extracts and uploads review data for all companies.', 'Tools used', 'Selenium', 'Heroku', 'Sheets API', 'BigQuery', 'Language/techniques used', 'Python', 'Skills used', 'Data extraction, cleaning and summarising. Web scraping.', 'Databases used', 'BigQuery –\xa0 LSA_Review_db', 'Web Cloud Servers used', 'Heroku', 'What are the technical Challenges Faced during Project Execution', 'Using Selenium to automate web browsing since it takes a large amount of RAM.', 'How the Technical Challenges were Solved', 'Using the proper type of dynos and managing their allotment to lower both costs as well as memory usage.', 'Previous article', 'Google Local Service Ads Missed Calls and Messages Automation Tool', 'Next article', 'Traction Dashboards of Marketing Campaigns and Posts', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2144,https://insights.blackcoffer.com/google-local-service-ads-missed-calls-and-messages-automation-tool/,"Google Local Service Ads Missed Calls and Messages Automation Tool - Blackcoffer Insights-['Client Background', 'Client:', 'A leading Marketing firm in the USA', 'Industry Type:', 'Marketing', 'Services:', 'Marketing consulting', 'Organization Size:', '100+', 'Project Objective', 'A real time tool to send a report of missed calls and messages to the client.', 'Project Description', 'Extracts data from CallRail database for the last 5 minutes', 'All the calls which are marked as “missed” and all messages in the data are sent in the form of a report to the client.', 'The script runs every 5 minutes and is deployed to Heroku by the name “missed-messages”.', 'The data is collected only for the companies that are not marked in red in the “Missed Messages Notification Automation – Master File” sheet.', 'The following data is uploaded:', 'Company Name', 'Date', 'Time', 'Customer Name', 'Contact No.', 'Customer Location', 'Call Type', 'In case of messages:', 'Company Name', 'Date', 'Time', 'Customer Name', 'Contact No.', 'No. of messages', 'Direction (Inbound/Outbound)', 'Content', 'Our Solution', 'To provide data real time, schedule the tool to check for data every 5 minutes.', 'Extract data from CallRail', 'Filter out all answered calls', 'Prepare report', 'Get email ids from sheets', 'Send email through SendGrid', 'Project Deliverables', 'An automated tool which provides real time updates to the client along with all information about the call.', 'Tools used', 'Heroku', 'CallRail API', 'SendGrid', 'Sheets API', 'Language/techniques used', 'Python', 'Skills used', 'Data extraction, cleaning and summarising', 'Databases used', 'Google Big Query', 'Web Cloud Servers used', 'Heroku', 'What are the technical Challenges Faced during Project Execution', 'Sending correct reports only to the companies which are active', 'How the Technical Challenges were Solved', 'Using Google Sheet’s cell formatting in Python', 'Previous article', 'Marketing Ads Leads Call Status Data Tool to BigQuery', 'Next article', 'Google Local Service Ads (LSA) Data Warehouse', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2145,https://insights.blackcoffer.com/marketing-ads-leads-call-status-data-tool-to-bigquery/,"Marketing Ads Leads Call Status Data Tool to BigQuery - Blackcoffer Insights-['Client Background', 'Client:', 'A leading Marketing firm in the USA', 'Industry Type:', 'Marketing', 'Services:', 'Marketing consulting', 'Organization Size:', '100+', 'Project Objective', 'Prepare a daily report for the companies and upload it to BigQuery database. Data is from callrail and contains all call information about a company.', 'Project Description', 'Extracts data from CallRail database for the last 24 hours', 'The data is uploaded to the Bigquery database called “Call_Status_From_CallRail”.', 'The script runs once a day and is deployed to Heroku by the name “lsa-call-status-db”.', 'The script runs for all companies in the CallRail database.', 'The following data is uploaded:', 'Company Name', 'Status', 'Location', 'Customer Name', 'Call Date', 'Call Time', 'Contact No', 'Call Status', 'Call Lead', 'Our Solution', 'Use CallRail API to get data from database.', 'Run script daily', 'Filter out excess data', 'Prepare report', 'Upload to BigQuery', 'Project Deliverables', 'A working deployed automated tool that runs once a day in the morning hours and uploads the data to BigQuery database. Tool is monitored daily.', 'Tools used', 'Heroku', 'CallRail API', 'BigQuery', 'Sheets API', 'Language/techniques used', 'Python', 'Skills used', 'Data extraction, cleaning, and summarising', 'Databases used', 'BigQuery –\xa0 Call_Status_From_CallRail', 'Web Cloud Servers used', 'Heroku', 'What are the technical Challenges Faced during Project Execution', 'Ensuring proper data upload to database', 'How the Technical Challenges were Solved', 'Proper monitoring of tool post-deployment.', 'Previous article', 'Marketing Analytics to Automate Leads Call Status and Reporting', 'Next article', 'Google Local Service Ads Missed Calls and Messages Automation Tool', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2146,https://insights.blackcoffer.com/marketing-analytics-to-automate-leads-call-status-and-reporting/,"Marketing Analytics to Automate Leads Call Status and Reporting - Blackcoffer Insights-['Client Background', 'Client:', 'A leading Marketing firm in the USA', 'Industry Type:', 'Marketing', 'Services:', 'Marketing consulting', 'Organization Size:', '100+', 'Project Objective', 'Prepare a daily report for the companies and upload it to Google Sheets. Data is from callrail and contains all call information about a company.', 'Project Description', 'Extracts data from CallRail database for the last 24 hours', 'The data is uploaded to the Google sheet “Call status record”', 'The script runs once a day and is deployed to Heroku by the name “call-status-to-sheets”.', 'The script runs for all companies in the CallRail database.', 'The following data is uploaded:', 'Company Name', 'Status', 'Location', 'Customer Name', 'Call Date', 'Call Time', 'Contact No', 'Call Status', 'Call Lead', 'Our Solution', 'Use CallRail API to get data from database.', 'Run script daily', 'Filter out excess data', 'Prepare report', 'Upload to Google Sheets', 'Project Deliverables', 'A working deployed automated tool that runs once a day in the morning hours and uploads the data to Google Sheets. Tool is monitored daily.', 'Tools used', 'Heroku', 'CallRail API', 'BigQuery', 'Sheets API', 'Language/techniques used', 'Python', 'Skills used', 'Data extraction, cleaning and summarising', 'Databases used', 'Google Sheets – \xa0 Call status record', 'Web Cloud Servers used', 'Heroku', 'What are the technical Challenges Faced during Project Execution', 'Ensuring proper amendment of data to sheets without overwrite', 'How the Technical Challenges were Solved', 'Proper monitoring before final deployment', 'Previous article', 'CallRail, Analytics & Leads Report Alert', 'Next article', 'Marketing Ads Leads Call Status Data Tool to BigQuery', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2147,https://insights.blackcoffer.com/callrail-analytics-leads-report-alert/,"CallRail, Analytics & Leads Report Alert - Blackcoffer Insights-['Client Background', 'Client:', 'A leading Marketing firm in the USA', 'Industry Type:', 'Marketing', 'Services:', 'Marketing consulting', 'Organization Size:', '100+', 'Project Objective', 'Prepare an annual report for the companies and upload it to database. Data is from callrail and contains call analytics.', 'Project Description', 'Extracts data from CallRail database for the last 1 year.', 'The data is uploaded to BigQuery database “lead_report_alert_callrail”', 'The script runs once a year and is deployed to Heroku by the name “lead-report-alert”.', 'Currently, the script is programmed to run for only 2 companies (on a trial basis) – Capital Law Firm and Wilshire Law Firm.', 'The following data is uploaded:', 'Company Name', 'No. of calls answered', 'No. of calls missed', 'No. of calls abandoned', 'No. of calls to voicemail', 'Total Calls', 'Our Solution', 'Use CallRail API to get data from database.', 'Set time window to be one year.', 'Filter out excess data', 'Prepare report', 'Upload to BigQuery', 'Project Deliverables', 'A working deployed automated tool that runs once a year in the morning hours and uploads the data to BigQuery. Tool is in prototype phase and hence is operational for 2 companies.', 'Tools used', 'Heroku', 'CallRail API', 'BigQuery', 'Language/techniques used', 'Python', 'Skills used', 'Data extraction, cleaning and summarising', 'Databases used', 'BigQuery –\xa0 lead_report_alert_callrail', 'Web Cloud Servers used', 'Heroku', 'What are the technical Challenges Faced during Project Execution', 'Working on a large amount of data since a year’s data contains hundred of thousands of records', 'How the Technical Challenges were Solved', 'Optimized code for faster processing.', 'Previous article', 'Marketing Tool to Notify Leads to Clients over Email and Phone', 'Next article', 'Marketing Analytics to Automate Leads Call Status and Reporting', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2148,https://insights.blackcoffer.com/marketing-automation-tool-to-notify-lead-details-to-clients-over-email-and-phone/,"Marketing Tool to Notify Leads to Clients over Email and Phone - Blackcoffer Insights-['Client Background', 'Client:', 'A leading Marketing firm in the USA', 'Industry Type:', 'Marketing', 'Services:', 'Marketing consulting', 'Organization Size:', '100+', 'Project Objective', 'Prepare a daily report for data from Local Service Ads dashboard and email to client.', 'Project Description', 'Extracts data from the LSA dashboard for the last 24 hours.', 'The data is sent to the client email in the form of a daily report using SendGrid.', 'The script runs every morning and is deployed to Heroku by the name “lead-details-to-email”.', 'The data is collected only for the companies that are not marked in red in the “Missed Messages Notification Automation – Master File” sheet.', 'The following data is uploaded:', 'Number of Leads', 'Cost Per Lead', 'Lead Type', 'Dispute amount to be approved', 'Dispute amount approved', 'Cost per Call', 'Our Solution', 'Use LSA API to extract data.', 'Clean the data to make it readable and dispose the data not needed.', 'Get the email id of each company from the given Sheet', 'Send an email to the client using SendGrid', 'Deploy to Heroku', 'Project Deliverables', 'A working deployed automated tool that runs everyday in the morning hours and sends a report to the client. Tool is monitored everyday.', 'Tools used', 'Heroku', 'LSA API', 'SendGrid', 'Sheets API', 'Language/techniques used', 'Python', 'Skills used', 'Data extraction, cleaning, and summarising', 'Databases used', 'Data is not stored and is sent directly to the client', 'Web Cloud Servers used', 'Heroku', 'What are the technical Challenges Faced during Project Execution', 'Ensuring a company’s data does not go to another company', 'How the Technical Challenges were Solved', 'Testing on multiple dummy email ids', 'Previous article', 'Data ETL: Local Service Ads Leads to BigQuery', 'Next article', 'CallRail, Analytics & Leads Report Alert', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2149,https://insights.blackcoffer.com/data-etl-local-service-ads-leads-to-bigquery/,"Data ETL: Local Service Ads Leads to BigQuery - Blackcoffer Insights-['Client Background', 'Client:', 'A leading Marketing firm in the USA', 'Industry Type:', 'Marketing', 'Services:', 'Marketing consulting', 'Organization Size:', '100+', 'Project Objective', 'Upload daily data from Google Local Service Ads dashboard to BigQuery database.', 'Project Description', 'Extracts data from LSA dashboard for the last 24 hours.', 'The data is uploaded to BigQuery database “lsa_lead_daily_data”', 'The script runs every morning and is deployed to Heroku by the name “lead-details-to-db”.', 'The data is collected only for the companies that are not marked in red in the “Missed Messages Notification Automation – Master File” sheet.', 'The following data is uploaded:', 'Number of Leads', 'Cost Per Lead', 'Lead Type', 'Dispute amount to be approved', 'Dispute amount approved', 'Cost per Call', 'Our Solution', 'Use LSA API to extract data.', 'Clean the data to make it readable and dispose the data not needed.', 'Upload data to a BigQuery database everyday at a fixed time.', 'Deploy to Heroku to run the script everyday.', 'Project Deliverables', 'A working deployed automated tool that runs everyday in the morning hours and uploads a report to database. Tool is monitored everyday.', 'Tools used', 'Heroku', 'LSA API', 'BigQuery API', 'Sheets API', 'Language/techniques used', 'Python', 'Skills used', 'Data extraction, cleaning and summarising', 'Databases used', 'BigQuery –\xa0 lsa_lead_daily_data', 'Web Cloud Servers used', 'Heroku', 'What are the technical Challenges Faced during Project Execution', 'Making sure that the data uploaded is for the right company.', 'How the Technical Challenges were Solved', 'Monitoring daily logs and uploads for some time and making sure data was correct', 'Previous article', 'Marbles Stimulation using python', 'Next article', 'Marketing Tool to Notify Leads to Clients over Email and Phone', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2150,https://insights.blackcoffer.com/marbles-stimulation-using-python/,"Marbles Stimulation using python - Blackcoffer Insights-['Client Background', 'Client:', 'A leading consulting firm in the USA', 'Industry Type:', 'IT Consulting', 'Services:', 'Consultanting', 'Organization Size:', '100+', 'Project Objective', 'For all 4 cases, use a random number generator that will give you numbers between 1 & a million [1,000,000].\xa0 Whatever generator you use, make sure to adjust the numbers so that they are between 1 & 1,000,000 distributed randomly.', 'For all tasks, we will have 5 colors, for example in Task 1, when the random number selected is between 1 & 5857 choose a bright color that is easily visible [I have called it Br. Clr. 1], for numbers between 5858 & 8678 choose another bright color [Br. Clr. 2], for numbers between 8679 & 11500 choose B (Blue), for numbers between 11501 & 50,000 choose R (Red), and > 50,000 choose G (Green). Simulate these 4 Task scenarios and represent them in a Table (1000 x 32) and collect statistics at the end. Replicate the simulation exercises for each Task with 3 different initial seed numbers. Likewise for 16 other Tasks.', 'Our Solution', 'Task involves creating 20 excel files running a Python Script in Jupyter Notebook which contains certain integer ranges indicating certain values and some other criteria the Random number range [1 to 1 million].', 'There are 20 Different tasks which have different conditions based on which need to form. Simulate these 20 Tasks and represent them in a Table (1000 x 32) and collect statistics at the end. Replicating the simulation exercises for each Task with 3 different initial seed numbers.', 'Then using the Find and Replace tab of excel to make it in the correct format with proper color. Data Representation in particular format and formatting colors, Text based on condition passed within excel.', 'Project Deliverables', 'Excel File', 'Tools used', 'JupyterNB', 'Sublime Text', 'MS Excel', 'Language/techniques used', 'Python', 'Models used', 'No Software model is being Used to Solve this Project', 'Skills used', 'Python programming', 'MS Excel Formatting', 'Databases used', 'No database were used stored complete data in MS Excel', 'Web Cloud Servers used', 'No cloud servers were used for this project', 'What are the technical Challenges Faced during Project Execution', 'Formatting Excel Files', 'How the Technical Challenges were Solved', 'Formatting Excel Files', 'Discovered a lot of Shortcuts Available within Excel to deal with Data Representation in particular format and learned about formatting colors, Text based on condition passed within excel.', 'Replication and Selecting Rows and Columns with shortcuts and in simplest way possible, transposing selected data and many more.', 'Project Snapshots', 'Figure 1: Sample Output File for Task 12 stimulation 3', 'In total there were 16 conditional tasks all of them had 3 stimulation which needed to be performed.', 'Previous article', 'Stocktwits Data Structurization', 'Next article', 'Data ETL: Local Service Ads Leads to BigQuery', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2151,https://insights.blackcoffer.com/stocktwits-data-structurization/,"Stocktwits Data Structurization - Blackcoffer Insights-['Client Background', 'Client:', 'A leading financial institution in the USA', 'Industry Type:', 'Financial services & Consulting', 'Services:', 'Financial consultant', 'Organization Size:', '100+', 'Project Objective', '>To process two json file stocktwits_legacy_msg_2015_10.txt (file size = 2 GB) & stocktwits_legacy_msg_2015_10.txt (file size = 3.5 GB).', '>To handle Nested Json for both files and after conversion into one merged Data Frame need to perform Data Structurization.', '>While accessing a Json file in JupyterNB, I need to perform Chunking as the file size is bigger and it is in json format to avoid PC standstill.', '>After Data Preprocessing I need to perform Exploratory Data Analysis on that Data.', '> Conditional Programming to deal with Data Transferring to a particular folder based on the column values.', 'Project Description', 'During the training period I was involved with 2 live projects, One project named ‘Stocktwits Data Structurization’ in which I have to process huge JSON Data which was already obtained the size of data was nearly 5 GB need to process the data by chunking with chunk size = 20000 rows at a time. The file has nested JSON data within it’s attributes so abstracts data from the nested columns into a new dataframe. Completed handling complex nested json formed columns abstracted from nested json. Then need to Handle the missing data by mapping it with another index dataset further missing values for certain attributes were handled by mean value and 0 substitution. This task involves numerous pandas operations along with multiple python functions. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes.', 'Our Solution', 'Worked on Accessing Json Data, done tree Analysis on Json Sample data.', 'Both the File was too big for reading and applying some Python Code in JupyterNb, so performed chunking of stocktwits_legacy_messages_2015_10.txt\xa0 with chunk size = 20000 rows at a time. Similarly trying for the other file.', 'Created a list of all the chunked files of Json Data & Concat all the files in that list.', 'The File has Nested Json data within it’s attributes so abstracted data from the nested columns into a new DataFrame. Completed handling complex nested json formed columns abstracted from nested json.', 'Renamed the columns with identification. (Eg: ‘id’ as ‘entities_id’) likewise for others. So that while merging the data doesn’t create any issue. Completed forming Preprocessed csv file for 1st json file which\xa0 Output2015.csv.', 'For Second file size was > 3gb so splitted the file into ten parts and then individually solved nested json for all these parts like done in the 1st file finally concat them into one, then handled columns arrangements and removed unwanted columns and finally removed dictionary representation from entity_sentiments column. Completed forming Preprocessed csv file for 2nd json file which is Output_Stocktwits_2017.csv.', 'The cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Conditional Programming to deal with Data Transferring to a particular folder based on the column values.', 'Project Deliverables', 'Categorized Preprocessed CSV Files', 'Python Script', 'iPython NB with comments on each performed code.', 'Tools used', '● Jupyter Notebook', '● Anaconda', '● Notepad++', '● Sublime Text', '● Brackets', '● JsonViewer', 'Language/techniques used', '● Python Programming', 'Models used', 'My project ‘Stocktwits Data Structurization’ developed with a software model which makes the project high quality, reliable and cost effective.', '● Software Model : RAD(Rapid Application Development model) Model', '● This project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.', '● Advantages of RAD Model:', 'o Changing requirements can be accommodated.', 'o Progress can be measured.', 'o Iteration time can be short with use of powerful RAD tools.', 'o Productivity with fewer people in a short time.', 'o Reduced development time.', 'o Increases reusability of components.', 'o Quick initial reviews occur.', 'o Encourages customer feedback.', 'o Integration from very beginning solves a lot of integration issues', 'Skills used', '● Data Mining', '● Data Wrangling', '● Data Visualization', '● Python Programming including OOPs and Exception Handling', 'Databases used', 'No Databases were used, all the data was stored on Google Drive and Local Device.', 'Web Cloud Servers used', 'No Cloud Server were used', 'What are the technical Challenges Faced during Project Execution', '● Handling Huge Data and Data Cleaning', '● JSON Data Serialization.', '● Solving Complex Nested JSON among the data provided.', 'How the Technical Challenges were Solved', '● Handling Huge Data and Data Cleaning', 'Solved by Breaking the Dataset into 10 stream parts as the data was too huge and was not able to read easily in Jupyter NB.', '● JSON Data Serialization', 'Solved by Data Chunking with chunk_size=20000 which means serialization of data with processing 20000 rows at a time.', '● Solving Complex Nested JSON among the data provided.', 'Viewed the Structure of the part of data in JSON Viewer then Changed the data in proper standard JSON Format. After Reading JSON Data Performing Normalization of Nested JSON data setting maximum level of normalization with specifying proper orient form. Then After Normalization remaining Unsolved Nested JSON was solved using Dictionary Conversions and Structuring the data.', 'Project Snapshots', 'Figure 1 Sample Input Dataframe After Converting Outer JSON', 'Figure 2 Sample Output Dataframe After Solving Nested JSON and Data Preprocessing', 'Previous article', 'How artificial intelligence can boost your productivity level?', 'Next article', 'Marbles Stimulation using python', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2152,https://insights.blackcoffer.com/sentimental-analysis-on-shareholder-letter-of-companies/,"Sentimental Analysis on Shareholder Letter of Companies - Blackcoffer Insights-['Client Background', 'Client:', 'A leading financial firm in the USA', 'Industry Type:', 'Financial services & Consulting', 'Services:', 'Financial consultant', 'Organization Size:', '100+', 'Project Objective', 'Project “Sentimental Analysis on Shareholder Letter of Companies” objective was to Predict the Sentiments columns Shareholder Letter in terms of Polarity and Subjectivity finally classification of data into positive, negative and neutral tone.', 'Project Description', 'The project ‘Sentimental Analysis on Shareholder Letter of US Companies’ task involved data cleaning on shareholder letters of different companies which includes lemmatization, lower case conversion, removing special character, \\n , \\t , punctuations, numbers & single character and tokenization. To generate polarity and subjectivity columns for the letter 1 & letter 2 columns using the Textblob library of NLTK. Based on the polarity categorizing it into positive, neutral\xa0 &\xa0 negative.', 'Our Solution', 'Letter Text Length Variation', 'Contraction mapping on dataset', 'Replacing missing value with some neutral tone string like None so that cleaning doesn’t generate any issue.', 'Data Cleaning and Preprocessing which involves :', 'i.\xa0 Lemmatisation', 'ii. lower case conversion', 'iii.\xa0 Removing Special character', 'iv.\xa0 Removing \\n , \\t etc', 'v.\xa0 remove punctuations, numbers & single character removal', 'vi.\xa0 forming list of letter data using tqdm', 'Tokenization and word count.', 'Used Textblob Library which is part of NLTK for Sentiment analysis.', 'Created Polarity and Subjectivity column for the Letter1 & Letter2 columns', 'Based on the polarity of letter 1 created a letter1_type column with values “positive” , “neutral”\xa0 &\xa0 “negative” category.', 'Project Deliverables', 'Output iPython File', 'Preprocessed Dataset', 'Tools used', '● Jupyter Notebook', '● Anaconda', '● Notepad++', '● Sublime Text', '● Brackets', '● Python 3.4', 'Language/techniques used', 'Python', 'Machine Learning', 'NLP (Natural Language Processing)', 'Models used', 'My project ‘Sentimental Analysis on Shareholder Letter of Companies’ developed with a software model which makes the project high quality, reliable and cost effective.', '● Software Model : Waterfall Model', '● For Project ‘Sentimental Analysis on Shareholder Letter of US Companies’ is a Waterfall Model as our model is not forming the loop from end to the start using Textblob which predicts Sentiments, Polarity and Subjectivity as the output following the Waterfall Model.', 'Skills used', 'Pandas Operations', 'Data Chunking and Integration', 'Data Visualization', 'Databases used', 'No Database is used to complete this project.', 'Web Cloud Servers used', 'No Web cloud Server was required for this work.', 'What are the technical Challenges Faced during Project Execution', 'I have worked before on tasks similar to this so there were no challenges faced but the data cleaning was a bit different and required time to complete.', 'How the Technical Challenges were Solved', 'As Discussed no technical Challenges were faced during this project.', 'Project Snapshots', 'Figure 1: Input Data Schema', 'Figure 2: Output Data Schema', 'Figure 3: Sample Input Dataset', 'figure 3 is pandas dataframe which was fetched from google cloud database there were 7 columns and 13290 rows.', 'Figure 4: Sample Output Dataset', 'figure 4 is output pandas dataframe after data cleaning and modeling of sentiment identification there are 13 columns and 13290 rows.', 'Figure 5: Sentiments assignment based on polarity', 'figure 5 represents the identification of sentiments and tone based on polarity and subjectivity. polarity>0 then sentiment type is positive,\xa0 if the polarity<0 sentiment type is negative and if the polarity=0 sentiment type is neutral.', 'Figure 6:\xa0 Histogram Representation of Length of Shareholder Letter 1', 'figure 6 is histogram plot between length of shareholder letter 1 among the final output dataset.', 'Figure 7:\xa0 Histogram Representation of Length of Shareholder Letter 2', 'figure 7 is Histogram plot between length of shareholder letter 2 among the final output dataset.', 'Figure 8: Flow Chart', 'Previous article', 'Population and Community Survey of America', 'Next article', 'How is AI used to solve traffic management?', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2153,https://insights.blackcoffer.com/population-and-community-survey-of-america/,"Population and Community Survey of America - Blackcoffer Insights-['Client Background', 'Client:', 'A leading marketing firm in the USA', 'Industry Type:', 'Marketing services & Consulting', 'Services:', 'Marketing consultant', 'Organization Size:', '100+', 'Project Objective', 'Project ‘Population and Community Survey of America’ objective were to perform Data Abstraction, Data Structurization, Data Preprocessing, Data Cleaning, and Combining Data from all the years listed and finally presenting insights of the data by Exploratory Data Analysis.', 'Project Description', 'For Project ‘Population and Community Survey of America’ task involved fetching json and unformatted csv data from numerous web links further needed to process data, handling nested JSON, data conversion of JSON data in dataframe, performing certain pandas operation for feature selection and structuring data. Concat all this data into one csv file then handle missing value by mapping with another dataset finally perform certain data visualization and exploratory data analysis.', 'Our Solution', 'Module 1: Data Abstraction', 'The process of data abstraction involves collecting data from numerous web links from Year 2005 to 2017 and viewing the data using JSON viewer in tree format.', 'Module 2: Data Chunking and Integration', 'Was unable to process data in pandas so performed data chunking with chunksize 10000 rows at a time for year 2005 likewise performed for all other years data till 2017 and finally combined all the dataframes into one containing all data from year 2005 to 2017.', 'Module 3: Handling Complexity of Nested Data & format the Unformatted CSV Files', 'Handling unformatted CSV in proper comma separated format so that data frame can be formed. Dataframe produced after merging for all the years from 2005 to 2017 contains a lot of nested JSON data among certain attributes so performed normalization of nested Json forming new_columns naming them based on their attributes key.', '2.2.4 Module 4: Data Cleaning and Preprocessing', 'Involves handling missing value, contraction mapping with another dataset to fill the missing State_Zip_Code column, handling inf and -inf within the dataset for some attributes and forming a new column population_ratio based on passing formula among other attributes.', '2.2.5 Module 5: Data Analysis', 'This step involves forming a correlation matrix to understand the relation between numeric attributes. performed Exploratory Data Analysis on strong correlated attributes to understand pattern/relation between them.', 'Project Deliverables', 'After completion of Project we provided:', 'Final Preprocessed CSV Files', 'Three iPython files:', 'Preprocessed dataset from year 2010 to 2015', 'Preprocessed dataset from year 2008 to 2017', 'Data Visualization and EDA.', 'Tools used', '● Jupyter Notebook', '● Anaconda', '● Notepad++', '● Sublime Text', '● Brackets', '● Python 3.4', '● JSON Viewer', 'Language/techniques used', '● Python', '● ETL Techniques', '● Advanced Excel Formatting', 'Models used', 'My project ‘Population and Community Survey of America’ developed with a software model which makes the project high quality, reliable and cost effective.', '● Software Model : RAD(Rapid Application Development model) Model', '● This Project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.', '● Advantages of RAD Model:', 'o Changing requirements can be accommodated.', 'o Progress can be measured.', 'o Iteration time can be short with use of powerful RAD tools.', 'o Productivity with fewer people in a short time.', 'o Reduced development time.', 'o Increases reusability of components.', 'o Quick initial reviews occur.', 'o Encourages customer feedback.', 'o Integration from very beginning solves a lot of integration issues', 'Skills used', 'Pandas Operations', 'Data Chunking and Integration', 'Data Visualization', 'Exploratory Data Analysis', 'Databases used', 'No Database is used in this project, only used Google Drive for Storing and Transferring Data.', 'Web Cloud Servers used', 'No Web Server is Used', 'What are the technical Challenges Faced during Project Execution', 'Data Cleaning and Filling out Missing Values by Data mapping with another dataset as the Data was not in proper format in the another dataset.', 'How the Technical Challenges were Solved', 'Data Cleaning was done using a few built in pandas operations to deal with Missing Values, Ordering Data Columns, Data Formatting, Changing of data types and many more. Filling of remaining Missing Data from columns using Outer Join among the datasets and using Map Function of Python.', 'Project Snapshots', 'Figure 1: Input Data Schema for Year 2008', 'Figure 2: Output Data Schema from Year 2005 to 2017', 'Figure 3: Dataset for Year 2008', 'figure 3 is pandas dataset of year 2008 which has 169595 rows and 25 columns which was fetched from authenticated survey web portal, data obtained were in JSON format which were converted into pandas dataframe likewise there are dataframes created from year 2005 to 2017.', 'Figure 4:\xa0 Output Preprocessed Dataset', 'figure 4 is an output preprocessed dataset from 2005 to 2017 which has 26,41,363 rows and 25 columns.', 'Figure 5: Describing Numeric Data of Preprocessed Dataset', 'Figure 6: Bar plot of attribute state_name', 'figure 6 represents the bar plot among the state_name on the final output dataset from year 2005 till 2017.', 'Figure 7: KDE Graph for all numeric population data column of dataset', 'figure 7 represents the Kernel Density Estimate Plot(KDE) among all Population estimate data columns for the Preprocessed Dataset. KDE plot is a method for visualizing the distribution of observations in a dataset, analogous to a histogram. KDE represents the data using a continuous probability density curve in one or more dimensions. Plotted many more graphs apart this between highly correlated attributes like pair plot, box plot, line plot etc.', 'Figure 8: Flow Chart', 'Previous article', 'Google LSA API Data Automation and Dashboarding', 'Next article', 'Sentimental Analysis on Shareholder Letter of Companies', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2154,https://insights.blackcoffer.com/google-lsa-api-data-automation-and-dashboarding/,"Google LSA API Data Automation and Dashboarding - Blackcoffer Insights-['Client Background', 'Client:', 'A leading marketing firm in the USA', 'Industry Type:', 'Marketing services & Consulting', 'Services:', 'Marketing consultant', 'Organization Size:', '100+', 'Project Objective', 'For this project objective was to perform API Data Abstraction using Google LSA API in GCP, Automation of data fetching and storing in BigQuery on daily basis, Storing Historical data for all active companies, Fetching Customer Report then storing data on daily basis in BigQuery also storing Historical data for all companies, Perform Linear Regression Modelling on Historical data for all companies and storing the modeling Summary in google sheet in a structured manner, Basecamp Automation with LSA Daily Data, Creating 4 BI Dashboard in Data Studio for Live, Historical, Modelling and Customer Report data for all companies.', 'Project Description', 'For this project task was to obtain an account report and detailed lead report for a specific dates and customer_id using Google Local Service Ads API Service in Google Cloud Platform. Further need to integrate with Google BigQuery database storing MCC data for all companies on a daily basis then storing Historical data for all active companies. Also notifying clients through email and passing messages containing daily account data in a message format to BaseCamp Message Board and Campfire of respective company projects through its API all with python programming, further deploying the script on Heroku Server for automating all this task. Then Creating BI Dashboard in Data Studio connecting with BigQuery and Creating Live Dashboard, Historical Dashboard for all companies.', 'On historical data for all companies, Linear Regression Modelling needs to perform and to create Modelling Dashboard for all companies in Data Studio. Further needs to do\xa0 Exploratory Data Analysis for all companies on Historical Data.', 'To Store Customer Account Report for message lead and phone lead on a daily basis, Script needs to be created and deployed in Heroku and also need to store Historical data for these companies and Finally Create Data Studio Dashboard on it.', 'Creating Sales Representation Dashboard for two Companies which involves multiple Reports and blending of multiple data sources from Big Query.', 'Our Solution', '>> Module 1: API Data Abstraction', 'Which first includes generation of the access token and refresh token with the scope of Google AdWord API for the authentication and connecting with Google LSA API. Then fetching daily data in JSON format for particular account name based on customer_id assigned in API URL while fetching data. Likewise generating a script that would Handle data generation for all other active accounts based on their customer id.', '>> Module 2: Data Imputation and Storing', 'Converting the JSON data to the pandas data frame forming a list of data frame for all the active accounts by looping them then deriving certain more attributes based on their handling the missing and inf values. Finally storing the data in Google Big Query database within the respective table for all accounts using Bigquery API.', '>> Module 3: Data Storing in BigQuery and Notification Automation', 'The task was to automate notifications sent to email and to Basecamp and the data transferred to the database on a daily basis by deploying the script to Heroku Server setting time parameters based on the New York time zone.', '>> Module 4: Automation tools created till now:', 'i. LSA_AccountReport_daily_BigQuery tool: For Automation of Account Report for all companies on a daily basis. Scheduling it at 1:00 am in the Los Angeles Timezone.', 'ii. LSA_AccountReport_Historical_API tool:\xa0 For Storing Historical Data for companies for the last few Years till the end date which we set.', 'iii. Basecamp_lsa_automation: This is used to pass the lsa data in a message format to Campfire for respective companies groups and store lsa data combined for all companies to Messageboard and Campfire at one Automation Python Group in Basecamp.', 'iv. LSA_DateRange Tool: Used to store missed out data for all the companies for a few sets of days or months as per the need.', 'v. LSA_MainSheet_AutoUpdation tool: For Auto updation of main sheet\xa0 ‘LSA Client Lead’\xa0 Google Sheet. As Daily Data are fetched on the basis of this list so it is required to auto update this sheet for all the new companies entered would store information of those like company name, account id and database name.', 'vi. LSA_daily_CustomerReport tool: Created to Store LSA Customer Report for all companies in database ‘CustomerReport_PhoneLead’ & ‘CustomerReport_MessageLead’ on daily basis.', 'vii. Historical_LSA_CustomerReport tool:\xa0 Created to Store LSA Customer Report for all companies in database ‘CustomerReport_PhoneLead’ & ‘CustomerReport_MessageLead’ storing historical data for year 2021.', '>> Module 5: Data Studio BI Dashboards Created:', 'i. Historical Dashboard', 'ii. Live Dashboard', 'ii. Customer Report Dashboard', 'iii. Modelling Report Dashboard', 'iv. Sales Representation Dashboard', 'Project Deliverables', 'Data Studio Dashboard Main Sheet', 'All Codes for the Deployed tools and for Modelling EDA and Test Purpose .', 'Tools used', '● PyCharm', '● Jupyter Notebook', '● Anaconda', '● Heroku', '● Notepad++', '● Google Sheet API', '● Google LSA API on GCP', '● Google BigQuery', '● Sublime Text', '● Brackets', '● JsonViewer', 'Language/techniques used', '● Python', '● SQL', 'Models used', 'My project ‘Google Adword LSA API Reports automation into Google Big Query database and Basecamp’ developed with a software model which makes the project high quality, reliable and cost-effective.', '● Software Model: RAD(Rapid Application Development model) Model', '● This project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.', '● Advantages of RAD Model:', 'o Changing requirements can be accommodated.', 'o Progress can be measured.', 'o Iteration time can be short with the use of powerful RAD tools.', 'o Productivity with fewer people in a short time.', 'o Reduced development time.', 'o Increases reusability of components.', 'o Quick initial reviews occur.', 'o Encourages customer feedback.', 'o Integration from the very beginning solves a lot of integration issues', 'Skills used', '● API Data Abstraction', '● Data Mining and Statistical Modelling', '● Data Wrangling', '● Deployment for Automation', '● Data Visualization', '● SQL', '● Machine Learning', '● Python Programming including OOPs and Exception Handling', 'Databases used', '● Google Firestore (Just for Testing Purpose)', '● Google BigQuery', 'Web Cloud Servers used', 'Google BigQuery Cloud Database with up to 1 TB of free storage is being used.', 'What are the technical Challenges Faced during Project Execution', '● Scheduling Automation of Python Script.', '● Data Exceptions and Duplication in BigQuery Tables.', '● Refresh token Expiration After 7 Days.', '● Data Exception due to Inactive companies or not Updation of LSA Main sheet.', '● Basecamp ProjectId Issue for transferring Data to multiple companies projects.', '● Data Studio Time Series Plot data mismatch due to multiple account id.', 'How the Technical Challenges were Solved', '● Scheduling Automation of Python Script.', 'Python Library BlockingScheduler were used and the Timezone variable ‘TZ’ was set to Los Angeles in Heroku', '● Data Exceptions and Duplication in BigQuery Tables.', 'Structuring SQL Query to deal with all the database issues which were being used in BigQuery to solve those issues.', '● Refresh token Expiration after 7 Days.', 'Initially ‘Auth Playground’ was used for generating Refresh token which was getting expired after every 7 Days so to last it longer for more than a year we are now using the refresh token which was generated using Python script where proper token endpoints and many other headers were defined before generating the refresh token.', '● Data Exception due to Inactive companies or not Updation of LSA Main sheet.', 'Data Exception occurred while API data abstraction for few of the companies which were solved by adding more nested try and except statements after understanding issues also ‘LSA Clients Lead’ main sheet was not being updated by other members due to which we missed out data for few of the companies which were solved by creating script which will automatically update the mainsheet when an error occurred.', '● Basecamp ProjectId Issue for transferring Data to multiple companies projects.', 'This issue was solved by creating Basecamp Main sheet where data was fetched now by mapping the account id of fetched data using LSA Main sheet and project id of all the basecamp companies.', '● Data Studio Time Series Plot data mismatch due to multiple account id.', 'Solved by adding many parameters like setting the metrics which will do a summation of all the companies on a particular day for all the account id.', 'Previous article', 'Healthcare Data Analysis', 'Next article', 'Population and Community Survey of America', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2155,https://insights.blackcoffer.com/healthcare-data-analysis/,"Healthcare Data Analysis - Blackcoffer Insights-['Client Background', 'Client:', 'A leading healthcare tech firm in the USA', 'Industry Type:', 'Healthcare Consulting', 'Services:', 'Management consultant', 'Organization Size:', '100+', 'Project Objective', 'The main objective of this project is to find the pattern in the vital signs of patients who were admitted to the hospital in past. And from this pattern, we get some ranges that help us to give early warnings.', 'Project Description', 'We are more interested in non-survivor patients’ vital signs as compare to survivor patients. we find patterns in', 'vital signs', 'that could better determine that patient died (ex. if Sp02 is below 70, patient in 95% of cases died, if Sp02 is below 50%, the death rate is 99.9%) or we can take correlations which can help us to find better patterns to define death cases.', 'Data The dataset which was used for analysis here is taken from the mimic website. But the dataset is not in the correct format which we want, after some manipulation, we get the data ready for the analysis.', 'Our Solution', 'Approach', 'To protect patient confidentiality date and time is shifted to future that’s not the actual time so from shifted time column we create an extra column hour which tells us the time passed in hours since first observation in ICU.', 'After all manipulation our final dataset contain vital signs values for each observation of patients with time in separate column and also the label fo Death (0 or 1) in another column.', 'There are two options to deal with missing values', 'Drop all rows which contain null values.', '2.Fill the missing values by some method using pandas.', 'I can’t go with 1st option because a major part of the data has missing values. so, I decided to go with the second option and fill missing values with the average of upper and lower values. But before that, I filtered the data and take only those patients’ data who died in a hospital or survive.', 'Project Deliverables', 'After performing EDA which also include the removal of some impossible outliers, we come up with a result of Analysis.', 'This result helps to build an early warning system which predict the condition of patients on the basis of their score, calculated on their condition using vital sign values.', 'Tools used', 'Google Colab Notebook', 'Language/techniques used', 'Python', 'Skills used', 'Data visualization', 'Data analysis', 'Pandas', 'Numpy', 'Seaborn', 'Databases used', 'SQL', 'MongoDB', 'Web Cloud Servers used', 'Google Cloud', 'Project Snapshots', 'Project website url', 'https://colab.research.google.com/drive/1mo7i32BoEVb0Ac6_CWwJd7_HVbliktx0?usp=sharing', 'Previous article', 'ELK Stack – Elastic Queries', 'Next article', 'Google LSA API Data Automation and Dashboarding', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2156,https://insights.blackcoffer.com/budget-sales-kpi-dashboard-using-power-bi/,"Budget, Sales KPI Dashboard using Power BI - Blackcoffer Insights-['Project Description', 'Weekly Data – clustered bar chart for weekly Budget & Actual value , weekly Total Budget & Actual value (completed)', 'YTD Data – clustered bar chart for monthly Budget & Actual value , monthly Total Budget & Actual value (completed)', 'Sales History – stacked chart for yearly sales with each month sales , total yearly sale (completed)', 'Dashlet – weekly data – Total weekly Budget , Total weekly Actual , % weekly Budget (completed)', 'Dashlet – YTD data – Total YTD Budget , Total YTD Actual , % YTD Budget (completed)', 'Dashlet – Sales History – Total Sales (completed)', 'Filters – select Area , select City , select Years (completed)', 'Data Visualization Deliverables', 'Presentation', 'Map', 'Dashboard', 'API Integration', 'Data Visualization Tools', 'Kibana', 'Google Data Studio', 'Microsoft Excel', 'Microsoft Power BI', 'Data Visualization Languages', 'JavaScript', 'SQL', 'Python', 'DAX', 'Demo', 'Previous article', 'Benefits of Big Data in Different fields', 'Next article', 'ELK Stack – Elastic Queries', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
bctech2157,https://insights.blackcoffer.com/amazon-buy-bot-an-automation-ai-tool-to-auto-checkouts/,"Amazon Buy Bot, an Automation AI tool to Auto-Checkouts - Blackcoffer Insights-['Client Background', 'Client:', 'A leading consulting firm in the USA', 'Industry Type:', 'Consulting', 'Services:', 'Management consultant', 'Organization Size:', '100+', 'Project Objective', 'The main objective of this project is to build the automation tool to buy product on amazon.', 'Project Description', 'This project is basically completed using selenium and Python. All we have done is write a python script for automation using Selenium.', 'Make some clicks use logics to check item is in stock or not. If the item is in stock then it buys the product otherwise repeat the process again.', 'Our Solution', 'A simple python code which uses selenium web driver to do all work.', 'Project Deliverables', 'Python Code', 'Tools used', 'Selenium Webdriver', 'Language/techniques used', 'Python', 'Skills used', 'Web Scraping', 'Selenium', 'Project Snapshots', 'Previous article', 'Predictive Modelling, AI, ML Dashboards in Power BI', 'Next article', 'How does Big Data Help in Finance and the Growth of Large Firms?', 'Ajay Bidyarthy', 'RELATED ARTICLES', 'MORE FROM AUTHOR', 'AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy', 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application', 'ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP']"
